{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/train-model/COS_760_Project_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gIBDcyI74-zo"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets fsspec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "V6MdIZgUxRgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from huggingface_hub import login\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "#shona_data = load_dataset(\"DigitalUmuganda/AfriVoice\", \"sn\", streaming=True, split=\"train[:10%]\")\n",
        "\n",
        "# english_data = load_dataset(\"oscar-corpus/OSCAR-2201\", language=\"en\", streaming=True)\n",
        "\n",
        "# num_samples = 500\n",
        "# samples = []\n",
        "# for i, example in enumerate(english_data[\"train\"]):\n",
        "#     if i >= num_samples:\n",
        "#         break\n",
        "#     samples.append(example['text'])\n",
        "\n",
        "# eng_df = pd.DataFrame(samples, columns=['text'])\n",
        "# eng_df.head()\n",
        "\n",
        "zulu_data = load_dataset(\"dsfsi/vukuzenzele-monolingual\", \"zul\")\n",
        "\n",
        "def reformatData(dataDict):\n",
        "  langList = []\n",
        "  for index, row in dataDict.iterrows():\n",
        "    # Access data using column names from the row Series\n",
        "    if 'text' in row and row['text']:\n",
        "        langList.append({\n",
        "        'text': row.get('text', ''),\n",
        "        'author': row.get('author', ''),\n",
        "        'title': row.get('title', ''),\n",
        "        'language': 'zul'\n",
        "              })\n",
        "\n",
        "  return langList\n",
        "\n",
        "human_data = reformatData(zulu_data['train'].to_pandas())\n",
        "\n",
        "zul_df = pd.DataFrame(human_data)\n",
        "zul_df['cleaned_text'] = zul_df['text'].apply(clean_text)\n",
        "zul_df['tokens'] = zul_df['cleaned_text'].apply(word_tokenize)\n",
        "zul_df['label'] = 0\n",
        "zul_df.head()"
      ],
      "metadata": {
        "id": "POzIGY7h5Pax",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eng_df['cleaned_text'] = eng_df['text'].apply(clean_text)\n",
        "# eng_df['tokens'] = eng_df['cleaned_text'].apply(word_tokenize)\n",
        "# print(eng_df.head())\n",
        "\n",
        "zul_df['cleaned_text'] = zul_df['text'].apply(clean_text)\n",
        "zul_df['tokens'] = zul_df['cleaned_text'].apply(word_tokenize)\n",
        "zul_df.head()"
      ],
      "metadata": {
        "id": "-tPmKuYUDCOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "en_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "\n",
        "english_prompts = [\n",
        "    \"Explain the significance of lobola in Southern Africa\",\n",
        "    \"Write a short dialogue between two friends in Johannesburg\",\n",
        "    \"Describe linguistic features that make isiZulu agglutinative\"\n",
        "]\n",
        "\n",
        "zulu_prompts = [\n",
        "    \"Chaza ngokubaluleka kwesiko lwelobola eNingizimu Afrika\",\n",
        "    \"Bhala inkulumo emfushane phakathi kwabangani ababili eGoli\",\n",
        "    \"Landela indaba yamaZulu ngokomlando\",\n",
        "    \"Chaza ngamasiko amasha eZulu eskhathini samanje\",\n",
        "    \"Bhala inganekwane ethi 'UNogwaja noFudu'\"\n",
        "]\n",
        "\n",
        "# def generate_with_prompts(prompts, generator, language, samples_per_prompt=3):\n",
        "#     data = []\n",
        "#     for prompt in prompts:\n",
        "#         for _ in range(samples_per_prompt):\n",
        "#             output = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
        "#             data.append({\n",
        "#                 'prompt': prompt,\n",
        "#                 'text': output[0]['generated_text'],\n",
        "#                 'label': 'machine',\n",
        "#                 'language': language,\n",
        "#                 'prompt_type': 'cultural' if \"tsika\" in prompt else 'linguistic'  # Tag for analysis\n",
        "#             })\n",
        "#     return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# eng_mg_df = generate_with_prompts(english_prompts, en_generator, 'English')\n",
        "# eng_mg_df['cleaned_text'] = eng_mg_df['text'].apply(clean_text)\n",
        "# eng_mg_df['tokens'] = eng_mg_df['cleaned_text'].apply(word_tokenize)\n",
        "# print(eng_mg_df['text'].iloc[0][:300])\n",
        "\n",
        "def load_jsonl_data(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:  # Skip empty lines\n",
        "                    try:\n",
        "                        data.append(json.loads(line))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"Error parsing JSON line: {e}\")\n",
        "                        continue\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return []\n",
        "\n",
        "    return data\n",
        "\n",
        "zul_mg_df = pd.DataFrame(load_jsonl_data('zulu_mg_text.jsonl'))\n",
        "\n",
        "#zul_mg_df = pd.DataFrame(data)\n",
        "machine_data =reformatData(zul_mg_df)\n",
        "zul_mg_df = pd.DataFrame(machine_data)\n",
        "zul_mg_df['text'] = zul_mg_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "zul_mg_df['language'] = 'zul'\n",
        "zul_mg_df['cleaned_text'] = zul_mg_df['text'].apply(clean_text)\n",
        "zul_mg_df['tokens'] = zul_mg_df['cleaned_text'].apply(word_tokenize)\n",
        "zul_mg_df['label'] = 1\n",
        "\n",
        "zul_mg_df.head()"
      ],
      "metadata": {
        "id": "rChZLJ-8E1MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMBINE THE DATASETS\n",
        "-------------------------"
      ],
      "metadata": {
        "id": "iIqYgsMmtYI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both datasets, this will be bad for us if we have more than 70% differnce in data length\n",
        "# Combine both datasets\n",
        "all_texts = pd.concat([zul_df, zul_mg_df], ignore_index=True)\n",
        "all_texts_shuffled = all_texts.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Total texts after combining: {len(all_texts)}\")\n",
        "print(f\"Human-written texts: {len(zul_df)}\")\n",
        "print(f\"Machine-generated texts: {len(zul_mg_df)}\")\n",
        "\n",
        "# Create a DataFrame for easier analysis\n",
        "combined_df = pd.DataFrame(all_texts_shuffled)\n",
        "combined_df.head()"
      ],
      "metadata": {
        "id": "XdcTeCfvtXhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a balanced dataset with equal samples from each class\n",
        "# we can choose to use this of the combined one\n",
        "def create_balanced_dataset(df, target_size_per_class=None):\n",
        "    class_counts = df['label'].value_counts()\n",
        "    min_class_size = class_counts.min()\n",
        "    if target_size_per_class:\n",
        "        sample_size = min(target_size_per_class, min_class_size)\n",
        "    else:\n",
        "        sample_size = min_class_size\n",
        "    balanced_df = df.groupby('label').sample(n=sample_size, random_state=42)\n",
        "    return balanced_df.reset_index(drop=True)\n",
        "\n",
        "# Create balanced dataset, because we need the same number of samples for each class\n",
        "balanced_df = create_balanced_dataset(combined_df)\n",
        "print(f\"\\nBalanced dataset created with {len(balanced_df)} samples\")\n",
        "print(f\"Label distribution in balanced dataset: \\n{balanced_df['label'].value_counts()}\")\n",
        "\n",
        "# Save balanced dataset\n",
        "balanced_output_path = 'balanced_zulu_texts.csv'\n",
        "balanced_df.to_csv(balanced_output_path, index=False, encoding='utf-8')\n",
        "print(f\"Balanced dataset saved to: {balanced_output_path}\")\n",
        "balanced_df.head()"
      ],
      "metadata": {
        "id": "gwRW8Jlxt66B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMRobertaForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class ZuluTextDataset(Dataset):\n",
        "    \"\"\"Custom Dataset class for Zulu text classification\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class ZuluTextClassifier:\n",
        "    \"\"\"XLM-RoBERTa classifier for Zulu human vs machine text detection\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='xlm-roberta-base', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
        "        self.model = XLMRobertaForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\"Model initialized on {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "    def prepare_data(self, df, text_column='cleaned_text', label_column='label', test_size=0.2):\n",
        "        \"\"\"Prepare and split the data\"\"\"\n",
        "        # Filter for Zulu language if language column exists\n",
        "        if 'language' in df.columns:\n",
        "            df_zulu = df[df['language'].str.lower().isin(['zul'])].copy()\n",
        "            print(f\"Filtered {len(df_zulu)} Zulu samples from {len(df)} total samples\")\n",
        "        else:\n",
        "            df_zulu = df.copy()\n",
        "            print(f\"Using all {len(df_zulu)} samples (no language filtering)\")\n",
        "\n",
        "        if len(df_zulu) == 0:\n",
        "            raise ValueError(\"No Zulu samples found in the dataset\")\n",
        "\n",
        "        # Extract texts and labels\n",
        "        texts = df_zulu[text_column].tolist()\n",
        "        labels = df_zulu[label_column].tolist()\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            texts, labels,\n",
        "            test_size=test_size,\n",
        "            random_state=42,\n",
        "            stratify=labels\n",
        "        )\n",
        "\n",
        "        print(f\"Training samples: {len(X_train)}\")\n",
        "        print(f\"Test samples: {len(X_test)}\")\n",
        "        print(f\"Label distribution in training: {np.bincount(y_train)}\")\n",
        "        print(f\"Label distribution in test: {np.bincount(y_test)}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_datasets(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Create PyTorch datasets\"\"\"\n",
        "        train_dataset = ZuluTextDataset(X_train, y_train, self.tokenizer, self.max_length)\n",
        "        test_dataset = ZuluTextDataset(X_test, y_test, self.tokenizer, self.max_length)\n",
        "\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset, output_dir='./zulu_classifier_results'):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=5,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir=f'{output_dir}/logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=None,  # Disable wandb\n",
        "            seed=42,\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def evaluate(self, test_dataset, trainer=None):\n",
        "        \"\"\"Evaluate the model\"\"\"\n",
        "        if trainer is None:\n",
        "            # Create a new trainer for evaluation\n",
        "            trainer = Trainer(model=self.model)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = predictions.label_ids\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "        # Per-class metrics\n",
        "        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average=None\n",
        "        )\n",
        "\n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'precision_per_class': precision_per_class,\n",
        "            'recall_per_class': recall_per_class,\n",
        "            'f1_per_class': f1_per_class,\n",
        "            'support': support,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_evaluation_results(self, results):\n",
        "        \"\"\"Print detailed evaluation results\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        print(f\"Overall Accuracy: {results['accuracy']:.4f}\")\n",
        "        print(f\"Weighted Precision: {results['precision']:.4f}\")\n",
        "        print(f\"Weighted Recall: {results['recall']:.4f}\")\n",
        "        print(f\"Weighted F1-Score: {results['f1']:.4f}\")\n",
        "\n",
        "        print(\"\\nPer-Class Results:\")\n",
        "        class_names = ['Human', 'Machine']\n",
        "        for i in range(len(class_names)):\n",
        "            print(f\"{class_names[i]}:\")\n",
        "            print(f\"  Precision: {results['precision_per_class'][i]:.4f}\")\n",
        "            print(f\"  Recall: {results['recall_per_class'][i]:.4f}\")\n",
        "            print(f\"  F1-Score: {results['f1_per_class'][i]:.4f}\")\n",
        "            print(f\"  Support: {results['support'][i]}\")\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(results['y_true'], results['y_pred'],\n",
        "                                  target_names=class_names))\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the Zulu text classification\"\"\"\n",
        "\n",
        "    df = balanced_df\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = ZuluTextClassifier()\n",
        "\n",
        "    # Prepare data\n",
        "    X_train, X_test, y_train, y_test = classifier.prepare_data(df)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset, test_dataset = classifier.create_datasets(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Train model\n",
        "    trainer = classifier.train(train_dataset, test_dataset)\n",
        "\n",
        "    # Evaluate\n",
        "    results = classifier.evaluate(test_dataset, trainer)\n",
        "    classifier.print_evaluation_results(results)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RhHatRIMMDYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}