{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/train-model/train_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers adapters datasets fsspec evaluate shap nltk lime textstat"
      ],
      "metadata": {
        "id": "FJGU3YBVFKBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries and Initial Setup**\n",
        "Here, we import the required Python libraries for data manipulation, model development, and analysis. This step also includes initial setup tasks and setting random seeds to ensure the reproducibility of our experiments."
      ],
      "metadata": {
        "id": "qrKXsFt1ba9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQF_DKiWFAeK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import gc\n",
        "import warnings\n",
        "import torch\n",
        "import nltk\n",
        "import evaluate\n",
        "import shap\n",
        "import lime\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from textstat import flesch_reading_ease, automated_readability_index\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, Dataset, ClassLabel, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# Initial setup\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Collection and Preprocessing Functions**\n",
        "This cell defines a set of helper functions to load, clean, and prepare the datasets, corresponding to Phase 1 of our methodology. This includes cleaning text by removing URLs and non-alphabetic characters, loading data from different sources, and preparing balanced datasets for human- vs. machine-generated text."
      ],
      "metadata": {
        "id": "XgKLtD1MbtQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def load_machine_text(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        # Open and read the machine-generated text file line by line\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line))\n",
        "\n",
        "        # Convert list of dicts to DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "        if not df.empty and isinstance(df['text'].iloc[0], list):\n",
        "            df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Machine-generated text file not found at {file_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def prepare_dataset(lang_code, human_data_source, machine_data_path, output_csv_path):\n",
        "    print(f\"Preparing dataset for language: {lang_code.upper()}\")\n",
        "\n",
        "    # Load human-written dataset from HuggingFace Datasets\n",
        "    human_dataset = load_dataset(human_data_source, lang_code)\n",
        "\n",
        "    human_df = human_dataset['train'].to_pandas()\n",
        "    human_df = human_df[['text']].dropna()\n",
        "    human_df['label'] = 0\n",
        "    human_df['language'] = lang_code\n",
        "\n",
        "    print(f\"Loaded {len(human_df)} human-written data.\")\n",
        "\n",
        "    # Load machine-generated dataset\n",
        "    machine_df = load_machine_text(machine_data_path)\n",
        "\n",
        "    if machine_df.empty:\n",
        "      return None\n",
        "\n",
        "    machine_df = machine_df[['text']].dropna()\n",
        "    machine_df['label'] = 1\n",
        "    machine_df['language'] = lang_code\n",
        "\n",
        "    print(f\"Loaded {len(machine_df)} machine-generated data.\")\n",
        "\n",
        "    # Combine human and machine data\n",
        "    combined_df = pd.concat([human_df, machine_df], ignore_index=True)\n",
        "    combined_df['text'] = combined_df['text'].apply(clean_text)\n",
        "    # Drop rows with missing text\n",
        "    combined_df.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "    # Balance the dataset by sampling the same number of samples from each class\n",
        "    min_class_size = combined_df['label'].value_counts().min()\n",
        "    balanced_df = combined_df.groupby('label').sample(n=min_class_size, random_state=42)\n",
        "    # Shuffle the balanced dataset\n",
        "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Created a balanced dataset with {len(balanced_df)} total samples ({min_class_size} per class).\")\n",
        "    # Save the balanced dataset to CSV\n",
        "    balanced_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "    print(f\"Balanced dataset saved to {output_csv_path}\")\n",
        "    return balanced_df\n",
        "\n"
      ],
      "metadata": {
        "id": "yCkJHAPs_eJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating and Balancing the Datasets**\n",
        "In this step, we execute the data preparation functions to create balanced training and testing datasets for both isiZulu and English.\n",
        "\n",
        "We load human-written text from the `vukuzenzele` dataset and combine it with machine-generated text samples to form the basis for our model training and evaluation."
      ],
      "metadata": {
        "id": "huLFVC1GcGMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create small example machine-generated text files if they don't exist\n",
        "if not os.path.exists('zulu_mg_text.jsonl'):\n",
        "    with open('zulu_mg_text.jsonl', 'w') as f:\n",
        "        f.write('{\"text\": \"Lokhu umbhalo owenziwe ngomshini mayelana namasiko akwaZulu.\"}\\n')\n",
        "        f.write('{\"text\": \"UNogwaja noFudu babengabangani abakhulu ehlathini.\"}\\n')\n",
        "\n",
        "if not os.path.exists('eng_mg_data.jsonl'):\n",
        "    with open('eng_mg_data.jsonl', 'w') as f:\n",
        "        f.write('{\"text\": \"This is machine-generated text about South African culture.\"}\\n')\n",
        "        f.write('{\"text\": \"The quick brown fox jumps over the lazy dog in Johannesburg.\"}\\n')\n",
        "\n",
        "# Prepare balanced datasets for isiZulu and English\n",
        "zulu_dataset = prepare_dataset('zul', 'dsfsi/vukuzenzele-monolingual', 'zulu_mg_text.jsonl', 'balanced_zulu_texts.csv')\n",
        "english_dataset = prepare_dataset('eng', 'dsfsi/vukuzenzele-monolingual', 'eng_mg_data.jsonl', 'balanced_english_texts.csv')\n",
        "\n",
        "# Display a sample of each dataset for verification\n",
        "print(\"\\nZulu Dataset Sample:\")\n",
        "print(pd.DataFrame(zulu_dataset).head())\n",
        "print(\"\\nEnglish Dataset Sample:\")\n",
        "print(pd.DataFrame(english_dataset).head())"
      ],
      "metadata": {
        "id": "aJeuL8nqBZCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Architecture: The Text Classifier Class**\n",
        "This class, `ZuluTextClassifier` aliased as `TextClassifier`, defines the core architecture for our model. It handles loading a pre-trained transformer model (like AfriBERTa), preparing data splits, setting up Low-Rank Adaptation (LoRA) for efficient fine-tuning, and implementing the training and evaluation loops, aligning with Phase 2 of our project plan."
      ],
      "metadata": {
        "id": "dLjNny_RcXhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ZuluTextClassifier:\n",
        "    \"\"\"AfriBERTa classifier for Zulu human vs machine text detection\"\"\"\n",
        "    def __init__(self, model_name='castorini/afriberta_base', max_length=512):\n",
        "        # Initialize the tokenizer and model for sequence classification\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "        self.max_length = max_length\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\"Model initialized on {self.device}\")\n",
        "\n",
        "    # Prepare train and test splits from a DataFrame\n",
        "    def prepare_data(self, df):\n",
        "        X = df['text'].tolist()\n",
        "        y = LabelEncoder().fit_transform(df['label'])\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    # Tokenize and encode the train and test data\n",
        "    def create_datasets(self, X_train, X_test, y_train, y_test):\n",
        "        train_encodings = self.tokenizer(X_train, truncation=True, padding=True, max_length=self.max_length)\n",
        "        test_encodings = self.tokenizer(X_test, truncation=True, padding=True, max_length=self.max_length)\n",
        "\n",
        "        train_data = {\n",
        "            'input_ids': train_encodings['input_ids'],\n",
        "            'attention_mask': train_encodings['attention_mask'],\n",
        "            'labels': y_train\n",
        "            }\n",
        "        test_data = {\n",
        "            'input_ids': test_encodings['input_ids'],\n",
        "            'attention_mask': test_encodings['attention_mask'],\n",
        "            'labels': y_test\n",
        "            }\n",
        "\n",
        "        train_dataset = Dataset.from_dict(train_data)\n",
        "        test_dataset = Dataset.from_dict(test_data)\n",
        "\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    # enable Low-Rank Adaptation for efficient fine-tuning\n",
        "    def setup_lora(self, use_lora=True):\n",
        "        if use_lora:\n",
        "            print(\"Setting up LoRA configuration...\")\n",
        "            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"query\", \"key\", \"value\"], lora_dropout=0.1, bias=\"none\", task_type=\"SEQ_CLS\")\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            print(f\"LoRA enabled. Trainable parameters: {self.model.num_parameters()}\")\n",
        "\n",
        "    # Train the model using HuggingFace Trainer\n",
        "    def train(self, train_dataset, eval_dataset, output_dir):\n",
        "        self.setup_lora(use_lora=True)\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=5,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            learning_rate=5e-5,\n",
        "            logging_steps=50,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            report_to=\"none\"\n",
        "            )\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            tokenizer=self.tokenizer\n",
        "            )\n",
        "\n",
        "        # Print where a specific model will be stored and save\n",
        "        print(f\"Starting training... Results will be saved to {output_dir}\")\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    # Evaluate the model and return metrics\n",
        "    def evaluate(self, test_dataset, trainer=None):\n",
        "        if trainer is None:\n",
        "          trainer = Trainer(model=self.model)\n",
        "\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = predictions.label_ids\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'precision_per_class': precision_per_class,\n",
        "            'recall_per_class': recall_per_class,\n",
        "            'f1_per_class': f1_per_class,\n",
        "            'support': support,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred}\n",
        "\n",
        "    # Print formatted evaluation results\n",
        "    def print_evaluation_results(self, results):\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\nEVALUATION RESULTS\\n\" + \"=\"*50)\n",
        "        print(f\"Overall Accuracy: {results['accuracy']:.4f}\\nWeighted Precision: {results['precision']:.4f}\\nWeighted Recall: {results['recall']:.4f}\\nWeighted F1-Score: {results['f1']:.4f}\\n\\nPer-Class Results:\")\n",
        "\n",
        "        class_names = ['Human', 'Machine']\n",
        "\n",
        "        for i in range(len(class_names)):\n",
        "            print(f\"{class_names[i]}:\\n  Precision: {results['precision_per_class'][i]:.4f}\\n  Recall: {results['recall_per_class'][i]:.4f}\\n  F1-Score: {results['f1_per_class'][i]:.4f}\\n  Support: {results['support'][i]}\")\n",
        "        print(\"\\nClassification Report:\\n\", classification_report(results['y_true'], results['y_pred'], target_names=class_names))\n",
        "\n",
        "    # Predict the class of a single text string\n",
        "    def predict_single_text(self, text):\n",
        "        self.model.eval()\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encoding)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = predictions.max().item()\n",
        "\n",
        "        class_names = ['Human', 'Machine']\n",
        "        return {\n",
        "            'predicted_class': class_names[predicted_class],\n",
        "            'confidence': confidence,\n",
        "            'probabilities': {\n",
        "                'Human': predictions[0][0].item(),\n",
        "                'Machine': predictions[0][1].item()\n",
        "                }\n",
        "            }"
      ],
      "metadata": {
        "id": "VgJgbeM5Gu87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explainability Framework: LIME and Linguistic Analysis**\n",
        "The `TextExplainabilityAnalyzer` class implements our explainability framework. It uses LIME (Local Interpretable Model-agnostic Explanations) to analyze feature importance, performs linguistic feature analysis (lexical diversity, sentence length) and conducts error analysis on misclassified examples to address our core research questions."
      ],
      "metadata": {
        "id": "eNnMdwoqcsK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explainability analysis for machine vs human text classification\n",
        "class TextExplainabilityAnalyzer:\n",
        "    # Initialize with model, tokenizer, device, and class names\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model, self.tokenizer, self.device, self.class_names = model, tokenizer, device, ['Human', 'Machine']\n",
        "        self.lime_explainer = LimeTextExplainer(class_names=self.class_names)\n",
        "\n",
        "    # Predict class probabilities for a list of texts for LIME\n",
        "    def predict_proba_for_lime(self, texts):\n",
        "        predictions = []\n",
        "        self.model.eval()\n",
        "\n",
        "        for text in texts:\n",
        "            encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "            encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoding)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predictions.append(probs.cpu().numpy()[0])\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    # Generate LIME explanation for a single text\n",
        "    def explain_with_lime(self, text, num_features=10):\n",
        "        return self.lime_explainer.explain_instance(text, self.predict_proba_for_lime, num_features=num_features, num_samples=500)\n",
        "\n",
        "    # Analyze most important features for each class using LIME\n",
        "    def analyze_feature_importance(self, test_texts, test_labels, sample_ratio=0.3):\n",
        "        results = {\n",
        "            'human_features': [],\n",
        "            'machine_features': []\n",
        "            }\n",
        "\n",
        "        num_samples = int(len(test_texts) * sample_ratio)\n",
        "        print(f\"Analyzing {num_samples} samples ({sample_ratio*100:.0f}%)\")\n",
        "\n",
        "        sample_indices, _ = train_test_split(range(len(test_texts)), test_size=1-sample_ratio, stratify=test_labels, random_state=42)\n",
        "        for idx in tqdm(sample_indices, desc=\"Generating LIME explanations\"):\n",
        "            text, true_label = test_texts[idx], test_labels[idx]\n",
        "\n",
        "            if not text or text.isspace():\n",
        "               continue\n",
        "            try:\n",
        "                features = self.explain_with_lime(text).as_list()\n",
        "\n",
        "                if true_label == 0:\n",
        "                  results['human_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "                else:\n",
        "                  results['machine_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "            except Exception as e:\n",
        "              print(f\"LIME Error on index {idx}: {e}\")\n",
        "\n",
        "        results['human_patterns'] = Counter(results['human_features']).most_common(20)\n",
        "        results['machine_patterns'] = Counter(results['machine_features']).most_common(20)\n",
        "\n",
        "        return results\n",
        "\n",
        "    # Compute basic linguistic features for sampled texts\n",
        "    def linguistic_feature_analysis(self, texts, labels, sample_ratio=0.3):\n",
        "        num_samples = int(len(texts) * sample_ratio)\n",
        "        sample_indices, _ = train_test_split(range(len(texts)), test_size=1-sample_ratio, stratify=labels, random_state=42)\n",
        "        sampled_texts, sampled_labels = [texts[i] for i in sample_indices], [labels[i] for i in sample_indices]\n",
        "        print(f\"Running linguistic analysis on {len(sampled_texts)} samples ({sample_ratio*100:.0f}%)\")\n",
        "\n",
        "        features = {'avg_sentence_length': [], 'lexical_diversity': [], 'repetition_score': []}\n",
        "        for text in tqdm(sampled_texts, desc=\"Computing linguistic features\"):\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            avg_sent_len = np.mean([len(s.split()) for s in sentences if s.strip()]) if any(s.strip() for s in sentences) else 0\n",
        "\n",
        "            features['avg_sentence_length'].append(avg_sent_len)\n",
        "            words = text.lower().split()\n",
        "\n",
        "            lexical_div = len(set(words)) / len(words) if words else 0\n",
        "            features['lexical_diversity'].append(lexical_div)\n",
        "\n",
        "            word_counts = Counter(words)\n",
        "            repetition = sum(count - 1 for count in word_counts.values()) / len(words) if words else 0\n",
        "\n",
        "            features['repetition_score'].append(repetition)\n",
        "\n",
        "        human_features = {k: [v[i] for i, label in enumerate(sampled_labels) if label == 0] for k, v in features.items()}\n",
        "        machine_features = {k: [v[i] for i, label in enumerate(sampled_labels) if label == 1] for k, v in features.items()}\n",
        "\n",
        "        return human_features, machine_features\n",
        "\n",
        "    # Plot histograms for each linguistic feature for both classes\n",
        "    def plot_feature_distributions(self, human_features, machine_features):\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        axes = axes.ravel()\n",
        "        feature_names = list(human_features.keys())\n",
        "\n",
        "        for i, feature in enumerate(feature_names):\n",
        "            ax = axes[i]\n",
        "            sns.histplot(human_features[feature], ax=ax, color='skyblue', label='Human', kde=True)\n",
        "            sns.histplot(machine_features[feature], ax=ax, color='salmon', label='Machine', kde=True)\n",
        "\n",
        "            ax.set_title(feature.replace('_', ' ').title(), fontsize=14)\n",
        "            ax.set_xlabel('Value')\n",
        "            ax.set_ylabel('Frequency')\n",
        "            ax.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Analyze misclassified examples and generate LIME explanations for a sample\n",
        "    def error_analysis(self, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "        all_errors = [\n",
        "            {\n",
        "                'text': test_texts[i],\n",
        "                'true_label': self.class_names[true_label],\n",
        "                'predicted_label': self.class_names[pred_label]\n",
        "                }\n",
        "            for i, (true_label, pred_label) in enumerate(zip(test_labels, predictions)) if true_label != pred_label\n",
        "            ]\n",
        "\n",
        "        num_errors_to_analyze = max(1, int(len(all_errors) * sample_ratio))\n",
        "        errors_to_analyze = np.random.choice(all_errors, min(num_errors_to_analyze, len(all_errors)), replace=False) if all_errors else []\n",
        "\n",
        "        print(f\"Found {len(all_errors)} total errors, analyzing {len(errors_to_analyze)} in detail\")\n",
        "        error_analysis_res = {'total_errors': len(all_errors), 'false_positives': len([e for e in all_errors if e['true_label'] == 'Human']), 'false_negatives': len([e for e in all_errors if e['true_label'] == 'Machine']), 'examples': all_errors[:10]}\n",
        "        error_explanations = []\n",
        "\n",
        "        for error in tqdm(errors_to_analyze[:5], desc=\"Generating error explanations\"):\n",
        "            try:\n",
        "                explanation = self.explain_with_lime(error['text'])\n",
        "                error_explanations.append({'error': error, 'explanation': explanation.as_list()})\n",
        "            except Exception as e: print(f\"Error generating explanation: {e}\")\n",
        "\n",
        "        error_analysis_res['explanations'] = error_explanations\n",
        "\n",
        "        return error_analysis_res\n",
        "\n",
        "    # Run all explainability analyses and print a summary report\n",
        "    def generate_explanation_report(self, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "        print(\"=\"*60 + f\"\\nMODEL EXPLAINABILITY ANALYSIS REPORT (Analyzing {sample_ratio*100:.0f}% of data)\\n\" + \"=\"*60)\n",
        "        print(\"\\n1. FEATURE IMPORTANCE ANALYSIS (LIME)\\n\" + \"-\" * 40)\n",
        "\n",
        "        feature_analysis = self.analyze_feature_importance(test_texts, test_labels, sample_ratio)\n",
        "        print(\"Top features indicating HUMAN text:\")\n",
        "        [print(f\"  - '{f}': {c}\") for f, c in feature_analysis['human_patterns'][:10]]\n",
        "        print(\"\\nTop features indicating MACHINE text:\")\n",
        "        [print(f\"  - '{f}': {c}\") for f, c in feature_analysis['machine_patterns'][:10]]\n",
        "\n",
        "        print(\"\\n2. LINGUISTIC FEATURE ANALYSIS\\n\" + \"-\" * 40)\n",
        "        human_features, machine_features = self.linguistic_feature_analysis(test_texts, test_labels, sample_ratio)\n",
        "        for name in human_features.keys():\n",
        "            print(f\"{name.replace('_', ' ').title()}:\\n  Human avg: {np.mean(human_features[name]):.3f}, Machine avg: {np.mean(machine_features[name]):.3f}\")\n",
        "\n",
        "        print(\"\\n3. ERROR ANALYSIS\\n\" + \"-\" * 40)\n",
        "        error_analysis_res = self.error_analysis(test_texts, test_labels, predictions, sample_ratio)\n",
        "        print(f\"Total classification errors: {error_analysis_res['total_errors']}\\nFalse positives (Human → Machine): {error_analysis_res['false_positives']}\\nFalse negatives (Machine → Human): {error_analysis_res['false_negatives']}\")\n",
        "        print(\"\\nExample misclassifications:\")\n",
        "        for i, ex in enumerate(error_analysis_res['examples'][:3]):\n",
        "            print(f\"  {i+1}. True: {ex['true_label']}, Predicted: {ex['predicted_label']}\\n     Text: {ex['text'][:100]}...\")\n",
        "\n",
        "        return {'feature_analysis': feature_analysis, 'linguistic_analysis': (human_features, machine_features), 'error_analysis': error_analysis_res}\n",
        "\n",
        "\n",
        "# Run explainability analysis and plot feature distributions\n",
        "def add_explainability_to_classifier(classifier, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "    explainer = TextExplainabilityAnalyzer(classifier.model, classifier.tokenizer, classifier.device)\n",
        "    report = explainer.generate_explanation_report(test_texts, test_labels, predictions, sample_ratio)\n",
        "\n",
        "    print(\"\\n4. LINGUISTIC FEATURE DISTRIBUTION PLOTS\\n\" + \"-\" * 40)\n",
        "    human_features, machine_features = report['linguistic_analysis']\n",
        "    explainer.plot_feature_distributions(human_features, machine_features)\n",
        "\n",
        "    return explainer, report"
      ],
      "metadata": {
        "id": "2wUTwfjLFOAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Orchestration: Defining Training Pipelines**\n",
        "This cell contains the high-level functions `run_experiment` and `run_zero_shot_experiment`. These functions orchestrate the entire training, evaluation and explainability pipeline for a given model and language dataset, enabling systematic testing of our hypotheses regarding cross-lingual transfer and model performance."
      ],
      "metadata": {
        "id": "EEaG6sxYdBu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alias for the main classifier class\n",
        "class TextClassifier(ZuluTextClassifier):\n",
        "  pass\n",
        "\n",
        "# Run a full experiment: train, evaluate, and explain\n",
        "def run_experiment(model_name, dataset_df, language_name, run_explainability=False):\n",
        "    print(\"\\n\" + \"=\"*80 + f\"\\nSTARTING EXPERIMENT: Model='{model_name}', Language='{language_name}'\\n\" + \"=\"*80)\n",
        "    sanitized_model_name = model_name.replace(\"/\", \"-\")\n",
        "    output_dir = f\"./fineTunedModes/results_{sanitized_model_name}_{language_name}\"\n",
        "\n",
        "    classifier = TextClassifier(model_name=model_name)\n",
        "\n",
        "    # Prepare data splits\n",
        "    X_train, X_test, y_train, y_test = classifier.prepare_data(dataset_df)\n",
        "\n",
        "    train_dataset, test_dataset = classifier.create_datasets(X_train, X_test, y_train, y_test)\n",
        "    trainer = classifier.train(train_dataset, test_dataset, output_dir=output_dir)\n",
        "\n",
        "    # Evaluate the model\n",
        "    results = classifier.evaluate(test_dataset, trainer)\n",
        "    classifier.print_evaluation_results(results)\n",
        "\n",
        "    if run_explainability:\n",
        "        print(\"\\nRunning Full Explainability Analysis\")\n",
        "        add_explainability_to_classifier(classifier, X_test, y_test, results['y_pred'], sample_ratio=0.3)\n",
        "\n",
        "    # Cleanup resources\n",
        "    del classifier, trainer, train_dataset, test_dataset\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"language\": language_name,\n",
        "        \"accuracy\": results['accuracy'],\n",
        "        \"f1_score\": results['f1'],\n",
        "        \"precision\": results['precision'],\n",
        "        \"recall\": results['recall']}\n",
        "\n",
        "# Train on source language then evaluate on target language\n",
        "def run_zero_shot_experiment(model_name, source_df, target_df):\n",
        "    print(\"\\n\" + \"=\"*80 + f\"\\nSTARTING ZERO-SHOT EXPERIMENT: Model='{model_name}', Source='English', Target='isiZulu'\\n\" + \"=\"*80)\n",
        "    sanitized_model_name = model_name.replace(\"/\", \"-\")\n",
        "    output_dir = f\"./fineTunedModes/results_{sanitized_model_name}_eng-for-zeroshot\"\n",
        "\n",
        "    # Initialize classifier with the specified model\n",
        "    classifier = TextClassifier(model_name=model_name)\n",
        "\n",
        "    # Prepare data splits for the source language (English)\n",
        "    X_train_src, X_test_src, y_train_src, y_test_src = classifier.prepare_data(source_df)\n",
        "    train_dataset_src, eval_dataset_src = classifier.create_datasets(X_train_src, X_test_src, y_train_src, y_test_src)\n",
        "\n",
        "    print(\"\\nTraining on source language (English)\")\n",
        "    # Train the model on the source language\n",
        "    trainer = classifier.train(train_dataset_src, eval_dataset_src, output_dir=output_dir)\n",
        "\n",
        "    # Prepare test data for the target language (isiZulu)\n",
        "    print(\"\\nEvaluating on target language (isiZulu)\")\n",
        "    _ , X_test_tgt, _, y_test_tgt = classifier.prepare_data(target_df)\n",
        "    target_encodings = classifier.tokenizer(X_test_tgt, truncation=True, padding=True, max_length=classifier.max_length)\n",
        "\n",
        "    target_test_data = {\n",
        "        'input_ids': target_encodings['input_ids'],\n",
        "        'attention_mask': target_encodings['attention_mask'],\n",
        "        'labels': y_test_tgt\n",
        "        }\n",
        "    test_dataset_tgt = Dataset.from_dict(target_test_data)\n",
        "\n",
        "    # Evaluate the trained model on the target language test set\n",
        "    results = classifier.evaluate(test_dataset_tgt, trainer)\n",
        "\n",
        "    classifier.print_evaluation_results(results)\n",
        "\n",
        "    # Cleanup resources\n",
        "    del classifier, trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"model_name\": f\"{model_name} (Zero-Shot)\",\n",
        "        \"language\": \"zul\",\n",
        "        \"accuracy\": results['accuracy'],\n",
        "        \"f1_score\": results['f1'],\n",
        "        \"precision\": results['precision'],\n",
        "        \"recall\": results['recall']\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "9jEhIQHIGcIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the Full Project Pipeline**\n",
        "The `full_project_pipeline` function defines the complete experimental workflow. It iterates through the specified models (AfriBERTa, XLM-Roberta) and languages (isiZulu, English), runs both standard fine-tuning and zero-shot experiments, and generates a final comparative analysis report with visualizations to summarize our findings.\n",
        "\n",
        "After training if you want to use the trained models, download the zipped file and extract it into where you have your server that you use for prediction"
      ],
      "metadata": {
        "id": "7tAHBaePdM4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all experiments and print comparative results\n",
        "def full_project_pipeline():\n",
        "    # List of models to test (AfriBERTa and XLM-R)\n",
        "    MODELS_TO_TEST = ['castorini/afriberta_base', 'xlm-roberta-base']\n",
        "    # Datasets for isiZulu and English\n",
        "    DATASETS = {\"zul\": zulu_dataset, \"eng\": english_dataset}\n",
        "    all_results = []\n",
        "\n",
        "    # Run standard experiments for each model and language\n",
        "    for lang_code, df in DATASETS.items():\n",
        "        for model in MODELS_TO_TEST:\n",
        "            is_primary_exp = (lang_code == 'zul' and model == 'castorini/afriberta_base')\n",
        "            result = run_experiment(model, df, lang_code, run_explainability=is_primary_exp)\n",
        "            all_results.append(result)\n",
        "\n",
        "    # Run zero-shot experiments (train on English, test on isiZulu)\n",
        "    for model in MODELS_TO_TEST:\n",
        "        zero_shot_result = run_zero_shot_experiment(model, DATASETS['eng'], DATASETS['zul'])\n",
        "        all_results.append(zero_shot_result)\n",
        "\n",
        "    # Print final comparative analysis report\n",
        "    print(\"\\n\\n\" + \"#\"*80 + \"\\n\" + \" \" * 20 + \"FINAL COMPARATIVE ANALYSIS REPORT\\n\" + \"#\"*80)\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    print(\"\\n--- Performance Metrics Across All Experiments ---\\n\", results_df)\n",
        "\n",
        "    # Plot F1-score comparison for all experiments\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    pivot_df = results_df.pivot(index='language', columns='model_name', values='f1_score')\n",
        "    pivot_df.plot(kind='bar', ax=ax, width=0.8)\n",
        "    ax.set_title('Comparative F1-Scores: Model Performance on English vs. isiZulu', fontsize=16)\n",
        "    ax.set_ylabel('F1-Score', fontsize=12)\n",
        "    ax.set_xlabel('Language', fontsize=12)\n",
        "    ax.tick_params(axis='x', rotation=0)\n",
        "    ax.legend(title='Model & Training Strategy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\nPACKAGING TRAINED MODELS FOR DEPLOYMENT\\n\" + \"=\"*80)\n",
        "\n",
        "    dir_path = \"./fineTunedModes\"\n",
        "    if os.path.exists(dir_path):\n",
        "        try:\n",
        "          shutil.make_archive(base_name=dir_path, format='zip', root_dir=dir_path)\n",
        "          print(f\"✅ Zipped '{dir_path}' to '{dir_path}.zip'\")\n",
        "        except Exception as e:\n",
        "          print(f\"❌ Failed to zip '{dir_path}'. Error: {e}\")\n",
        "    else:\n",
        "      print(f\"⚠️ Directory not found, skipping zip: '{dir_path}'\")\n"
      ],
      "metadata": {
        "id": "PP1sfsuOMytD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Executing All Experiments**\n",
        "This is the main execution cell. It calls the `full_project_pipeline` function to run all defined experiments, generate performance metrics, create comparison plots, and package the fine-tuned models for deployment, delivering the primary outcomes of the projec"
      ],
      "metadata": {
        "id": "M2yXwXCadgAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'zulu_dataset' in locals() and 'english_dataset' in locals():\n",
        "  full_project_pipeline()\n",
        "else:\n",
        "  print(\"Data preparation failed or was skipped. Cannot run the main pipeline.\")"
      ],
      "metadata": {
        "id": "cA9qjlySJ8Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interactive Demonstration with the Fine-Tuned Zulu Model**\n",
        "\n",
        "Note: ***This is for quick testing only, just after training. The main prediction will be done in the web interface provided in the project***\n",
        "\n",
        "This final cell provides an interactive command-line interface to test the best-performing model for isiZulu (`castorini/afriberta_base`). It allows a user to input Zulu text and receive a real-time classification of whether it is likely human- or machine-generated, demonstrating a practical application of the research."
      ],
      "metadata": {
        "id": "bK76wj1AeGhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  print(\"\\nStarting interactive session with the fine-tuned AfriBERTa Zulu model...\")\n",
        "  interactive_model_path = \"./fineTunedModes/results_castorini-afriberta_base_zul\"\n",
        "  if os.path.exists(interactive_model_path):\n",
        "      try:\n",
        "        # Load the fine-tuned classifier for interactive use\n",
        "          interactive_classifier = TextClassifier(model_name=interactive_model_path)\n",
        "          print(\"\\nZulu Text Classification Tool\")\n",
        "          print(\"Enter Zulu text to classify. Type 'exit' to quit.\")\n",
        "\n",
        "          while True:\n",
        "              user_input = input(\"\\nEnter Zulu text: \").strip()\n",
        "              if user_input.lower() == \"exit\":\n",
        "                print(\"Exiting...\")\n",
        "                break\n",
        "              if not user_input:\n",
        "                print(\"Please enter some text.\")\n",
        "                continue\n",
        "\n",
        "              prediction = interactive_classifier.predict_single_text(user_input)\n",
        "              print(f\"\\nPrediction for '{user_input}':\")\n",
        "              print(f\"  Predicted: {prediction['predicted_class']} (Confidence: {prediction['confidence']:.4f})\")\n",
        "              print(f\"  Probabilities: Human - {prediction['probabilities']['Human']:.4f}, Machine - {prediction['probabilities']['Machine']:.4f}\")\n",
        "              print(\"-\" * 50)\n",
        "      except Exception as e:\n",
        "          print(f\"\\nCould not start interactive session. An error occurred during model loading: {e}\")\n",
        "  else:\n",
        "      print(f\"\\nCould not start interactive session. Model directory not found at: {interactive_model_path}\")"
      ],
      "metadata": {
        "id": "2ZxsWH72J2qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}