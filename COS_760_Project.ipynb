{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ5xBqdia4v6UkX/KQRp17",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/development/COS_760_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gIBDcyI74-zo",
        "outputId": "faf02b75-7aee-4add-e34b-00372d8344d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "#shona_data = load_dataset(\"DigitalUmuganda/AfriVoice\", \"sn\", streaming=True, split=\"train[:10%]\")\n",
        "\n",
        "\n",
        "english_data = load_dataset(\"oscar-corpus/OSCAR-2201\", language=\"en\", streaming=True)\n",
        "\n",
        "num_samples = 1000\n",
        "samples = []\n",
        "for i, example in enumerate(english_data[\"train\"]):\n",
        "    if i >= num_samples:\n",
        "        break\n",
        "    samples.append(example['text'])\n",
        "\n",
        "eng_df = pd.DataFrame(samples, columns=['text'])\n",
        "#print(eng_df.head())\n",
        "\n",
        "zulu_data = load_dataset(\"dsfsi/vukuzenzele-sentence-aligned\", \"eng-zul\", streaming=True)\n",
        "zul_df = pd.DataFrame(zulu_data[\"train\"])\n",
        "zul_df = zul_df.rename(columns={'__index_level_0__': 'sentence_id'})\n",
        "#print(zul_df.head())"
      ],
      "metadata": {
        "id": "POzIGY7h5Pax"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "eng_df['cleaned_text'] = eng_df['text'].apply(clean_text)\n",
        "eng_df['tokens'] = eng_df['cleaned_text'].apply(word_tokenize)\n",
        "print(eng_df.head())\n",
        "\n",
        "zul_df['cleaned_text'] = zul_df['zul'].apply(clean_text)\n",
        "zul_df['tokens'] = zul_df['cleaned_text'].apply(word_tokenize)\n",
        "print(zul_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tPmKuYUDCOZ",
        "outputId": "83218740-e2d5-412b-fa1f-df297a2b9c71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Founded in 2015, Golden Bees is a leading prog...   \n",
            "1  For your team The Smarter pms Channel Manager ...   \n",
            "2  Importing Viagra To Us, Sildenafil Citrate 100...   \n",
            "3  The topic we will be looking at today is praye...   \n",
            "4  Empire Events | Asian Wedding Services East Lo...   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  founded in golden bees is a leading programmat...   \n",
            "1  for your team the smarter pms channel manager ...   \n",
            "2  importing viagra to us sildenafil citrate mg i...   \n",
            "3  the topic we will be looking at today is praye...   \n",
            "4  empire events asian wedding services east lond...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [founded, in, golden, bees, is, a, leading, pr...  \n",
            "1  [for, your, team, the, smarter, pms, channel, ...  \n",
            "2  [importing, viagra, to, us, sildenafil, citrat...  \n",
            "3  [the, topic, we, will, be, looking, at, today,...  \n",
            "4  [empire, events, asian, wedding, services, eas...  \n",
            "                                                 eng  \\\n",
            "0  what hiv positive breastfeeding moms should kn...   \n",
            "1  it is responsive, because it uses the state’s ...   \n",
            "2  as we transition from relief to recovery, we h...   \n",
            "3  question: my neighbour said that i should get ...   \n",
            "4  using a garden hose could use as much as 30 li...   \n",
            "\n",
            "                                                 zul     score  sentence_id  \\\n",
            "0  okumele kwaziwe ngomama abanehiv abancelisayo ...  0.888614         3720   \n",
            "1  kuyaphenduleka, ngoba kusetshenziswa izinsiza ...  0.885719         1345   \n",
            "2  ekuguqukeni sisuka esigabeni sokudinga usizo s...  0.813556         1659   \n",
            "3  umbuzo: umakhelwane wami uthe kufanele ngithol...  0.901471         3729   \n",
            "4  ukusebe nzisa ipayipi lasengadini kungasebenzi...  0.873329         3690   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  okumele kwaziwe ngomama abanehiv abancelisayo ...   \n",
            "1  kuyaphenduleka ngoba kusetshenziswa izinsiza z...   \n",
            "2  ekuguqukeni sisuka esigabeni sokudinga usizo s...   \n",
            "3  umbuzo umakhelwane wami uthe kufanele ngithole...   \n",
            "4  ukusebe nzisa ipayipi lasengadini kungasebenzi...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [okumele, kwaziwe, ngomama, abanehiv, abanceli...  \n",
            "1  [kuyaphenduleka, ngoba, kusetshenziswa, izinsi...  \n",
            "2  [ekuguqukeni, sisuka, esigabeni, sokudinga, us...  \n",
            "3  [umbuzo, umakhelwane, wami, uthe, kufanele, ng...  \n",
            "4  [ukusebe, nzisa, ipayipi, lasengadini, kungase...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "en_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "\n",
        "english_prompts = [\n",
        "    \"Explain the significance of lobola in Southern Africa\",\n",
        "    \"Write a short dialogue between two friends in Johannesburg\",\n",
        "    \"Describe linguistic features that make isiZulu agglutinative\"\n",
        "]\n",
        "\n",
        "zulu_prompts = [\n",
        "    \"Chaza ngokubaluleka kwesiko lwelobola eNingizimu Afrika\",\n",
        "    \"Bhala inkulumo emfushane phakathi kwabangani ababili eGoli\",\n",
        "    \"Landela indaba yamaZulu ngokomlando\",\n",
        "    \"Chaza ngamasiko amasha eZulu eskhathini samanje\",\n",
        "    \"Bhala inganekwane ethi 'UNogwaja noFudu'\"\n",
        "]\n",
        "\n",
        "def generate_with_prompts(prompts, generator, language, samples_per_prompt=3):\n",
        "    data = []\n",
        "    for prompt in prompts:\n",
        "        for _ in range(samples_per_prompt):\n",
        "            output = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
        "            data.append({\n",
        "                'prompt': prompt,\n",
        "                'text': output[0]['generated_text'],\n",
        "                'label': 'machine',\n",
        "                'language': language,\n",
        "                'prompt_type': 'cultural' if \"tsika\" in prompt else 'linguistic'  # Tag for analysis\n",
        "            })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "eng_mg_df = generate_with_prompts(english_prompts, en_generator, 'English')\n",
        "eng_mg_df['cleaned_text'] = eng_mg_df['text'].apply(clean_text)\n",
        "eng_mg_df['tokens'] = eng_mg_df['cleaned_text'].apply(word_tokenize)\n",
        "print(eng_mg_df['text'].iloc[0][:300])\n",
        "\n",
        "with open('zulu_mg_text.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "zul_mg_df = pd.DataFrame(data)\n",
        "zul_mg_df['text'] = zul_mg_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "zul_mg_df['cleaned_text'] = zul_mg_df['text'].apply(clean_text)\n",
        "zul_mg_df['tokens'] = zul_mg_df['cleaned_text'].apply(word_tokenize)\n",
        "print(zul_mg_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rChZLJ-8E1MT",
        "outputId": "533cf0aa-4f10-4c7e-bef8-fd7c0393be34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain the significance of lobola in Southern Africa from the first and most recent days of the last century, and how they may have provided a useful platform for the colonization of Africa. The following is a list of relevant documents and other resources that have been provided.\n",
            "\n",
            "References:\n",
            "\n",
            "[1]\n",
            "                                              prompt  \\\n",
            "0  Chaza ngokubaluleka kwesiko lwelobola eNingizi...   \n",
            "1  Bhala inkulumo emfushane phakathi kwabangani a...   \n",
            "2                Landela indaba yamaZulu ngokomlando   \n",
            "3    Chaza ngamasiko amasha eZulu eskhathini samanje   \n",
            "4           Bhala inganekwane ethi 'UNogwaja noFudu'   \n",
            "\n",
            "                                                text    label language  \\\n",
            "0  Ilobolo liyisiko elibalulekile eliqinisekisa u...  machine     Zulu   \n",
            "1  Sipho: Sawubona Thabo, kudala singabonani! \\nT...  machine     Zulu   \n",
            "2  AmaZulu aqala ukukhula njengohlanga olukhulu n...  machine     Zulu   \n",
            "3  Ezulwini lanamuhla, amaZulu asebenzisa izindle...  machine     Zulu   \n",
            "4  Kudala-dala, kwakukhona uNogwaja okhanyayo noF...  machine     Zulu   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  ilobolo liyisiko elibalulekile eliqinisekisa u...   \n",
            "1  sipho sawubona thabo kudala singabonani thabo ...   \n",
            "2  amazulu aqala ukukhula njengohlanga olukhulu n...   \n",
            "3  ezulwini lanamuhla amazulu asebenzisa izindlel...   \n",
            "4  kudaladala kwakukhona unogwaja okhanyayo nofud...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [ilobolo, liyisiko, elibalulekile, eliqiniseki...  \n",
            "1  [sipho, sawubona, thabo, kudala, singabonani, ...  \n",
            "2  [amazulu, aqala, ukukhula, njengohlanga, olukh...  \n",
            "3  [ezulwini, lanamuhla, amazulu, asebenzisa, izi...  \n",
            "4  [kudaladala, kwakukhona, unogwaja, okhanyayo, ...  \n"
          ]
        }
      ]
    }
  ]
}