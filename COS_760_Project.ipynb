{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/train-model/COS_760_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gIBDcyI74-zo"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers datasets fsspec evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, Dataset, ClassLabel\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "mfSUUjirjAKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text"
      ],
      "metadata": {
        "id": "V6MdIZgUxRgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shona_data = load_dataset(\"DigitalUmuganda/AfriVoice\", \"sn\", streaming=True, split=\"train[:10%]\")\n",
        "\n",
        "# english_data = load_dataset(\"oscar-corpus/OSCAR-2201\", language=\"en\", streaming=True)\n",
        "\n",
        "# num_samples = 500\n",
        "# samples = []\n",
        "# for i, example in enumerate(english_data[\"train\"]):\n",
        "#     if i >= num_samples:\n",
        "#         break\n",
        "#     samples.append(example['text'])\n",
        "\n",
        "# eng_df = pd.DataFrame(samples, columns=['text'])\n",
        "# eng_df.head()\n",
        "\n",
        "zulu_data = load_dataset(\"dsfsi/vukuzenzele-monolingual\", \"zul\")\n",
        "\n",
        "def reformatData(dataDict):\n",
        "  langList = []\n",
        "  for index, row in dataDict.iterrows():\n",
        "    # Access data using column names from the row Series\n",
        "    if 'text' in row and row['text']:\n",
        "        langList.append({\n",
        "        'text': row.get('text', ''),\n",
        "        'author': row.get('author', ''),\n",
        "        'title': row.get('title', ''),\n",
        "        'language': 'zul'\n",
        "              })\n",
        "\n",
        "  return langList\n",
        "\n",
        "human_data = reformatData(zulu_data['train'].to_pandas())\n",
        "\n",
        "zul_df = pd.DataFrame(human_data)\n",
        "zul_df['cleaned_text'] = zul_df['text'].apply(clean_text)\n",
        "zul_df['tokens'] = zul_df['cleaned_text'].apply(word_tokenize)\n",
        "zul_df['label'] = 0\n",
        "zul_df.head()"
      ],
      "metadata": {
        "id": "POzIGY7h5Pax",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eng_df['cleaned_text'] = eng_df['text'].apply(clean_text)\n",
        "# eng_df['tokens'] = eng_df['cleaned_text'].apply(word_tokenize)\n",
        "# print(eng_df.head())\n",
        "\n",
        "zul_df['cleaned_text'] = zul_df['text'].apply(clean_text)\n",
        "zul_df['tokens'] = zul_df['cleaned_text'].apply(word_tokenize)\n",
        "zul_df.head()"
      ],
      "metadata": {
        "id": "-tPmKuYUDCOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "en_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "\n",
        "english_prompts = [\n",
        "    \"Explain the significance of lobola in Southern Africa\",\n",
        "    \"Write a short dialogue between two friends in Johannesburg\",\n",
        "    \"Describe linguistic features that make isiZulu agglutinative\"\n",
        "]\n",
        "\n",
        "zulu_prompts = [\n",
        "    \"Chaza ngokubaluleka kwesiko lwelobola eNingizimu Afrika\",\n",
        "    \"Bhala inkulumo emfushane phakathi kwabangani ababili eGoli\",\n",
        "    \"Landela indaba yamaZulu ngokomlando\",\n",
        "    \"Chaza ngamasiko amasha eZulu eskhathini samanje\",\n",
        "    \"Bhala inganekwane ethi 'UNogwaja noFudu'\"\n",
        "]\n",
        "\n",
        "# def generate_with_prompts(prompts, generator, language, samples_per_prompt=3):\n",
        "#     data = []\n",
        "#     for prompt in prompts:\n",
        "#         for _ in range(samples_per_prompt):\n",
        "#             output = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
        "#             data.append({\n",
        "#                 'prompt': prompt,\n",
        "#                 'text': output[0]['generated_text'],\n",
        "#                 'label': 'machine',\n",
        "#                 'language': language,\n",
        "#                 'prompt_type': 'cultural' if \"tsika\" in prompt else 'linguistic'  # Tag for analysis\n",
        "#             })\n",
        "#     return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# eng_mg_df = generate_with_prompts(english_prompts, en_generator, 'English')\n",
        "# eng_mg_df['cleaned_text'] = eng_mg_df['text'].apply(clean_text)\n",
        "# eng_mg_df['tokens'] = eng_mg_df['cleaned_text'].apply(word_tokenize)\n",
        "# print(eng_mg_df['text'].iloc[0][:300])\n",
        "\n",
        "def load_jsonl_data(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:  # Skip empty lines\n",
        "                    try:\n",
        "                        data.append(json.loads(line))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"Error parsing JSON line: {e}\")\n",
        "                        continue\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return []\n",
        "\n",
        "    return data\n",
        "\n",
        "zul_mg_df = pd.DataFrame(load_jsonl_data('zulu_mg_text.json'))\n",
        "\n",
        "machine_data =reformatData(zul_mg_df)\n",
        "zul_mg_df = pd.DataFrame(machine_data)\n",
        "zul_mg_df['text'] = zul_mg_df['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "zul_mg_df['language'] = 'zul'\n",
        "zul_mg_df['cleaned_text'] = zul_mg_df['text'].apply(clean_text)\n",
        "zul_mg_df['tokens'] = zul_mg_df['cleaned_text'].apply(word_tokenize)\n",
        "zul_mg_df['label'] = 1\n",
        "\n",
        "zul_mg_df.head()"
      ],
      "metadata": {
        "id": "rChZLJ-8E1MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMBINE THE DATASETS\n",
        "-------------------------"
      ],
      "metadata": {
        "id": "iIqYgsMmtYI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both datasets, this will be bad for us if we have more than 70% differnce in data length\n",
        "# Combine both datasets\n",
        "all_texts = pd.concat([zul_df, zul_mg_df], ignore_index=True)\n",
        "all_texts_shuffled = all_texts.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Total texts after combining: {len(all_texts)}\")\n",
        "print(f\"Human-written texts: {len(zul_df)}\")\n",
        "print(f\"Machine-generated texts: {len(zul_mg_df)}\")\n",
        "\n",
        "# Create a DataFrame for easier analysis\n",
        "combined_df = pd.DataFrame(all_texts_shuffled)\n",
        "combined_df.head()"
      ],
      "metadata": {
        "id": "XdcTeCfvtXhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a balanced dataset with equal samples from each class\n",
        "# we can choose to use this of the combined one\n",
        "def create_balanced_dataset(df, target_size_per_class=None):\n",
        "    class_counts = df['label'].value_counts()\n",
        "    min_class_size = class_counts.min()\n",
        "    if target_size_per_class:\n",
        "        sample_size = min(target_size_per_class, min_class_size)\n",
        "    else:\n",
        "        sample_size = min_class_size\n",
        "    balanced_df = df.groupby('label').sample(n=sample_size, random_state=42)\n",
        "    return balanced_df.reset_index(drop=True)\n",
        "\n",
        "# Create balanced dataset, because we need the same number of samples for each class\n",
        "balanced_df = create_balanced_dataset(combined_df)\n",
        "print(f\"\\nBalanced dataset created with {len(balanced_df)} samples\")\n",
        "print(f\"Label distribution in balanced dataset: \\n{balanced_df['label'].value_counts()}\")\n",
        "\n",
        "# Save balanced dataset\n",
        "balanced_output_path = 'balanced_zulu_texts.csv'\n",
        "balanced_df.to_csv(balanced_output_path, index=False, encoding='utf-8')\n",
        "print(f\"Balanced dataset saved to: {balanced_output_path}\")\n",
        "balanced_df.head()"
      ],
      "metadata": {
        "id": "gwRW8Jlxt66B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training section\n",
        "----------------------"
      ],
      "metadata": {
        "id": "MfJh3HL9K57I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the balanced dataset\n",
        "balanced_data_path = 'balanced_zulu_texts.csv'\n",
        "df = pd.read_csv(balanced_data_path, encoding='utf-8')\n",
        "\n",
        "# Convert pandas DataFrame to Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df[['text', 'label']])\n",
        "\n",
        "# Cast the 'label' column to ClassLabel\n",
        "dataset = dataset.cast_column('label', ClassLabel(num_classes=2, names=['human', 'machine']))\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_val_split = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
        "train_dataset = train_val_split['train']\n",
        "val_dataset = train_val_split['test']\n",
        "\n",
        "# Load AfroXLMR tokenizer\n",
        "model_name = \"Davlan/afro-xlmr-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# Load AfroXLMR model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Define metrics for evaluation\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    return {\n",
        "        'accuracy': accuracy['accuracy'],\n",
        "        'f1': f1['f1']\n",
        "    }\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/afroxlmr_finetuned',\n",
        "    run_name='/afroxlmr_finetune_zulu',  # too much warnings so better to use a custom run name\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=torch.cuda.is_available(),  # Enable mixed precision only if GPU is available\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained('/afroxlmr_finetuned/model')\n",
        "tokenizer.save_pretrained('/afroxlmr_finetuned/tokenizer')\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ],
      "metadata": {
        "id": "yS1jyhV4LAWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}