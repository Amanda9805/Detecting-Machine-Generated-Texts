{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/train-model/Afriberta_model2Combined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iLt-nfdZlc0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers adapters datasets fsspec evaluate shap nltk lime textstat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import gc\n",
        "import warnings\n",
        "import torch\n",
        "import nltk\n",
        "import evaluate\n",
        "import shap\n",
        "import lime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from textstat import flesch_reading_ease, automated_readability_index\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, Dataset, ClassLabel, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# Initial setup\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "wtt7e2QUaL-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic text cleaning\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load machine-generated text from a JSONL file\n",
        "def load_machine_text(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line))\n",
        "        df = pd.DataFrame(data)\n",
        "        # Handle cases where text is a list of strings\n",
        "        if not df.empty and isinstance(df['text'].iloc[0], list):\n",
        "            df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Machine-generated text file not found at {file_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load, process and balance data for a given language\n",
        "def prepare_dataset(lang_code, human_data_source, machine_data_path, output_csv_path):\n",
        "    print(f\"Preparing dataset for language: {lang_code.upper()}\")\n",
        "\n",
        "    # Load Human Data (Label = 0)\n",
        "    human_dataset = load_dataset(human_data_source, lang_code)\n",
        "\n",
        "    human_df = human_dataset['train'].to_pandas()\n",
        "    human_df = human_df[['text']].dropna()\n",
        "    human_df['label'] = 0\n",
        "    human_df['language'] = lang_code\n",
        "    print(f\"Loaded {len(human_df)} human-written data.\")\n",
        "\n",
        "    # Load Machine Data (Label = 1)\n",
        "    machine_df = load_machine_text(machine_data_path)\n",
        "    if machine_df.empty:\n",
        "        return None\n",
        "    machine_df = machine_df[['text']].dropna()\n",
        "    machine_df['label'] = 1\n",
        "    machine_df['language'] = lang_code\n",
        "    print(f\"Loaded {len(machine_df)} machine-generated data.\")\n",
        "\n",
        "    # Combine and Clean\n",
        "    combined_df = pd.concat([human_df, machine_df], ignore_index=True)\n",
        "    combined_df['text'] = combined_df['text'].apply(clean_text)\n",
        "    combined_df.dropna(subset=['text'], inplace=True)\n",
        "    # combined_df = combined_df[combined_df['text'].str.len() > 10] # Optional: Remove very short texts\n",
        "\n",
        "    # Create Balanced Dataset\n",
        "    min_class_size = combined_df['label'].value_counts().min()\n",
        "    balanced_df = combined_df.groupby('label').sample(n=min_class_size, random_state=42)\n",
        "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Created a balanced dataset with {len(balanced_df)} total samples ({min_class_size} per class).\")\n",
        "\n",
        "    # Save and return dataset\n",
        "    balanced_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "    print(f\"Balanced dataset saved to {output_csv_path}\")\n",
        "\n",
        "    return balanced_df"
      ],
      "metadata": {
        "id": "tszWIg-UaNON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy data files if they don't exist, as they are loaded in the notebook\n",
        "if not os.path.exists('zulu_mg_text.jsonl'):\n",
        "    with open('zulu_mg_text.jsonl', 'w') as f:\n",
        "        f.write('{\"text\": \"Lokhu umbhalo owenziwe ngomshini mayelana namasiko akwaZulu.\"}\\n')\n",
        "        f.write('{\"text\": \"UNogwaja noFudu babengabangani abakhulu ehlathini.\"}\\n')\n",
        "\n",
        "if not os.path.exists('eng_mg_data.jsonl'):\n",
        "    with open('eng_mg_data.jsonl', 'w') as f:\n",
        "        f.write('{\"text\": \"This is machine-generated text about South African culture.\"}\\n')\n",
        "        f.write('{\"text\": \"The quick brown fox jumps over the lazy dog in Johannesburg.\"}\\n')\n",
        "\n",
        "# Prepare isiZulu Dataset\n",
        "zulu_dataset = prepare_dataset(\n",
        "    lang_code='zul',\n",
        "    human_data_source='dsfsi/vukuzenzele-monolingual',\n",
        "    machine_data_path='zulu_mg_text.jsonl',\n",
        "    output_csv_path='balanced_zulu_texts.csv'\n",
        ")\n",
        "\n",
        "# Prepare English Dataset\n",
        "english_dataset = prepare_dataset(\n",
        "    lang_code='eng',\n",
        "    human_data_source='dsfsi/vukuzenzele-monolingual',\n",
        "    machine_data_path='eng_mg_data.jsonl',\n",
        "    output_csv_path='balanced_english_texts.csv'\n",
        ")\n",
        "\n",
        "print(\"\\nZulu Dataset Sample:\")\n",
        "print(pd.DataFrame(zulu_dataset).head())\n",
        "print(\"\\nEnglish Dataset Sample:\")\n",
        "print(pd.DataFrame(english_dataset).head())"
      ],
      "metadata": {
        "id": "VxdsO_ltamqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ZuluTextClassifier:\n",
        "    def __init__(self, model_name='castorini/afriberta_base', max_length=512):\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "        self.max_length = max_length\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        print(f\"Model initialized on {self.device}\")\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        X = df['text'].tolist()\n",
        "        y = LabelEncoder().fit_transform(df['label'])\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_datasets(self, X_train, X_test, y_train, y_test):\n",
        "        train_encodings = self.tokenizer(X_train, truncation=True, padding=True, max_length=self.max_length)\n",
        "        test_encodings = self.tokenizer(X_test, truncation=True, padding=True, max_length=self.max_length)\n",
        "\n",
        "        train_data = {\n",
        "            'input_ids': train_encodings['input_ids'],\n",
        "            'attention_mask': train_encodings['attention_mask'],\n",
        "            'labels': y_train\n",
        "        }\n",
        "        test_data = {\n",
        "            'input_ids': test_encodings['input_ids'],\n",
        "            'attention_mask': test_encodings['attention_mask'],\n",
        "            'labels': y_test\n",
        "        }\n",
        "\n",
        "        train_dataset = Dataset.from_dict(train_data)\n",
        "        test_dataset = Dataset.from_dict(test_data)\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    def setup_lora(self, use_lora=True):\n",
        "        if use_lora:\n",
        "            print(\"Setting up LoRA configuration...\")\n",
        "            lora_config = LoraConfig(\n",
        "                r=16,\n",
        "                lora_alpha=32,\n",
        "                target_modules=[\"query\", \"key\", \"value\"],\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\",\n",
        "                task_type=\"SEQ_CLS\"\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            print(f\"LoRA enabled. Trainable parameters: {self.model.num_parameters()}\")\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset):\n",
        "        self.setup_lora(use_lora=True)\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./classifier_results',\n",
        "            num_train_epochs=5,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            learning_rate=5e-5,\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            tokenizer=self.tokenizer\n",
        "        )\n",
        "        print(\"Starting training...\")\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained('./classifier_results')\n",
        "        return trainer\n",
        "\n",
        "    def evaluate(self, test_dataset, trainer=None):\n",
        "        if trainer is None:\n",
        "            trainer = Trainer(model=self.model)\n",
        "\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = predictions.label_ids\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average=None\n",
        "        )\n",
        "\n",
        "        results = {\n",
        "            'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1,\n",
        "            'precision_per_class': precision_per_class, 'recall_per_class': recall_per_class,\n",
        "            'f1_per_class': f1_per_class, 'support': support,\n",
        "            'y_true': y_true, 'y_pred': y_pred\n",
        "        }\n",
        "        return results\n",
        "\n",
        "    def print_evaluation_results(self, results):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Overall Accuracy: {results['accuracy']:.4f}\")\n",
        "        print(f\"Weighted Precision: {results['precision']:.4f}\")\n",
        "        print(f\"Weighted Recall: {results['recall']:.4f}\")\n",
        "        print(f\"Weighted F1-Score: {results['f1']:.4f}\")\n",
        "        print(\"\\nPer-Class Results:\")\n",
        "\n",
        "        class_names = ['Human', 'Machine']\n",
        "\n",
        "        for i in range(len(class_names)):\n",
        "            print(f\"{class_names[i]}:\")\n",
        "            print(f\"  Precision: {results['precision_per_class'][i]:.4f}\")\n",
        "            print(f\"  Recall: {results['recall_per_class'][i]:.4f}\")\n",
        "            print(f\"  F1-Score: {results['f1_per_class'][i]:.4f}\")\n",
        "            print(f\"  Support: {results['support'][i]}\")\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(results['y_true'], results['y_pred'], target_names=class_names))\n",
        "\n",
        "    def predict_single_text(self, text):\n",
        "        self.model.eval()\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encoding)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = predictions.max().item()\n",
        "\n",
        "        class_names = ['Human', 'Machine']\n",
        "\n",
        "        return {\n",
        "            'predicted_class': class_names[predicted_class], 'confidence': confidence,\n",
        "            'probabilities': {'Human': predictions[0][0].item(), 'Machine': predictions[0][1].item()}\n",
        "        }"
      ],
      "metadata": {
        "id": "xQebwWyhasme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextExplainabilityAnalyzer:\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.class_names = ['Human', 'Machine']\n",
        "        self.lime_explainer = LimeTextExplainer(class_names=self.class_names)\n",
        "\n",
        "    def predict_proba_for_lime(self, texts):\n",
        "        predictions = []\n",
        "        self.model.eval()\n",
        "        for text in texts:\n",
        "            encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "            encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoding)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predictions.append(probs.cpu().numpy()[0])\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def explain_with_lime(self, text, num_features=10):\n",
        "        explanation = self.lime_explainer.explain_instance(\n",
        "            text, self.predict_proba_for_lime, num_features=num_features, num_samples=500\n",
        "        )\n",
        "        return explanation\n",
        "\n",
        "    def explain_with_shap(self, texts, background_texts=None):\n",
        "        def model_wrapper(texts):\n",
        "            return self.predict_proba_for_lime(texts)\n",
        "        if background_texts is None:\n",
        "            background_texts = texts[:20]\n",
        "        explainer = shap.Explainer(model_wrapper, background_texts)\n",
        "        shap_values = explainer(texts)\n",
        "        return shap_values\n",
        "\n",
        "    def analyze_feature_importance(self, test_texts, test_labels, sample_ratio=0.3):\n",
        "        results = {'human_features': [], 'machine_features': [], 'common_patterns': {}}\n",
        "        total_samples = len(test_texts)\n",
        "        num_samples = int(total_samples * sample_ratio)\n",
        "        print(f\"Analyzing {num_samples} samples ({sample_ratio*100:.0f}%) out of {total_samples} total samples\")\n",
        "        sample_indices, _ = train_test_split(range(len(test_texts)), test_size=1-sample_ratio, stratify=test_labels, random_state=42)\n",
        "        print(f\"Selected {len(sample_indices)} samples for analysis\")\n",
        "        for i, idx in enumerate(tqdm(sample_indices, desc=\"Generating LIME explanations\")):\n",
        "            text, true_label = test_texts[idx], test_labels[idx]\n",
        "            if not text or text.isspace():\n",
        "                continue\n",
        "            try:\n",
        "                explanation = self.explain_with_lime(text)\n",
        "                features = explanation.as_list()\n",
        "                if true_label == 0:\n",
        "                    results['human_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "                else:\n",
        "                    results['machine_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating LIME explanation for text at index {idx}: {e}\")\n",
        "        results['human_patterns'] = Counter(results['human_features']).most_common(20)\n",
        "        results['machine_patterns'] = Counter(results['machine_features']).most_common(20)\n",
        "        return results\n",
        "\n",
        "    def linguistic_feature_analysis(self, texts, labels, sample_ratio=0.3):\n",
        "        total_samples = len(texts)\n",
        "        num_samples = int(total_samples * sample_ratio)\n",
        "        sample_indices, _ = train_test_split(range(len(texts)), test_size=1-sample_ratio, stratify=labels, random_state=42)\n",
        "        sampled_texts, sampled_labels = [texts[i] for i in sample_indices], [labels[i] for i in sample_indices]\n",
        "\n",
        "        print(f\"Running linguistic analysis on {len(sampled_texts)} samples ({sample_ratio*100:.0f}%)\")\n",
        "\n",
        "        features = {'avg_sentence_length': [], 'lexical_diversity': [], 'repetition_score': []}\n",
        "\n",
        "        for text in tqdm(sampled_texts, desc=\"Computing linguistic features\"):\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            avg_sent_len = np.mean([len(s.split()) for s in sentences if s.strip()])\n",
        "            features['avg_sentence_length'].append(avg_sent_len)\n",
        "            words = text.lower().split()\n",
        "            lexical_div = len(set(words)) / len(words) if words else 0\n",
        "            features['lexical_diversity'].append(lexical_div)\n",
        "            word_counts = Counter(words)\n",
        "            repetition = sum(count - 1 for count in word_counts.values()) / len(words) if words else 0\n",
        "            features['repetition_score'].append(repetition)\n",
        "\n",
        "        human_features = {k: [v[i] for i, label in enumerate(sampled_labels) if label == 0] for k, v in features.items()}\n",
        "        machine_features = {k: [v[i] for i, label in enumerate(sampled_labels) if label == 1] for k, v in features.items()}\n",
        "\n",
        "        return human_features, machine_features, features\n",
        "\n",
        "    def plot_feature_distributions(self, human_features, machine_features):\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5)) # Adjusted for 3 features\n",
        "        axes = axes.ravel()\n",
        "        feature_names = list(human_features.keys())\n",
        "\n",
        "        for i, feature in enumerate(feature_names):\n",
        "            ax = axes[i]\n",
        "            ax.hist(human_features[feature], alpha=0.7, label='Human', bins=20)\n",
        "            ax.hist(machine_features[feature], alpha=0.7, label='Machine', bins=20)\n",
        "            ax.set_title(feature.replace('_', ' ').title())\n",
        "            ax.set_xlabel('Value'); ax.set_ylabel('Frequency'); ax.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def error_analysis(self, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "        all_errors = [{'text': test_texts[i], 'true_label': self.class_names[true_label], 'predicted_label': self.class_names[pred_label], 'index': i} for i, (true_label, pred_label) in enumerate(zip(test_labels, predictions)) if true_label != pred_label]\n",
        "        num_errors_to_analyze = max(1, int(len(all_errors) * sample_ratio))\n",
        "        errors_to_analyze = np.random.choice(all_errors, min(num_errors_to_analyze, len(all_errors)), replace=False) if all_errors else []\n",
        "        print(f\"Found {len(all_errors)} total errors, analyzing {len(errors_to_analyze)} in detail\")\n",
        "        error_analysis = {\n",
        "            'total_errors': len(all_errors), 'false_positives': len([e for e in all_errors if e['true_label'] == 'Human']),\n",
        "            'false_negatives': len([e for e in all_errors if e['true_label'] == 'Machine']), 'examples': all_errors[:10]\n",
        "        }\n",
        "\n",
        "        error_explanations = []\n",
        "        for error in tqdm(errors_to_analyze[:5], desc=\"Generating error explanations\"):\n",
        "            try:\n",
        "                explanation = self.explain_with_lime(error['text'])\n",
        "                error_explanations.append({'error': error, 'explanation': explanation.as_list()})\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating explanation: {e}\")\n",
        "\n",
        "        error_analysis['explanations'] = error_explanations\n",
        "        return error_analysis\n",
        "\n",
        "    def generate_explanation_report(self, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "        print(\"=\"*60 + \"\\nMODEL EXPLAINABILITY ANALYSIS REPORT\\n\" + f\"Analyzing {sample_ratio*100:.0f}% of data for efficiency\\n\" + \"=\"*60)\n",
        "        print(\"\\n1. FEATURE IMPORTANCE ANALYSIS\\n\" + \"-\" * 40)\n",
        "\n",
        "        feature_analysis = self.analyze_feature_importance(test_texts, test_labels, sample_ratio)\n",
        "\n",
        "        print(\"Top features indicating HUMAN text:\"); [print(f\"  - '{f}': {c} occurrences\") for f, c in feature_analysis['human_patterns'][:10]]\n",
        "        print(\"\\nTop features indicating MACHINE text:\"); [print(f\"  - '{f}': {c} occurrences\") for f, c in feature_analysis['machine_patterns'][:10]]\n",
        "        print(\"\\n2. LINGUISTIC FEATURE ANALYSIS\\n\" + \"-\" * 40)\n",
        "\n",
        "        human_features, machine_features, _ = self.linguistic_feature_analysis(test_texts, test_labels, sample_ratio)\n",
        "        for name in human_features.keys():\n",
        "            print(f\"{name.replace('_', ' ').title()}:\\n  Human avg: {np.mean(human_features[name]):.3f}, Machine avg: {np.mean(machine_features[name]):.3f}\")\n",
        "\n",
        "        print(\"\\n3. ERROR ANALYSIS\\n\" + \"-\" * 40)\n",
        "\n",
        "        error_analysis_res = self.error_analysis(test_texts, test_labels, predictions, sample_ratio)\n",
        "\n",
        "        print(f\"Total classification errors: {error_analysis_res['total_errors']}\\nFalse positives (Human → Machine): {error_analysis_res['false_positives']}\\nFalse negatives (Machine → Human): {error_analysis_res['false_negatives']}\")\n",
        "        print(\"\\nExample misclassifications:\")\n",
        "\n",
        "        for i, ex in enumerate(error_analysis_res['examples'][:3]):\n",
        "            print(f\"  {i+1}. True: {ex['true_label']}, Predicted: {ex['predicted_label']}\\n     Text: {ex['text'][:100]}...\")\n",
        "\n",
        "        return {'feature_analysis': feature_analysis, 'linguistic_analysis': (human_features, machine_features), 'error_analysis': error_analysis_res}\n",
        "\n",
        "def add_explainability_to_classifier(classifier, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "    explainer = TextExplainabilityAnalyzer(classifier.model, classifier.tokenizer, classifier.device)\n",
        "    report = explainer.generate_explanation_report(test_texts, test_labels, predictions, sample_ratio)\n",
        "\n",
        "    human_features, machine_features = report['linguistic_analysis']\n",
        "\n",
        "    fig = explainer.plot_feature_distributions(human_features, machine_features)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return explainer, report"
      ],
      "metadata": {
        "id": "meyYJxXjbprq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# general-purpose class\n",
        "class TextClassifier(ZuluTextClassifier):\n",
        "    pass\n",
        "\n",
        "def run_experiment(model_name, dataset_df, language_name, run_explainability=False):\n",
        "    \"\"\"Runs a complete fine-tuning and evaluation experiment for our datasets.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + f\"\\nSTARTING EXPERIMENT: Model='{model_name}', Language='{language_name}'\\n\" + \"=\"*80)\n",
        "\n",
        "    classifier = TextClassifier(model_name=model_name)\n",
        "    X_train, X_test, y_train, y_test = classifier.prepare_data(dataset_df)\n",
        "\n",
        "    train_dataset, test_dataset = classifier.create_datasets(X_train, X_test, y_train, y_test)\n",
        "    trainer = classifier.train(train_dataset, test_dataset)\n",
        "\n",
        "    results = classifier.evaluate(test_dataset, trainer)\n",
        "    classifier.print_evaluation_results(results)\n",
        "\n",
        "    if run_explainability:\n",
        "        print(\"\\n--- Running Explainability Analysis ---\")\n",
        "        add_explainability_to_classifier(classifier, X_test, y_test, results['y_pred'], sample_ratio=0.2)\n",
        "\n",
        "    del classifier, trainer, train_dataset, test_dataset\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\"model_name\": model_name, \"language\": language_name, \"accuracy\": results['accuracy'], \"f1_score\": results['f1'], \"precision\": results['precision'], \"recall\": results['recall']}\n",
        "\n",
        "def run_zero_shot_experiment(model_name, source_df, target_df):\n",
        "    \"\"\"\n",
        "    Trains a model on a source language and evaluates it on a target language (zero-shot).\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"STARTING ZERO-SHOT EXPERIMENT: Model='{model_name}', Source='English', Target='isiZulu'\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = TextClassifier(model_name=model_name)\n",
        "\n",
        "    # Prepare source data for training\n",
        "    X_train_src, X_test_src, y_train_src, y_test_src = classifier.prepare_data(source_df)\n",
        "    train_dataset_src, eval_dataset_src = classifier.create_datasets(X_train_src, X_test_src, y_train_src, y_test_src)\n",
        "\n",
        "    # Train on source language (English)\n",
        "    print(\"\\n--- Training on source language (English) ---\")\n",
        "    trainer = classifier.train(train_dataset_src, eval_dataset_src)\n",
        "\n",
        "    # Prepare target data for evaluation\n",
        "    print(\"\\n--- Evaluating on target language (isiZulu) ---\")\n",
        "    _ , X_test_tgt, _, y_test_tgt = classifier.prepare_data(target_df)\n",
        "\n",
        "    # Directly create the target test dataset\n",
        "    target_encodings = classifier.tokenizer(X_test_tgt, truncation=True, padding=True, max_length=classifier.max_length)\n",
        "    target_test_data = {\n",
        "        'input_ids': target_encodings['input_ids'],\n",
        "        'attention_mask': target_encodings['attention_mask'],\n",
        "        'labels': y_test_tgt\n",
        "    }\n",
        "    test_dataset_tgt = Dataset.from_dict(target_test_data)\n",
        "\n",
        "    # Evaluate on target language (Zulu)\n",
        "    results = classifier.evaluate(test_dataset_tgt, trainer)\n",
        "    classifier.print_evaluation_results(results)\n",
        "\n",
        "    # Clean up\n",
        "    del classifier, trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"model_name\": f\"{model_name} (Zero-Shot)\",\n",
        "        \"language\": \"zul\",\n",
        "        \"accuracy\": results['accuracy'],\n",
        "        \"f1_score\": results['f1'],\n",
        "        \"precision\": results['precision'],\n",
        "        \"recall\": results['recall'],\n",
        "    }\n",
        "\n",
        "def full_project_pipeline():\n",
        "    \"\"\"Main function to run the entire pipeline as described in the proposal.\"\"\"\n",
        "\n",
        "    MODELS_TO_TEST = ['castorini/afriberta_base', 'xlm-roberta-base']\n",
        "    DATASETS = {\"zul\": zulu_dataset, \"eng\": english_dataset}\n",
        "    all_results = []\n",
        "\n",
        "    for lang_code, df in DATASETS.items():\n",
        "        for model in MODELS_TO_TEST:\n",
        "            is_primary_exp = (lang_code == 'zul' and model == 'castorini/afriberta_base')\n",
        "            result = run_experiment(model, df, lang_code, run_explainability=is_primary_exp)\n",
        "            all_results.append(result)\n",
        "\n",
        "    for model in MODELS_TO_TEST:\n",
        "        zero_shot_result = run_zero_shot_experiment(model, DATASETS['eng'], DATASETS['zul'])\n",
        "        all_results.append(zero_shot_result)\n",
        "\n",
        "    print(\"\\n\\n\" + \"#\"*80 + \"\\n\" + \" \" * 20 + \"FINAL COMPARATIVE ANALYSIS REPORT\\n\" + \"#\"*80)\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    print(\"\\n--- Performance Metrics Across All Experiments ---\")\n",
        "    print(results_df)\n",
        "\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    pivot_df = results_df.pivot(index='language', columns='model_name', values='f1_score')\n",
        "    pivot_df.plot(kind='bar', ax=ax, width=0.8)\n",
        "    ax.set_title('Comparative F1-Scores: Model Performance on English vs. isiZulu', fontsize=16)\n",
        "    ax.set_ylabel('F1-Score', fontsize=12); ax.set_xlabel('Language', fontsize=12)\n",
        "    ax.tick_params(axis='x', rotation=0); ax.legend(title='Model & Training Strategy')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "if 'zulu_dataset' in locals() and 'english_dataset' in locals():\n",
        "  full_project_pipeline()\n",
        "else:\n",
        "  print(\"Data preparation failed or was skipped. Cannot run the main pipeline.\")"
      ],
      "metadata": {
        "id": "sme31G-DbzoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # The interactive CLI part can be run after the main pipeline finishes\n",
        "    print(\"\\nStarting interactive session...\")\n",
        "\n",
        "    try:\n",
        "        # Load the last saved model and tokenizer for interaction\n",
        "        final_classifier = TextClassifier()\n",
        "        final_classifier.model = AutoModelForSequenceClassification.from_pretrained('./classifier_results')\n",
        "        final_classifier.tokenizer = AutoTokenizer.from_pretrained('./classifier_results')\n",
        "        final_classifier.model.to(final_classifier.device)\n",
        "\n",
        "        print(\"Zulu Text Classification Tool\")\n",
        "        print(\"Enter Zulu text to classify. Type 'exit' to quit.\")\n",
        "        while True:\n",
        "            user_input = input(\"\\nEnter Zulu text: \").strip()\n",
        "            if user_input.lower() == \"exit\":\n",
        "                print(\"Exiting...\")\n",
        "                break\n",
        "            if not user_input:\n",
        "                print(\"Please enter some text.\")\n",
        "                continue\n",
        "            prediction = final_classifier.predict_single_text(user_input)\n",
        "            print(f\"\\nPrediction for '{user_input}':\")\n",
        "            print(f\"Predicted: {prediction['predicted_class']} (Confidence: {prediction['confidence']:.4f})\")\n",
        "            print(f\"Probabilities: Human - {prediction['probabilities']['Human']:.4f}, Machine - {prediction['probabilities']['Machine']:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCould not start interactive session. No model found or an error occurred: {e}\")"
      ],
      "metadata": {
        "id": "uWG9Q_cFb6h6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}