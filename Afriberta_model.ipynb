{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsNHnIxBSGxHjpJ6irLCa2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/train-model/Afriberta_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DCT-EdJT5TS"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets fsspec\n",
        "!pip install shap lime textstat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_and_reformat_data(df, text_column='text', language='zul', clean_text_flag=True, label=None):\n",
        "    \"\"\"\n",
        "    Returns a cleaned DataFrame with:\n",
        "    - Original text ('text')\n",
        "    - Cleaned text ('cleaned_text')\n",
        "    - Word tokens ('tokens')\n",
        "    - Language label ('language')\n",
        "    - Optional class label ('label')\n",
        "    \"\"\"\n",
        "\n",
        "    def clean_text(text):\n",
        "        \"\"\"Cleans text while preserving original\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        cleaned = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        cleaned = re.sub(r'[^a-zA-Z\\s]', '', cleaned)\n",
        "        cleaned = cleaned.lower()\n",
        "        cleaned = ' '.join(cleaned.split())\n",
        "        return cleaned\n",
        "\n",
        "    result = {\n",
        "        'text': [],\n",
        "        'cleaned_text': [],\n",
        "        'tokens': [],\n",
        "        'language': [],\n",
        "        'label': []\n",
        "    }\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Get raw text\n",
        "        raw_text = row.get(text_column, '') or row.get('answers', '')\n",
        "\n",
        "        # Skip empty entries\n",
        "        if pd.isna(raw_text) or raw_text == '' or (isinstance(raw_text, (list, tuple)) and len(raw_text) == 0):\n",
        "            continue\n",
        "\n",
        "        # Handle list-type text\n",
        "        if isinstance(raw_text, (list, tuple)):\n",
        "            raw_text = ' '.join([str(t).strip() for t in raw_text if str(t).strip()])\n",
        "            if not raw_text:\n",
        "                continue\n",
        "\n",
        "        # Clean text\n",
        "        cleaned = clean_text(raw_text) if clean_text_flag else raw_text\n",
        "        if not cleaned.strip():\n",
        "            continue\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(cleaned)\n",
        "\n",
        "        # Store results\n",
        "        result['text'].append(raw_text)\n",
        "        result['cleaned_text'].append(cleaned)\n",
        "        result['tokens'].append(tokens)\n",
        "        result['language'].append(language)\n",
        "        result['label'].append(label if label is not None else row.get('label', None))\n",
        "\n",
        "    return pd.DataFrame(result)"
      ],
      "metadata": {
        "id": "QuHPtQP5CWb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from huggingface_hub import login\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "#shona_data = load_dataset(\"DigitalUmuganda/AfriVoice\", \"sn\", streaming=True, split=\"train[:10%]\")\n",
        "\n",
        "eng_data = load_dataset(\"dsfsi/vukuzenzele-monolingual\", \"eng\")\n",
        "zulu_data = load_dataset(\"dsfsi/vukuzenzele-monolingual\", \"zul\")\n",
        "zulu_data2 = load_dataset(\"masakhane/afriqa\", \"zul\")\n",
        "\n",
        "combined_eng = pd.concat([\n",
        "    eng_data['train'].to_pandas(),\n",
        "    eng_data['test'].to_pandas()\n",
        "])\n",
        "\n",
        "combined_data = pd.concat([\n",
        "    zulu_data['train'].to_pandas(),\n",
        "    zulu_data['test'].to_pandas()\n",
        "])\n",
        "\n",
        "combined_data2 = pd.concat([\n",
        "    zulu_data2['train'].to_pandas(),\n",
        "    zulu_data2['validation'].to_pandas(),\n",
        "    zulu_data2['test'].to_pandas()\n",
        "])\n",
        "\n",
        "eng_df = clean_and_reformat_data(combined_eng, text_column='text', language='eng', clean_text_flag=True, label=0)\n",
        "zul_df1 = clean_and_reformat_data(combined_data, text_column='text', language='zul', clean_text_flag=True, label=0)\n",
        "zul_df2 = clean_and_reformat_data(combined_data2, text_column='answer', language='zul', clean_text_flag=True, label=0)\n",
        "\n",
        "zul_df = pd.concat([zul_df1, zul_df2], ignore_index=True)\n",
        "zul_df.head()\n",
        "eng_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "n8iYGKo6Mpgn",
        "outputId": "117728e6-a4e6-42ea-e7c6-b87497e1d442"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  The Special Investigation Unit’s (SIU) final r...   \n",
              "1  Sumitomo manufactures the popular Dunlop, Sumi...   \n",
              "2  According to the Road Traffic Management Corpo...   \n",
              "3   The escalating situation in Israel and Palest...   \n",
              "4  At the time, the nation was reeling from the m...   \n",
              "\n",
              "                                        cleaned_text  \\\n",
              "0  the special investigation units siu final repo...   \n",
              "1  sumitomo manufactures the popular dunlop sumit...   \n",
              "2  according to the road traffic management corpo...   \n",
              "3  the escalating situation in israel and palesti...   \n",
              "4  at the time the nation was reeling from the mu...   \n",
              "\n",
              "                                              tokens language  label  \n",
              "0  [the, special, investigation, units, siu, fina...      eng      0  \n",
              "1  [sumitomo, manufactures, the, popular, dunlop,...      eng      0  \n",
              "2  [according, to, the, road, traffic, management...      eng      0  \n",
              "3  [the, escalating, situation, in, israel, and, ...      eng      0  \n",
              "4  [at, the, time, the, nation, was, reeling, fro...      eng      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fc02427-16f5-4966-9967-d977093310b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Special Investigation Unit’s (SIU) final r...</td>\n",
              "      <td>the special investigation units siu final repo...</td>\n",
              "      <td>[the, special, investigation, units, siu, fina...</td>\n",
              "      <td>eng</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sumitomo manufactures the popular Dunlop, Sumi...</td>\n",
              "      <td>sumitomo manufactures the popular dunlop sumit...</td>\n",
              "      <td>[sumitomo, manufactures, the, popular, dunlop,...</td>\n",
              "      <td>eng</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>According to the Road Traffic Management Corpo...</td>\n",
              "      <td>according to the road traffic management corpo...</td>\n",
              "      <td>[according, to, the, road, traffic, management...</td>\n",
              "      <td>eng</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The escalating situation in Israel and Palest...</td>\n",
              "      <td>the escalating situation in israel and palesti...</td>\n",
              "      <td>[the, escalating, situation, in, israel, and, ...</td>\n",
              "      <td>eng</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>At the time, the nation was reeling from the m...</td>\n",
              "      <td>at the time the nation was reeling from the mu...</td>\n",
              "      <td>[at, the, time, the, nation, was, reeling, fro...</td>\n",
              "      <td>eng</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fc02427-16f5-4966-9967-d977093310b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7fc02427-16f5-4966-9967-d977093310b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7fc02427-16f5-4966-9967-d977093310b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-36cbe1de-45b7-4f88-b9d8-d21db74321f1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36cbe1de-45b7-4f88-b9d8-d21db74321f1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-36cbe1de-45b7-4f88-b9d8-d21db74321f1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "eng_df",
              "summary": "{\n  \"name\": \"eng_df\",\n  \"rows\": 129,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 129,\n        \"samples\": [\n          \" The 114 South Africans who were evacuated from Wuhan in China some weeks ago ended their quarantine and were reunited with their families. They have all tested negative for the virus and are in good health and good spirits. For months, they had been in lockdown, first in Wuhan for some 51 days and then in Polokwane for 14 days. They were unable to be with their loved ones, unable to leave their living quarters and uncertain about when their ordeal would end. When we add the remaining 17 days that South Africa will be under lockdown they will have been under lockdown for 82 days. It was wonderful to spend time with this diverse group of South Africans made up of all ages, languages and backgrounds. I was impressed by their resilience and courage and by their determination to remain healthy. They have come from the epicentre of the coronavirus in Wuhan in China and have seen the devastating impact this virus is wreaking on human life. It is not surprising to hear them say that they are on a mission to safe-guard the health of those around them. Now their patience and fortitude has been rewarded, because they are returning to their families. Their return home was made possible by a great many people who went to great lengths to make this repatriation operation a success. As a nation, we are extremely grateful to the Government and the people of China for taking such good care of our citizens, and for their assistance in organising their repatriation. It is significant that several of the South Africans in Wuhan were on study scholarships from the Chinese government; an act of generosity that is deeply appreciated. We are grateful too to all the people who were involved in the operation, from the SAA flight crew to the medical team to the police and soldiers who brought them home. Each and every one of them stepped forward to take responsibility for the safety and well-being of others. They were prepared to undertake a difficult and dangerous mission and to subject themselves to quarantine. And now, they all tell me, they are ready for their next mission. I wish to thank the staff and management of the Ranch Hotel in Polokwane, who took great care of the returnees. They were prepared to play their part in our national effort to overcome this disease. Everyone involved in this operation has done South Africa proud. The experience of the South Africans in Wuhan demonstrates the effectiveness \\u2013 and the necessity \\u2013 of a state of lockdown. It was due to the drastic actions that the Chinese government took to con tain the disease in the city of Wuhan, that all of our people were able to return uninfected and healthy. Wuhan, a city of 11 million people in the province of Hubei, had more than 50,000 infections. Now, after more than two months after stringent lockdown measures were put in place, the province has had fewer than 20 new cases in the past two weeks. The containment of the disease in Wuhan City, in Hubei Province and in other places across China required a massive and extraordinary effort. It involved drastic restrictions on daily life and is having a severe impact on the Chinese economy. Other countries that have taken similar measures are having greater success in managing the spread of the disease than countries that have been slower to respond.As the total number of confirmed COVID-19 cases worldwide grows to over 700,000 and the number of deaths exceeds 33,000, we can draw lessons from these countries.It is now abundantly clear that the most effective way for a society to contain the spread of the disease is for the population to remain at home and physically isolated from each other for at least several weeks. And it is important that this lockdown and all other emergency measures are both strictly adhered to and consistently enforced. As the South Africans from Wuhan can testify, such restrictions on daily life, on movement and on ordinary human contact are extremely difficult to endure. In the South African context, a lockdown brings additional hardship and strain, and we are doing everything within our means to lessen the impact on our people. But the lesson from the South Africans in Wuhan is that a lockdown works. It shows that if we strictly observe the rules in place to stop the virus spreading, we will be able to bring infection rates down. It shows that if we cooperate with health authorities in doing what we have to do, we won\\u2019t be just saving our own lives but those around us too. The story of our South African returnees from Wuhan should give us encouragement and hope in the difficult weeks that lie ahead. Their story tells us that there is a light at the end of the tunnel, that if we stay the course, that if we remain disciplined and respect the lockdown, that if we work together, we will overcome. \",\n          \"Many South Af ricans who test positive for the coronavirus (COVID-19) cannot self-isolate because their home is too small for them to live and sleep in a room that\\u2019s not shared by anyone else.Government has ensured that dedicated isolation facilities are available for people who have tested positive, and dedicated quarantine facilities are available for people who are still waiting for their test results. Both services are free.According to Dr Marlin McCay, a general practitioner based in Florida on the West Rand, the first thing you need to do when diagnosed with COVID-19 is to limit the spread of the infection. \\u201cYou want to protect your family,\\u201d he says. To do this, you need to go into isolation \\u2013 either at home or at an isolation facility. \\u201cIf you are self-isolating it means you have to find a part of the house where you are totally cut off from the rest of the family. No one should come within three metres of you. You must eat on your own; use your own cutlery and crockery; use your own bathroom, if possible; and make sure there\\u2019s no contact with anyone else in the family.\\u201cYour doctor or primary care giver should help you with certain basic medications to ease most of the symptoms, especially things like body aches and pains, headache and fever. There are also some good vitamins one can use to stay healthy,\\u201d he says.It\\u2019s also important to get enough rest and drink enough water or clear fluids to make sure that your urine stays a pale clear colour. Most people with mild illness will start feeling better within a week of the first symptoms, but it\\u2019s important to monitor your symptoms carefully. If you develop any emergency warning signs, such as trouble breathing, chest pain or pressure in your chest that does not go away, coughing up blood, becoming confused, severe sleepi ness or blue lips or face you must call an ambulance or go to hospital immediately.For those who are not able to keep themselves away from others living in the home, government has setup free quarantine facilities around the country to keep you and your loved ones safe.The Western Cape Government recently published some comments from patients about its isolation facilities.Marie Jantjies from Witzenberg decided to follow healthcare workers\\u2019 advice and go to the isolation facility because her 97-year-old mother lives with her and her children and she wanted to protect them. \\u201cI was treated very well. The food was nice \\u2013 even nicer than at home. I realised I just had to stay positive. I was anxious, but I prayed and tried to relax,\\u201d she said.\\u201cThat place can save lives! It really is the best,\\u201d said John Arnoldus, who recovered from COVID-19 at an isolation facility in Drakenstein. \\u201cOne of my friends was in quarantine at home and he struggled to recover on his own. I told him to contact the people who cared for me \\u2013 that is the way to get better. The support of the medical staff and the manager there \\u2013 that pulls you through,\\u201d he said.Arnoldus was initially hesitant to go to the isolation facility because of the wrong perception that if you go to a facility like this you will get sicker and die. \\u201cI expected the worst, but I was received so well. Hats off to the staff! They explained to me that as I already have COVID-19, no one at the facility could make me sick,\\u201d he said.What can you expect from a quarantine facility?According to the Western Cape Government, all of your needs are taken of. You will receive regular meals, health monitoring by a healthcare worker, laundry services, comfort and quiet while you recover and free transport to and from the facility.\\n\\u201cIt\\u2019s also important to get enough rest and drink enough water or clear fluids to make sure that your urine stays a pale clear colour.\\u201d\",\n          \"We have come a long way from the days where social protest by artists attracted banning orders, and critical reporting by journalists risked imprisonment or the closure of publications.Recently, the organisation Reporters without Borders published the 2021 World Press Freedom Index, a ba-rometer of the state of media freedom across the globe.Overall, it was found that there has been a decline in public access to information and an increase in obstacles to news coverage in a number of countries. The report said that jour-nalism is \\u201ctotally blocked or seriously impeded\\u201d in 73 countries and \\u201cconstrained\\u201d in 59 others. What is worrying is that media freedom has deterio-rated under the COVID-19 pandemic, with the various restrictions put in place having seemingly been used to curtail media activity in several places.In this latest report South Africa ranked 32nd out of 180 countries. The index describes the state of media freedom in South Africa as \\u201cguaranteed but fragile\\u201d. It notes that while the South African Constitution protects freedom and we have an established culture of investigative journalism, a number of impediments still hinder journalists in the performance of their duties. This includes legal injunc-tions against taking images of National Key Points or re-porting on matters involving state security. The report also notes an increase during 2020 of the intimidation of journalists, especially female journalists on social media. Such intimidation is totally unacceptable, but is par-ticularly harmful when it is directed at female journalists and is occasionally accom-panied by threats of sexual violence. This is a matter of great concern and cannot be allowed.At the same time, we take great comfort in the knowl-edge that we have a free, robust media that is able to report without fear or favour about those in power, about the most pressing social issues of our time, and to provide accurate, impartial information to the public.At a time when we are working together to rebuild our economy and our society in the midst of the Coronavi-rus pandemic, a robust media is more critical than ever. The South African media has played a pivotal role in uncovering much of what we know today about the true extent of capture of the state by self-serving, corrupt individuals and entities.They sustained their re-porting even in the face of intimidation, disinformation and attacks on their person.Corruption is by no means the only challenge we face as a country. The daily lives of many South Africans are still affected by poverty, inequal-ity and underdevelopment, poor service delivery and lack of access to opportunities. If the media is to remain true to its responsibility to support democracy, our journalists must continue to report without fear or favour on the other issues of the day. Their sustained coverage must include gender-based violence, crime in our com-munities and social ills like substance abuse. Our media should provide accurate and impartial infor-mation, enabling the public to make informed decisions, to access opportunities and to improve their lives. They should continue to produce journalism that goes beyond the headlines and front pages and that contrib-utes to human development. They should report both the good news and the bad news, the progress we make and the challenges we face. Credibility is key to sustain-ing trust between journalists and the public. When journalists allow themselves or their platforms to be used to fight political battles or settle scores on be-half of vested interests, their credibility suffers. When media disseminate stories that are inaccurate or that they know to be false, the public loses faith in them. It is in the best interests of all who love this country and wish for it to succeed that our media is supported, and not hindered in its work.  As a society, let us continue to work together to jealously safeguard our country\\u2019s me-dia freedom. It was hard won and without it, we cannot hope to flourish.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 129,\n        \"samples\": [\n          \"the south africans who were evacuated from wuhan in china some weeks ago ended their quarantine and were reunited with their families they have all tested negative for the virus and are in good health and good spirits for months they had been in lockdown first in wuhan for some days and then in polokwane for days they were unable to be with their loved ones unable to leave their living quarters and uncertain about when their ordeal would end when we add the remaining days that south africa will be under lockdown they will have been under lockdown for days it was wonderful to spend time with this diverse group of south africans made up of all ages languages and backgrounds i was impressed by their resilience and courage and by their determination to remain healthy they have come from the epicentre of the coronavirus in wuhan in china and have seen the devastating impact this virus is wreaking on human life it is not surprising to hear them say that they are on a mission to safeguard the health of those around them now their patience and fortitude has been rewarded because they are returning to their families their return home was made possible by a great many people who went to great lengths to make this repatriation operation a success as a nation we are extremely grateful to the government and the people of china for taking such good care of our citizens and for their assistance in organising their repatriation it is significant that several of the south africans in wuhan were on study scholarships from the chinese government an act of generosity that is deeply appreciated we are grateful too to all the people who were involved in the operation from the saa flight crew to the medical team to the police and soldiers who brought them home each and every one of them stepped forward to take responsibility for the safety and wellbeing of others they were prepared to undertake a difficult and dangerous mission and to subject themselves to quarantine and now they all tell me they are ready for their next mission i wish to thank the staff and management of the ranch hotel in polokwane who took great care of the returnees they were prepared to play their part in our national effort to overcome this disease everyone involved in this operation has done south africa proud the experience of the south africans in wuhan demonstrates the effectiveness and the necessity of a state of lockdown it was due to the drastic actions that the chinese government took to con tain the disease in the city of wuhan that all of our people were able to return uninfected and healthy wuhan a city of million people in the province of hubei had more than infections now after more than two months after stringent lockdown measures were put in place the province has had fewer than new cases in the past two weeks the containment of the disease in wuhan city in hubei province and in other places across china required a massive and extraordinary effort it involved drastic restrictions on daily life and is having a severe impact on the chinese economy other countries that have taken similar measures are having greater success in managing the spread of the disease than countries that have been slower to respondas the total number of confirmed covid cases worldwide grows to over and the number of deaths exceeds we can draw lessons from these countriesit is now abundantly clear that the most effective way for a society to contain the spread of the disease is for the population to remain at home and physically isolated from each other for at least several weeks and it is important that this lockdown and all other emergency measures are both strictly adhered to and consistently enforced as the south africans from wuhan can testify such restrictions on daily life on movement and on ordinary human contact are extremely difficult to endure in the south african context a lockdown brings additional hardship and strain and we are doing everything within our means to lessen the impact on our people but the lesson from the south africans in wuhan is that a lockdown works it shows that if we strictly observe the rules in place to stop the virus spreading we will be able to bring infection rates down it shows that if we cooperate with health authorities in doing what we have to do we wont be just saving our own lives but those around us too the story of our south african returnees from wuhan should give us encouragement and hope in the difficult weeks that lie ahead their story tells us that there is a light at the end of the tunnel that if we stay the course that if we remain disciplined and respect the lockdown that if we work together we will overcome\",\n          \"many south af ricans who test positive for the coronavirus covid cannot selfisolate because their home is too small for them to live and sleep in a room thats not shared by anyone elsegovernment has ensured that dedicated isolation facilities are available for people who have tested positive and dedicated quarantine facilities are available for people who are still waiting for their test results both services are freeaccording to dr marlin mccay a general practitioner based in florida on the west rand the first thing you need to do when diagnosed with covid is to limit the spread of the infection you want to protect your family he says to do this you need to go into isolation either at home or at an isolation facility if you are selfisolating it means you have to find a part of the house where you are totally cut off from the rest of the family no one should come within three metres of you you must eat on your own use your own cutlery and crockery use your own bathroom if possible and make sure theres no contact with anyone else in the familyyour doctor or primary care giver should help you with certain basic medications to ease most of the symptoms especially things like body aches and pains headache and fever there are also some good vitamins one can use to stay healthy he saysits also important to get enough rest and drink enough water or clear fluids to make sure that your urine stays a pale clear colour most people with mild illness will start feeling better within a week of the first symptoms but its important to monitor your symptoms carefully if you develop any emergency warning signs such as trouble breathing chest pain or pressure in your chest that does not go away coughing up blood becoming confused severe sleepi ness or blue lips or face you must call an ambulance or go to hospital immediatelyfor those who are not able to keep themselves away from others living in the home government has setup free quarantine facilities around the country to keep you and your loved ones safethe western cape government recently published some comments from patients about its isolation facilitiesmarie jantjies from witzenberg decided to follow healthcare workers advice and go to the isolation facility because her yearold mother lives with her and her children and she wanted to protect them i was treated very well the food was nice even nicer than at home i realised i just had to stay positive i was anxious but i prayed and tried to relax she saidthat place can save lives it really is the best said john arnoldus who recovered from covid at an isolation facility in drakenstein one of my friends was in quarantine at home and he struggled to recover on his own i told him to contact the people who cared for me that is the way to get better the support of the medical staff and the manager there that pulls you through he saidarnoldus was initially hesitant to go to the isolation facility because of the wrong perception that if you go to a facility like this you will get sicker and die i expected the worst but i was received so well hats off to the staff they explained to me that as i already have covid no one at the facility could make me sick he saidwhat can you expect from a quarantine facilityaccording to the western cape government all of your needs are taken of you will receive regular meals health monitoring by a healthcare worker laundry services comfort and quiet while you recover and free transport to and from the facility its also important to get enough rest and drink enough water or clear fluids to make sure that your urine stays a pale clear colour\",\n          \"we have come a long way from the days where social protest by artists attracted banning orders and critical reporting by journalists risked imprisonment or the closure of publicationsrecently the organisation reporters without borders published the world press freedom index a barometer of the state of media freedom across the globeoverall it was found that there has been a decline in public access to information and an increase in obstacles to news coverage in a number of countries the report said that journalism is totally blocked or seriously impeded in countries and constrained in others what is worrying is that media freedom has deteriorated under the covid pandemic with the various restrictions put in place having seemingly been used to curtail media activity in several placesin this latest report south africa ranked nd out of countries the index describes the state of media freedom in south africa as guaranteed but fragile it notes that while the south african constitution protects freedom and we have an established culture of investigative journalism a number of impediments still hinder journalists in the performance of their duties this includes legal injunctions against taking images of national key points or reporting on matters involving state security the report also notes an increase during of the intimidation of journalists especially female journalists on social media such intimidation is totally unacceptable but is particularly harmful when it is directed at female journalists and is occasionally accompanied by threats of sexual violence this is a matter of great concern and cannot be allowedat the same time we take great comfort in the knowledge that we have a free robust media that is able to report without fear or favour about those in power about the most pressing social issues of our time and to provide accurate impartial information to the publicat a time when we are working together to rebuild our economy and our society in the midst of the coronavirus pandemic a robust media is more critical than ever the south african media has played a pivotal role in uncovering much of what we know today about the true extent of capture of the state by selfserving corrupt individuals and entitiesthey sustained their reporting even in the face of intimidation disinformation and attacks on their personcorruption is by no means the only challenge we face as a country the daily lives of many south africans are still affected by poverty inequality and underdevelopment poor service delivery and lack of access to opportunities if the media is to remain true to its responsibility to support democracy our journalists must continue to report without fear or favour on the other issues of the day their sustained coverage must include genderbased violence crime in our communities and social ills like substance abuse our media should provide accurate and impartial information enabling the public to make informed decisions to access opportunities and to improve their lives they should continue to produce journalism that goes beyond the headlines and front pages and that contributes to human development they should report both the good news and the bad news the progress we make and the challenges we face credibility is key to sustaining trust between journalists and the public when journalists allow themselves or their platforms to be used to fight political battles or settle scores on behalf of vested interests their credibility suffers when media disseminate stories that are inaccurate or that they know to be false the public loses faith in them it is in the best interests of all who love this country and wish for it to succeed that our media is supported and not hindered in its work as a society let us continue to work together to jealously safeguard our countrys media freedom it was hard won and without it we cannot hope to flourish\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"eng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# en_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "\n",
        "# english_prompts = [\n",
        "#     \"Explain the significance of lobola in Southern Africa\",\n",
        "#     \"Write a short dialogue between two friends in Johannesburg\",\n",
        "#     \"Describe linguistic features that make isiZulu agglutinative\",\n",
        "#     \"Generate a  news article about corruption in the style of South African government communications.\",\n",
        "#     \"Write a news report about economic development in rural communities.\",\n",
        "#     \"Write a  discussion about community development challenges and solutions.\",\n",
        "#     \"Create a  text about youth employment opportunities and skills development.\",\n",
        "#     \"Generate a  article about women's empowerment in rural communities.\",\n",
        "#     \"Write about community health and wellness practices in  communities.\",\n",
        "#     \"Compose a  text discussing the importance of preserving local languages.\",\n",
        "#     \"Create a formal  announcement about a community meeting.\",\n",
        "#     \"Write a formal  notice about public services or facilities.\",\n",
        "#     \"Write an informal  text message exchange about planning an event.\",\n",
        "#     \"Create a  social media post about local community events.\",\n",
        "#     \"Create a  troubleshooting guide for common problems.\",\n",
        "#     \"Create a  article discussing healthcare initiatives, written as if for a government publication.\",\n",
        "#     \"Compose a  news piece about education policy changes, using official government communication style.\",\n",
        "#     \"Generate a  article about infrastructure development projects in KwaZulu-Natal.\",\n",
        "#     \"Describe a traditional Zulu ceremony in detail, including customs, rituals, and their significance.\",\n",
        "#     \"Create a  text explaining traditional farming methods and seasonal practices.\",\n",
        "#     \"Generate a  story about ubuntu philosophy and community values.\",\n",
        "#     \"Write about traditional  crafts and their cultural importance.\",\n",
        "#     \"Compose a  text describing traditional music and dance forms.\",\n",
        "#     \"Write a  text teaching basic mathematics concepts to young learners.\",\n",
        "#     \"Create a  guide explaining digital literacy for adult learners.\"\n",
        "# ]\n",
        "\n",
        "\n",
        "# def generate_with_prompts(prompts, generator, language, samples_per_prompt=3):\n",
        "#     data = []\n",
        "#     for prompt in prompts:\n",
        "#         for _ in range(samples_per_prompt):\n",
        "#             output = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
        "#             data.append({\n",
        "#                 'text': output\n",
        "#             })\n",
        "#     return pd.DataFrame(data)\n",
        "\n",
        "# eng_mg_df = generate_with_prompts(english_prompts, en_generator, 'English')\n",
        "# eng_mg_df = clean_and_reformat_data(eng_mg_df, text_column='text', language='eng', clean_text_flag=True, label=1)\n",
        "\n",
        "def load_jsonl_data(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:  # Skip empty lines\n",
        "                    try:\n",
        "                        data.append(json.loads(line))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"Error parsing JSON line: {e}\")\n",
        "                        continue\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return []\n",
        "\n",
        "    return data\n",
        "\n",
        "zul_mg_df = pd.DataFrame(load_jsonl_data('zulu_mg_text.jsonl'))\n",
        "zul_mg_df2 = pd.DataFrame(load_jsonl_data('mg_text.jsonl'))\n",
        "\n",
        "zul_mg_df1 = clean_and_reformat_data(zul_mg_df, text_column='text', language='zul', clean_text_flag=True, label=1)\n",
        "zul_mg_df2 = clean_and_reformat_data(zul_mg_df2, text_column='text', language='zul', clean_text_flag=True, label=1)\n",
        "\n",
        "zul_mg_df = pd.concat([zul_mg_df1, zul_mg_df2], ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Iw-T-_M91O",
        "outputId": "5d1fdcb3-2bb3-48ed-de50-ba41073e3860"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error parsing JSON line: Invalid \\escape: line 1 column 331 (char 330)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMBINE THE DATASETS\n",
        "-------------------------"
      ],
      "metadata": {
        "id": "KKhdMXTaNC30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both datasets, this will be bad for us if we have more than 70% differnce in data length\n",
        "# Combine both datasets\n",
        "all_texts = pd.concat([zul_df, zul_mg_df], ignore_index=True)\n",
        "all_texts_clean = all_texts[\n",
        "    (all_texts['text'].notna()) &  # Remove NaN/None\n",
        "    (all_texts['text'].astype(str).str.strip() != '[]') &  # Remove empty lists\n",
        "    (all_texts['text'].astype(str).str.strip() != \"['']\") &  # Remove lists with empty strings\n",
        "    (all_texts['text'].astype(str).str.strip() != '') &  # Remove empty strings\n",
        "    (all_texts['text'].str.len() > 0)  # Remove zero-length strings\n",
        "].copy()\n",
        "\n",
        "all_texts_shuffled = all_texts_clean.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Total texts after combining: {len(all_texts)}\")\n",
        "print(f\"Human-written texts: {len(zul_df)}\")\n",
        "print(f\"Machine-generated texts: {len(zul_mg_df)}\")\n",
        "\n",
        "# Create a DataFrame for easier analysis\n",
        "combined_df = pd.DataFrame(all_texts_shuffled)\n",
        "combined_df.head()\n",
        "\n",
        "# Create a balanced dataset with equal samples from each class\n",
        "# we can choose to use this of the combined one\n",
        "def create_balanced_dataset(df, target_size_per_class=None):\n",
        "    class_counts = df['label'].value_counts()\n",
        "    min_class_size = class_counts.min()\n",
        "    if target_size_per_class:\n",
        "        sample_size = min(target_size_per_class, min_class_size)\n",
        "    else:\n",
        "        sample_size = min_class_size\n",
        "    balanced_df = df.groupby('label').sample(n=sample_size, random_state=42)\n",
        "    return balanced_df.reset_index(drop=True)\n",
        "\n",
        "# Create balanced dataset, because we need the same number of samples for each class\n",
        "balanced_df = create_balanced_dataset(combined_df)\n",
        "print(f\"\\nBalanced dataset created with {len(balanced_df)} samples\")\n",
        "print(f\"Label distribution in balanced dataset: \\n{balanced_df['label'].value_counts()}\")\n",
        "\n",
        "# Save balanced dataset\n",
        "balanced_output_path = 'balanced_zulu_texts.csv'\n",
        "balanced_df.to_csv(balanced_output_path, index=False, encoding='utf-8')\n",
        "print(f\"Balanced dataset saved to: {balanced_output_path}\")\n",
        "balanced_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "HO3JSaWjNIA5",
        "outputId": "c0140e6a-f5a7-4ae0-a0a4-8e92497e42fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total texts after combining: 1161\n",
            "Human-written texts: 852\n",
            "Machine-generated texts: 309\n",
            "\n",
            "Balanced dataset created with 618 samples\n",
            "Label distribution in balanced dataset: \n",
            "label\n",
            "0    309\n",
            "1    309\n",
            "Name: count, dtype: int64\n",
            "Balanced dataset saved to: balanced_zulu_texts.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  Lokhu kugcizelelwe nguMongameli u-Cyril Ramaph...   \n",
              "1                                            ['cha']   \n",
              "2                 ['izigidi ezingama-246 zamarandi']   \n",
              "3                                       ['onegunya']   \n",
              "4                                            ['cha']   \n",
              "\n",
              "                                        cleaned_text  \\\n",
              "0  lokhu kugcizelelwe ngumongameli ucyril ramapho...   \n",
              "1                                                cha   \n",
              "2                         izigidi ezingama zamarandi   \n",
              "3                                           onegunya   \n",
              "4                                                cha   \n",
              "\n",
              "                                              tokens language  label  \n",
              "0  [lokhu, kugcizelelwe, ngumongameli, ucyril, ra...      zul      0  \n",
              "1                                              [cha]      zul      0  \n",
              "2                     [izigidi, ezingama, zamarandi]      zul      0  \n",
              "3                                         [onegunya]      zul      0  \n",
              "4                                              [cha]      zul      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38b524a7-2b1d-488d-bcb3-eaee327e87b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lokhu kugcizelelwe nguMongameli u-Cyril Ramaph...</td>\n",
              "      <td>lokhu kugcizelelwe ngumongameli ucyril ramapho...</td>\n",
              "      <td>[lokhu, kugcizelelwe, ngumongameli, ucyril, ra...</td>\n",
              "      <td>zul</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['cha']</td>\n",
              "      <td>cha</td>\n",
              "      <td>[cha]</td>\n",
              "      <td>zul</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['izigidi ezingama-246 zamarandi']</td>\n",
              "      <td>izigidi ezingama zamarandi</td>\n",
              "      <td>[izigidi, ezingama, zamarandi]</td>\n",
              "      <td>zul</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['onegunya']</td>\n",
              "      <td>onegunya</td>\n",
              "      <td>[onegunya]</td>\n",
              "      <td>zul</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['cha']</td>\n",
              "      <td>cha</td>\n",
              "      <td>[cha]</td>\n",
              "      <td>zul</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38b524a7-2b1d-488d-bcb3-eaee327e87b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38b524a7-2b1d-488d-bcb3-eaee327e87b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38b524a7-2b1d-488d-bcb3-eaee327e87b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a33ab2e8-f00d-431f-aadf-9d3ec5bce998\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a33ab2e8-f00d-431f-aadf-9d3ec5bce998')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a33ab2e8-f00d-431f-aadf-9d3ec5bce998 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "balanced_df",
              "summary": "{\n  \"name\": \"balanced_df\",\n  \"rows\": 618,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 552,\n        \"samples\": [\n          \" Isimo esiqhubekayo kwa-Israeli kanye nase-Palestine siqinisekisa lokhu abantu baseNingizimu Afrika abakwaziyo nabo, ukuthi izingxabano ezinzima zingaxazululwa kuphela ngokuxoxisana ngokuthula.  Siphinde futhi sibonise ukuthi ngaphandle kokuthi kukhulunywe ngesisusa sengxabano, kulesi sigameko okungukuhlala ngoku ngekho emthethweni kwa-bantu bakwa-Israeli emhlabeni wabantu base-Palestine kanye nokunqatshelwa kwelungelo lokuzibusa labantu base-Palestine, ngeke kuze kube nokuthula. Udlame lwakamuva nje luqutshulwe isinqumo senkantolo yakwa-Israeli  ebesixosha imindeni ema khaya ayo endaweni yase-Sheikh Jarrah ese Mpumalanga ye-Jerusalema ukuze kunikezwe indawo yokuhlala kubantu bakwa-Israeli. Ukubuka amadoda, abesifazane kanye nezingane bexoshwa emakhaya imi-ndeni yabo ebisiphile khona izizukulwane ngezizuku-lwane kubuyisa isithombe senkumbulo yenhlanganisela yemicabango kanye nobuhlungu bomunye nomunye beningi labantu baseNingizimu Afrika \\u2013 abasuswa ngendluzula ezinda -weni zabo futhi bathathelwa umhlaba wabo. Kwakubuhlungu futhi kuyimpoxo eyayibhekene nomndeni wami, kanye neningi lemindeni yabantu baseNingizimu Afrika. Umndeni wami wathu thelwa ezindaweni eze-hlukene zezwe ngenkani izikhathi kwaze kwaba kabili.  Ukuphoqwa ngenkani ukuthi uphume ekhaya ukhonjwe ngesibhamu kungubuhlungu obukhulu lobo futhi okungelula ukuthi ubukhohlwe futhi kudlulela ezizukulwaneni ngezizu-kulwane. Njengezwe, sisahleli nemi-phumela eyinsalela yezenzo ezibuhlungu ezenziwa egameni lokuhlelwa kwendawo yesikhathi sobandlululo.Kubo bonke labo aba-kholelwa ekulinganeni, ubulungiswa kanye nama lungelo abantu, ngeke  sikwazi kodwa ukushu kunyiswa futhi sithukuthe-liswe ngempela, ngubuhlu-ngu nokuphoxeka okwe nziwa kubantu base Palestine; ngoba nakhu ku nanelwa okwethu.Izenzo zabantu bakwa-Israeli zingukuphula umthetho womhlaba ji kelele. Batshengise ukunga-yishayi ndiva impumelelo yezinqubo zoMkhandlu Wezokuphepha zoMhlabu-hlangene ezazicela ukuthi kumiswe nsi ukusetshe-nziswa komhlaba wase-Palestine futhi kufezwe ngokuphelele amalungelo abantu base-Palestine. Selokhu amasosha ezokuphepha akwa-Israeli aqala ukuhlasela amakholwa  endaweni yawo ebizwa nge Al Aqsa Mosque  eseJe rusalema ngesonto ele dlule, lolu dlame selumboze  i-Gaza Strip okuyizingxenye ezinkulu ze-West Bank namadolobha amaningi akwa-Israeli. Lokhu sekudlule nemiphefumulo eminingi yabantu, okubandakanya nezingane.Kuyakhathaza kakhulu futhi ukuthi amasosha akwa-Israeli acekele phansi isakhiwo esiyisitezi nesinezinhlangano eziningi ze zindaba, lokhu bekwenze-lwa ukuthumela umyalezo othusayo kwabezindaba ababika ngalolu dlame.Ukuqhunyiswa kwama-bhomu yi-Israeli e-Gaza okungenazwelo futhi okuqhubekayo kuzoba nemiphumela emibi kubantu abangaphezu kwezigidi ezimbili abebekhahlanyezwe ukuvinjwa ngokunge-mthetho yizwe lakwa-Israeli iminyaka eyi-14. Sicela wonke amaqembu abandakanyekayo ukuthi azibambe, ahloniphe impi-lo yomuntu, futhi aqede ubutha obukhona njengamanje. NjengeNingizimu Afrika, sizinikele ukuthi sibe yi ngxenye yemizamo yama zwe omhlaba okuhloswe ngayo ukuvuselela inqu-bo yezepolitiki ezoholela ekwakhiweni kombuso wase-Palestine osebenzayo futhi okhona ezinhla-ngothini zombili ukudala ukuthula ne-Israeli futhi nangaphakathi emingceleni  eyaziwa emhlabeni jikelele. Isixazululo semibuso yomibili sisamile njengendlela eyiyo yabantu bakwa-Israeli kanye ne-Palestine futhi kumele kuqhutshekwe nokuthi sesekwe. Ngesikhathi amasosha ezokuphepha akwa-Israeli ehlasela amakhohlwa e Al Aqsa Mosque, nathi eNingizimu Afrika besilu-ngiselela ukugubha ikhulu leminyaka lesibhicongo sokubulawa kwabantu  i-Bulhoek Massacre enda weni yenkolo eNtabelanga eMpumalanga Kapa. Mhla zingama-24 ku Nhlaba 1921, amasosha ezokuphepha amakholoni ayehlome ngezibhamu nemishini yezikhali avulela ngenhlamvu kumakholwa, kwashona abantu abanga-phezu kwe-160 kwalimala kabi abacishe bafinyelele kwabayi-130.Lesi sibhicongo sokubulawa kwabantu siveze obala isihluku hhayi kuphela esombutho wamaphoyisa weNyunyana yaseNi-ngizimu Afrika, kodwa futhi nenqubo yokucwasa ngokwebala eyabekwa icala lokusivumela.Njengengxabano endaweni i-Sheik Jarrah, isihluku  e-Bulhoek sasingeyona nje ingxabano yendawo; kwakungokuphathelene nokuphucwa umhlaba ngenkani, ngokuqonelwa komhlaba, ngokubandlu lulwa ngokobuhlanga nango kucindezelwa kwabantu abangavumelani nezenzo zangaleso sikhathi.Njengoba sibheka inkinga yase-Middle East futhi  ikakhulukazi ngokuhlu pheka kwabantu base-Palestine, kungakuhle  sikhumbule amazwi ka-Selby Msimang, ilungu elasungula i-African National Congress.Ngemuva kwesibhicongo sase-Bulhoek wabhala wathi: \\\"Umlando ukhombisile ukuthi umphefumulo womuntu uvukela ukungabi nabulungiswa\\\".Imibhikisho kanye nokuvukelana kwabantu base-Ningizimu Afrika ababe cindezelwe belwa nobuqo nela nobandlululo kwa fakazela ubuqiniso balo mbono.Njengabathandi benkululeko nobulungiswa, simi nabantu base-Palestine emzamweni wabo wokuzi busa, kepha futhi ekume laneni kwabo nokuphucwa amalungelo abo kanye nokuphucwa isithunzi sabo. Njengezakhamuzi zezwe elikwazile ukufulathela inzondo yobuhlanga  nokuchitheka kwe gazi nokwakha umpha kathi obandakanya bonke abantu ogxile emalunge-lweni abantu bonke, ku-yithemba lethu sonke ukuthi abantu bakwa-Israeli kanye ne-Palestine bazolandela  indlela efanayo; yokuthi  bakwazi ukuthi bagcine bebona ngokufana, futhi bazokwazi ukuthola uku-thula.\",\n          \"['Imeya yaseGoli']\",\n          \"['Port Elizabeth']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 547,\n        \"samples\": [\n          \"us wezigidi\",\n          \"okubili\",\n          \"ezempilo nezindlela zokuphila ezinhle emiphakathini yesizulu ukuba nempilo enhle kubalulekile empilweni yonke yomuntu emiphakathini yesizulu kunemikhuba eminingi emihle yezempilo efana nokudla okunempilo ukuzivocavoca umzimba kanye nokuhlala uhlanzekile kubalulekile ukuqhubeka nokufundisa umphakathi ngezifo ezivamile kanye nezindlela zokuzivikela kuzo ukufinyelela ezinsizakalweni zezempilo eziyisisekelo kubalulekile imitholampilo nezikhungo zezempilo kufanele zibe kalula ukufinyeleleka futhi zinikeze izinsiza ezisezingeni eliphezulu siphinde sigxile ekufundiseni umphakathi ngezifo ezingathathelani njengesifo sikashukela nomfutho wegazi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"zul\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28z9e72eMM9v",
        "outputId": "c3a59cd8-01e4-4f14-82ef-0f514921f548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading AfriBERTa model: castorini/afriberta_base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at castorini/afriberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized on cuda\n",
            "Model parameters: 111,456,770\n",
            "Filtered 618 Zulu samples from 618 total samples\n",
            "Training samples: 494\n",
            "Test samples: 124\n",
            "Label distribution in training: [247 247]\n",
            "Label distribution in test: [62 62]\n",
            "Setting up LoRA configuration...\n",
            "LoRA enabled. Trainable parameters: 112638724\n",
            "Starting training with AfriBERTa...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [310/310 00:49, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.474600</td>\n",
              "      <td>0.400817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.280900</td>\n",
              "      <td>0.287466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.340200</td>\n",
              "      <td>0.282339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.396700</td>\n",
              "      <td>0.276188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.158200</td>\n",
              "      <td>0.293520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.229700</td>\n",
              "      <td>0.283705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "AFRIBERTA ZULU CLASSIFIER - EVALUATION RESULTS\n",
            "==================================================\n",
            "Overall Accuracy: 0.9032\n",
            "Weighted Precision: 0.9140\n",
            "Weighted Recall: 0.9032\n",
            "Weighted F1-Score: 0.9026\n",
            "\n",
            "Per-Class Results:\n",
            "Human:\n",
            "  Precision: 0.9808\n",
            "  Recall: 0.8226\n",
            "  F1-Score: 0.8947\n",
            "  Support: 62\n",
            "Machine:\n",
            "  Precision: 0.8472\n",
            "  Recall: 0.9839\n",
            "  F1-Score: 0.9104\n",
            "  Support: 62\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.98      0.82      0.89        62\n",
            "     Machine       0.85      0.98      0.91        62\n",
            "\n",
            "    accuracy                           0.90       124\n",
            "   macro avg       0.91      0.90      0.90       124\n",
            "weighted avg       0.91      0.90      0.90       124\n",
            "\n",
            "============================================================\n",
            "MODEL EXPLAINABILITY ANALYSIS REPORT\n",
            "Analyzing 30% of data for efficiency\n",
            "============================================================\n",
            "\n",
            "1. FEATURE IMPORTANCE ANALYSIS\n",
            "----------------------------------------\n",
            "Analyzing 37 samples (30%) out of 124 total samples\n",
            "Selected 37 samples for analysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating LIME explanations: 100%|██████████| 37/37 [04:47<00:00,  7.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top features indicating HUMAN text:\n",
            "  - 'wokukhishwa': 1 occurrences\n",
            "  - 'ukusebenzisana': 1 occurrences\n",
            "  - 'zokulwisana': 1 occurrences\n",
            "  - 'ngezinhlelo': 1 occurrences\n",
            "  - 'abakuleliukwengeza': 1 occurrences\n",
            "  - 'africa': 1 occurrences\n",
            "  - 'yale': 1 occurrences\n",
            "  - 'umntwana': 1 occurrences\n",
            "  - 'bengasanakekelwa': 1 occurrences\n",
            "  - 'isibonelelo': 1 occurrences\n",
            "\n",
            "Top features indicating MACHINE text:\n",
            "  - 'kuqinisekiswe': 2 occurrences\n",
            "  - 'ukuthi': 2 occurrences\n",
            "  - 'ukudla': 2 occurrences\n",
            "  - 'umsebenzi': 2 occurrences\n",
            "  - 'labahleli': 1 occurrences\n",
            "  - 'ngicabanga': 1 occurrences\n",
            "  - 'umqondo': 1 occurrences\n",
            "  - 'ngiyavumelana': 1 occurrences\n",
            "  - 'ngingcono': 1 occurrences\n",
            "  - 'abantu': 1 occurrences\n",
            "\n",
            "2. LINGUISTIC FEATURE ANALYSIS\n",
            "----------------------------------------\n",
            "Running linguistic analysis on 37 samples (30%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing linguistic features: 100%|██████████| 37/37 [00:00<00:00, 4251.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Sentence Length:\n",
            "  Human avg: 130.167, Machine avg: 219.632\n",
            "Lexical Diversity:\n",
            "  Human avg: 0.936, Machine avg: 0.868\n",
            "Repetition Score:\n",
            "  Human avg: 0.064, Machine avg: 0.132\n",
            "\n",
            "3. ERROR ANALYSIS\n",
            "----------------------------------------\n",
            "Found 12 total errors, analyzing 3 in detail\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating error explanations: 100%|██████████| 3/3 [00:24<00:00,  8.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total classification errors: 12\n",
            "False positives (Human → Machine): 11\n",
            "False negatives (Machine → Human): 1\n",
            "\n",
            "Example misclassifications:\n",
            "  1. True: Human, Predicted: Machine\n",
            "     Text: isikhwama esibhekele udlame olubhekiswe kwabo bulili obuthile nokubulawa kwabesifazane igbvf sihlose...\n",
            "  2. True: Human, Predicted: Machine\n",
            "     Text: lolu hlelo luhlose ukuthuthukisa ikhono labantwana lokufunda ngokunciphisa ukungondleki kahle nendla...\n",
            "  3. True: Human, Predicted: Machine\n",
            "     Text: umphakathi wasedouble drift ngasegrahamstown uhlangani sa abantu abayi abaqha muka emindenini engama...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample prediction for 'Ezinye izisindo ze-XLMClassification azizange ziqaliswe endaweni yokuhlola yemodeli e-castorini futhi zisanda kuqaliswa.':\n",
            "Predicted: Machine (confidence: 0.6246)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import shap\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import re\n",
        "from textstat import flesch_reading_ease, automated_readability_index\n",
        "from collections import Counter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class ZuluTextDataset(Dataset):\n",
        "    \"\"\"Custom Dataset class for Zulu text classification\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class ZuluTextClassifier:\n",
        "    \"\"\"AfriBERTa classifier for Zulu human vs machine text detection\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='castorini/afriberta_base', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize tokenizer and model with AfriBERTa\n",
        "        print(f\"Loading AfriBERTa model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\"Model initialized on {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "    def prepare_data(self, df, text_column='cleaned_text', label_column='label', test_size=0.2):\n",
        "        \"\"\"Prepare and split the data\"\"\"\n",
        "        # Filter for Zulu language if language column exists\n",
        "        if 'language' in df.columns:\n",
        "            df_zulu = df[df['language'].str.lower().isin(['zul'])].copy()\n",
        "            print(f\"Filtered {len(df_zulu)} Zulu samples from {len(df)} total samples\")\n",
        "        else:\n",
        "            df_zulu = df.copy()\n",
        "            print(f\"Using all {len(df_zulu)} samples (no language filtering)\")\n",
        "\n",
        "        if len(df_zulu) == 0:\n",
        "            raise ValueError(\"No Zulu samples found in the dataset\")\n",
        "\n",
        "        # Extract texts and labels\n",
        "        texts = df_zulu[text_column].tolist()\n",
        "        labels = df_zulu[label_column].tolist()\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            texts, labels,\n",
        "            test_size=test_size,\n",
        "            random_state=42,\n",
        "            stratify=labels\n",
        "        )\n",
        "\n",
        "        print(f\"Training samples: {len(X_train)}\")\n",
        "        print(f\"Test samples: {len(X_test)}\")\n",
        "        print(f\"Label distribution in training: {np.bincount(y_train)}\")\n",
        "        print(f\"Label distribution in test: {np.bincount(y_test)}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_datasets(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Create PyTorch datasets\"\"\"\n",
        "        train_dataset = ZuluTextDataset(X_train, y_train, self.tokenizer, self.max_length)\n",
        "        test_dataset = ZuluTextDataset(X_test, y_test, self.tokenizer, self.max_length)\n",
        "\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    def setup_lora(self, use_lora=True, lora_r=16, lora_alpha=32, lora_dropout=0.1):\n",
        "        \"\"\"Setup LoRA for parameter-efficient fine-tuning\"\"\"\n",
        "        if use_lora:\n",
        "            print(\"Setting up LoRA configuration...\")\n",
        "            lora_config = LoraConfig(\n",
        "                r=lora_r,\n",
        "                lora_alpha=lora_alpha,\n",
        "                target_modules=[\"query\", \"key\", \"value\"],\n",
        "                lora_dropout=lora_dropout,\n",
        "                bias=\"none\",\n",
        "                task_type=\"SEQ_CLS\"\n",
        "            )\n",
        "\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            print(f\"LoRA enabled. Trainable parameters: {self.model.num_parameters()}\")\n",
        "        else:\n",
        "            print(\"Using full fine-tuning (no LoRA)\")\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset, output_dir='./afriberta_zulu_classifier_results',\n",
        "              use_lora=True, epochs=5, batch_size=8, learning_rate=5e-5):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "\n",
        "        # Setup LoRA if requested\n",
        "        self.setup_lora(use_lora=use_lora)\n",
        "\n",
        "        # Training arguments optimized for AfriBERTa\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=epochs,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            learning_rate=learning_rate,\n",
        "            logging_dir=f'{output_dir}/logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",  # Disable wandb\n",
        "            seed=42,\n",
        "            dataloader_pin_memory=False,  # Better for some setups\n",
        "            fp16=torch.cuda.is_available(),  # Use mixed precision if available\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training with AfriBERTa...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def evaluate(self, test_dataset, trainer=None):\n",
        "        \"\"\"Evaluate the model\"\"\"\n",
        "        if trainer is None:\n",
        "            # Create a new trainer for evaluation\n",
        "            trainer = Trainer(model=self.model)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = predictions.label_ids\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "        # Per-class metrics\n",
        "        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average=None\n",
        "        )\n",
        "\n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'precision_per_class': precision_per_class,\n",
        "            'recall_per_class': recall_per_class,\n",
        "            'f1_per_class': f1_per_class,\n",
        "            'support': support,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_evaluation_results(self, results):\n",
        "        \"\"\"Print detailed evaluation results\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"AFRIBERTA ZULU CLASSIFIER - EVALUATION RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        print(f\"Overall Accuracy: {results['accuracy']:.4f}\")\n",
        "        print(f\"Weighted Precision: {results['precision']:.4f}\")\n",
        "        print(f\"Weighted Recall: {results['recall']:.4f}\")\n",
        "        print(f\"Weighted F1-Score: {results['f1']:.4f}\")\n",
        "\n",
        "        print(\"\\nPer-Class Results:\")\n",
        "        class_names = ['Human', 'Machine']\n",
        "        for i in range(len(class_names)):\n",
        "            print(f\"{class_names[i]}:\")\n",
        "            print(f\"  Precision: {results['precision_per_class'][i]:.4f}\")\n",
        "            print(f\"  Recall: {results['recall_per_class'][i]:.4f}\")\n",
        "            print(f\"  F1-Score: {results['f1_per_class'][i]:.4f}\")\n",
        "            print(f\"  Support: {results['support'][i]}\")\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(results['y_true'], results['y_pred'],\n",
        "                                  target_names=class_names))\n",
        "\n",
        "    def predict_single_text(self, text):\n",
        "        \"\"\"Predict a single text sample\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tokenize the input\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encoding)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = predictions.max().item()\n",
        "\n",
        "        class_names = ['Human', 'Machine']\n",
        "        return {\n",
        "            'predicted_class': class_names[predicted_class],\n",
        "            'confidence': confidence,\n",
        "            'probabilities': {\n",
        "                'Human': predictions[0][0].item(),\n",
        "                'Machine': predictions[0][1].item()\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class TextExplainabilityAnalyzer:\n",
        "    \"\"\"\n",
        "    Explainability analysis for machine vs human text classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.class_names = ['Human', 'Machine']\n",
        "\n",
        "        # Initialize LIME explainer\n",
        "        self.lime_explainer = LimeTextExplainer(\n",
        "            class_names=self.class_names\n",
        "        )\n",
        "\n",
        "    def predict_proba_for_lime(self, texts):\n",
        "        \"\"\"\n",
        "        Prediction function compatible with LIME\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        self.model.eval()\n",
        "\n",
        "        for text in texts:\n",
        "            # Tokenize\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=512,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "\n",
        "            # Get prediction\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoding)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predictions.append(probs.cpu().numpy()[0])\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def explain_with_lime(self, text, num_features=10):\n",
        "        \"\"\"\n",
        "        Generate LIME explanation for a single text\n",
        "        \"\"\"\n",
        "        explanation = self.lime_explainer.explain_instance(\n",
        "            text,\n",
        "            self.predict_proba_for_lime,\n",
        "            num_features=num_features,\n",
        "            num_samples=500  # Reduced from 1000 to 500 for faster processing\n",
        "        )\n",
        "\n",
        "        return explanation\n",
        "\n",
        "    def explain_with_shap(self, texts, background_texts=None):\n",
        "        \"\"\"\n",
        "        Generate SHAP explanations for texts\n",
        "        \"\"\"\n",
        "        # Create a wrapper for SHAP\n",
        "        def model_wrapper(texts):\n",
        "            return self.predict_proba_for_lime(texts)\n",
        "\n",
        "        # Initialize SHAP explainer\n",
        "        if background_texts is None:\n",
        "            # Use a smaller subset of training data as background\n",
        "            background_texts = texts[:20]  # Reduced from 50 to 20\n",
        "\n",
        "        explainer = shap.Explainer(model_wrapper, background_texts)\n",
        "        shap_values = explainer(texts)\n",
        "\n",
        "        return shap_values\n",
        "\n",
        "    def analyze_feature_importance(self, test_texts, test_labels, sample_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Analyze what features the model focuses on for classification\n",
        "        Args:\n",
        "            test_texts: List of test texts\n",
        "            test_labels: List of test labels\n",
        "            sample_ratio: Ratio of samples to analyze (default 0.3 for 30%)\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'human_features': [],\n",
        "            'machine_features': [],\n",
        "            'common_patterns': {}\n",
        "        }\n",
        "\n",
        "        # Calculate number of samples to analyze\n",
        "        total_samples = len(test_texts)\n",
        "        num_samples = int(total_samples * sample_ratio)\n",
        "\n",
        "        print(f\"Analyzing {num_samples} samples ({sample_ratio*100:.0f}%) out of {total_samples} total samples\")\n",
        "\n",
        "        # Sample texts for analysis with stratification to maintain class balance\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        # Create stratified sample\n",
        "        sample_indices, _ = train_test_split(\n",
        "            range(len(test_texts)),\n",
        "            test_size=1-sample_ratio,\n",
        "            stratify=test_labels,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Selected {len(sample_indices)} samples for analysis\")\n",
        "\n",
        "        for i, idx in enumerate(tqdm(sample_indices, desc=\"Generating LIME explanations\")):\n",
        "            text = test_texts[idx]\n",
        "            true_label = test_labels[idx]\n",
        "\n",
        "            # Check if the text is empty or whitespace-only\n",
        "            if not text or text.isspace():\n",
        "                print(f\"Skipping analysis for empty or whitespace-only text at index {idx}\")\n",
        "                continue\n",
        "\n",
        "            # Add a try-except block in case LIME encounters issues with the text\n",
        "            try:\n",
        "                explanation = self.explain_with_lime(text)\n",
        "\n",
        "                # Extract important features\n",
        "                features = explanation.as_list()\n",
        "\n",
        "                if true_label == 0:  # Human\n",
        "                    results['human_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "                else:  # Machine\n",
        "                    results['machine_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating LIME explanation for text at index {idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Find common patterns\n",
        "        from collections import Counter\n",
        "        results['human_patterns'] = Counter(results['human_features']).most_common(20)\n",
        "        results['machine_patterns'] = Counter(results['machine_features']).most_common(20)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_explanations(self, text, explanation_type='lime'):\n",
        "        \"\"\"\n",
        "        Visualize model explanations\n",
        "        \"\"\"\n",
        "        if explanation_type == 'lime':\n",
        "            explanation = self.explain_with_lime(text)\n",
        "\n",
        "            # Show in notebook (if available) or save as HTML\n",
        "            explanation.show_in_notebook(text=True)\n",
        "\n",
        "            # Also create a matplotlib visualization\n",
        "            fig = explanation.as_pyplot_figure()\n",
        "            plt.tight_layout()\n",
        "            return fig\n",
        "\n",
        "    def linguistic_feature_analysis(self, texts, labels, sample_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Analyze linguistic features that distinguish human vs machine text\n",
        "        Args:\n",
        "            texts: List of texts\n",
        "            labels: List of labels\n",
        "            sample_ratio: Ratio of samples to analyze (default 0.3 for 30%)\n",
        "        \"\"\"\n",
        "        # Sample texts for analysis\n",
        "        total_samples = len(texts)\n",
        "        num_samples = int(total_samples * sample_ratio)\n",
        "\n",
        "        # Create stratified sample\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        sample_indices, _ = train_test_split(\n",
        "            range(len(texts)),\n",
        "            test_size=1-sample_ratio,\n",
        "            stratify=labels,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        sampled_texts = [texts[i] for i in sample_indices]\n",
        "        sampled_labels = [labels[i] for i in sample_indices]\n",
        "\n",
        "        print(f\"Running linguistic analysis on {len(sampled_texts)} samples ({sample_ratio*100:.0f}%)\")\n",
        "\n",
        "        features = {\n",
        "            'avg_sentence_length': [],\n",
        "            'lexical_diversity': [],\n",
        "            'repetition_score': []\n",
        "        }\n",
        "\n",
        "        for text in tqdm(sampled_texts, desc=\"Computing linguistic features\"):\n",
        "            # Average sentence length\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            avg_sent_len = np.mean([len(s.split()) for s in sentences if s.strip()])\n",
        "            features['avg_sentence_length'].append(avg_sent_len)\n",
        "\n",
        "            # Lexical diversity (unique words / total words)\n",
        "            words = text.lower().split()\n",
        "            lexical_div = len(set(words)) / len(words) if words else 0\n",
        "            features['lexical_diversity'].append(lexical_div)\n",
        "\n",
        "            # Repetition score (simple measure)\n",
        "            word_counts = Counter(words)\n",
        "            repetition = sum(count - 1 for count in word_counts.values()) / len(words) if words else 0\n",
        "            features['repetition_score'].append(repetition)\n",
        "\n",
        "        # Analyze differences between human and machine text\n",
        "        human_features = {k: [v[i] for i, label in enumerate(sampled_labels) if label == 0]\n",
        "                         for k, v in features.items()}\n",
        "        machine_features = {k: [v[i] for i, label in enumerate(sampled_labels) if label == 1]\n",
        "                           for k, v in features.items()}\n",
        "\n",
        "        return human_features, machine_features, features\n",
        "\n",
        "    def plot_feature_distributions(self, human_features, machine_features):\n",
        "        \"\"\"\n",
        "        Plot distributions of linguistic features\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        feature_names = list(human_features.keys())\n",
        "\n",
        "        for i, feature in enumerate(feature_names):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Plot histograms\n",
        "            ax.hist(human_features[feature], alpha=0.7, label='Human', bins=20)\n",
        "            ax.hist(machine_features[feature], alpha=0.7, label='Machine', bins=20)\n",
        "\n",
        "            ax.set_title(feature.replace('_', ' ').title())\n",
        "            ax.set_xlabel('Value')\n",
        "            ax.set_ylabel('Frequency')\n",
        "            ax.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def error_analysis(self, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Analyze model errors and identify patterns\n",
        "        Args:\n",
        "            test_texts: List of test texts\n",
        "            test_labels: List of test labels\n",
        "            predictions: List of predictions\n",
        "            sample_ratio: Ratio of samples to analyze for detailed error analysis\n",
        "        \"\"\"\n",
        "        # Find all misclassified examples\n",
        "        all_errors = []\n",
        "        for i, (true_label, pred_label) in enumerate(zip(test_labels, predictions)):\n",
        "            if true_label != pred_label:\n",
        "                all_errors.append({\n",
        "                    'text': test_texts[i],\n",
        "                    'true_label': self.class_names[true_label],\n",
        "                    'predicted_label': self.class_names[pred_label],\n",
        "                    'index': i\n",
        "                })\n",
        "\n",
        "        # Sample errors for detailed analysis\n",
        "        num_errors_to_analyze = max(1, int(len(all_errors) * sample_ratio))\n",
        "        errors_to_analyze = np.random.choice(all_errors,\n",
        "                                           min(num_errors_to_analyze, len(all_errors)),\n",
        "                                           replace=False) if all_errors else []\n",
        "\n",
        "        print(f\"Found {len(all_errors)} total errors, analyzing {len(errors_to_analyze)} in detail\")\n",
        "\n",
        "        # Analyze error patterns\n",
        "        error_analysis = {\n",
        "            'total_errors': len(all_errors),\n",
        "            'false_positives': len([e for e in all_errors if e['true_label'] == 'Human']),\n",
        "            'false_negatives': len([e for e in all_errors if e['true_label'] == 'Machine']),\n",
        "            'examples': all_errors[:10]  # Show first 10 errors\n",
        "        }\n",
        "\n",
        "        # Get explanations for sampled error cases\n",
        "        error_explanations = []\n",
        "        for error in tqdm(errors_to_analyze[:5], desc=\"Generating error explanations\"):  # Analyze top 5 sampled errors\n",
        "            try:\n",
        "                explanation = self.explain_with_lime(error['text'])\n",
        "                error_explanations.append({\n",
        "                    'error': error,\n",
        "                    'explanation': explanation.as_list()\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating explanation: {e}\")\n",
        "                continue\n",
        "\n",
        "        error_analysis['explanations'] = error_explanations\n",
        "\n",
        "        return error_analysis\n",
        "\n",
        "    def generate_explanation_report(self, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "        \"\"\"\n",
        "        Generate comprehensive explanation report\n",
        "        Args:\n",
        "            test_texts: List of test texts\n",
        "            test_labels: List of test labels\n",
        "            predictions: List of predictions\n",
        "            sample_ratio: Ratio of samples to use for analysis (default 0.3 for 30%)\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"MODEL EXPLAINABILITY ANALYSIS REPORT\")\n",
        "        print(f\"Analyzing {sample_ratio*100:.0f}% of data for efficiency\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Feature importance analysis\n",
        "        print(\"\\n1. FEATURE IMPORTANCE ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        feature_analysis = self.analyze_feature_importance(test_texts, test_labels, sample_ratio)\n",
        "\n",
        "        print(\"Top features indicating HUMAN text:\")\n",
        "        for feature, count in feature_analysis['human_patterns'][:10]:\n",
        "            print(f\"  - '{feature}': {count} occurrences\")\n",
        "\n",
        "        print(\"\\nTop features indicating MACHINE text:\")\n",
        "        for feature, count in feature_analysis['machine_patterns'][:10]:\n",
        "            print(f\"  - '{feature}': {count} occurrences\")\n",
        "\n",
        "        # Linguistic analysis\n",
        "        print(\"\\n2. LINGUISTIC FEATURE ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        human_features, machine_features, all_features = self.linguistic_feature_analysis(\n",
        "            test_texts, test_labels, sample_ratio\n",
        "        )\n",
        "\n",
        "        for feature_name in human_features.keys():\n",
        "            human_mean = np.mean(human_features[feature_name])\n",
        "            machine_mean = np.mean(machine_features[feature_name])\n",
        "            print(f\"{feature_name.replace('_', ' ').title()}:\")\n",
        "            print(f\"  Human avg: {human_mean:.3f}, Machine avg: {machine_mean:.3f}\")\n",
        "\n",
        "        # Error analysis\n",
        "        print(\"\\n3. ERROR ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        error_analysis = self.error_analysis(test_texts, test_labels, predictions, sample_ratio)\n",
        "\n",
        "        print(f\"Total classification errors: {error_analysis['total_errors']}\")\n",
        "        print(f\"False positives (Human → Machine): {error_analysis['false_positives']}\")\n",
        "        print(f\"False negatives (Machine → Human): {error_analysis['false_negatives']}\")\n",
        "\n",
        "        print(\"\\nExample misclassifications:\")\n",
        "        for i, example in enumerate(error_analysis['examples'][:3]):\n",
        "            print(f\"  {i+1}. True: {example['true_label']}, Predicted: {example['predicted_label']}\")\n",
        "            print(f\"     Text: {example['text'][:100]}...\")\n",
        "\n",
        "        return {\n",
        "            'feature_analysis': feature_analysis,\n",
        "            'linguistic_analysis': (human_features, machine_features),\n",
        "            'error_analysis': error_analysis\n",
        "        }\n",
        "\n",
        "\n",
        "# Updated usage function with sampling control\n",
        "def add_explainability_to_classifier(classifier, test_texts, test_labels, predictions, sample_ratio=0.3):\n",
        "    \"\"\"\n",
        "    Add explainability analysis to existing classifier\n",
        "    Args:\n",
        "        classifier: Trained classifier instance\n",
        "        test_texts: List of test texts\n",
        "        test_labels: List of test labels\n",
        "        predictions: List of predictions\n",
        "        sample_ratio: Ratio of samples to analyze (default 0.3 for 30%)\n",
        "    \"\"\"\n",
        "    explainer = TextExplainabilityAnalyzer(\n",
        "        classifier.model,\n",
        "        classifier.tokenizer,\n",
        "        classifier.device\n",
        "    )\n",
        "\n",
        "    # Generate comprehensive report with sampling\n",
        "    report = explainer.generate_explanation_report(\n",
        "        test_texts, test_labels, predictions, sample_ratio\n",
        "    )\n",
        "\n",
        "    # Create visualizations\n",
        "    human_features, machine_features = report['linguistic_analysis']\n",
        "    fig = explainer.plot_feature_distributions(human_features, machine_features)\n",
        "    plt.show()\n",
        "\n",
        "    return explainer, report\n",
        "\n",
        "\n",
        "# Updated main function\n",
        "def main():\n",
        "    \"\"\"Main function to run the Zulu text classification with AfriBERTa\"\"\"\n",
        "\n",
        "    # Assuming balanced_df is available in your environment\n",
        "    df = balanced_df\n",
        "\n",
        "    # Initialize classifier with AfriBERTa\n",
        "    classifier = ZuluTextClassifier(model_name='castorini/afriberta_base')\n",
        "\n",
        "    # Prepare data\n",
        "    X_train, X_test, y_train, y_test = classifier.prepare_data(df)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset, test_dataset = classifier.create_datasets(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Train model with LoRA (recommended for efficiency)\n",
        "    trainer = classifier.train(\n",
        "        train_dataset,\n",
        "        test_dataset,\n",
        "        use_lora=True,\n",
        "        epochs=5,\n",
        "        batch_size=8,\n",
        "        learning_rate=5e-5\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    results = classifier.evaluate(test_dataset, trainer)\n",
        "    classifier.print_evaluation_results(results)\n",
        "\n",
        "    # Run explainability analysis on 30% of data\n",
        "    explainer, report = add_explainability_to_classifier(\n",
        "        classifier, X_test, y_test, results['y_pred'], sample_ratio=0.3\n",
        "    )\n",
        "\n",
        "    # Test single prediction\n",
        "    sample_text = \"Ezinye izisindo ze-XLMClassification azizange ziqaliswe endaweni yokuhlola yemodeli e-castorini futhi zisanda kuqaliswa.\"\n",
        "    prediction = classifier.predict_single_text(sample_text)\n",
        "    print(f\"\\nSample prediction for '{sample_text}':\")\n",
        "    print(f\"Predicted: {prediction['predicted_class']} (confidence: {prediction['confidence']:.4f})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}