{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrTR81pFjl52Nm/DCX3lui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/train-model/Afriberta_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DCT-EdJT5TS"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets fsspec\n",
        "!pip install shap lime textstat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_and_reformat_data(df, text_column='text', language='zul', clean_text_flag=True, label=None):\n",
        "    \"\"\"\n",
        "    Returns a cleaned DataFrame with:\n",
        "    - Original text ('text')\n",
        "    - Cleaned text ('cleaned_text')\n",
        "    - Word tokens ('tokens')\n",
        "    - Language label ('language')\n",
        "    - Optional class label ('label')\n",
        "    \"\"\"\n",
        "\n",
        "    def clean_text(text):\n",
        "        \"\"\"Cleans text while preserving original\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        cleaned = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        cleaned = re.sub(r'[^a-zA-Z\\s]', '', cleaned)\n",
        "        cleaned = cleaned.lower()\n",
        "        cleaned = ' '.join(cleaned.split())\n",
        "        return cleaned\n",
        "\n",
        "    result = {\n",
        "        'text': [],\n",
        "        'cleaned_text': [],\n",
        "        'tokens': [],\n",
        "        'language': [],\n",
        "        'label': []\n",
        "    }\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Get raw text\n",
        "        raw_text = row.get(text_column, '') or row.get('answers', '')\n",
        "\n",
        "        # Skip empty entries\n",
        "        if pd.isna(raw_text) or raw_text == '' or (isinstance(raw_text, (list, tuple)) and len(raw_text) == 0):\n",
        "            continue\n",
        "\n",
        "        # Handle list-type text\n",
        "        if isinstance(raw_text, (list, tuple)):\n",
        "            raw_text = ' '.join([str(t).strip() for t in raw_text if str(t).strip()])\n",
        "            if not raw_text:\n",
        "                continue\n",
        "\n",
        "        # Clean text\n",
        "        cleaned = clean_text(raw_text) if clean_text_flag else raw_text\n",
        "        if not cleaned.strip():\n",
        "            continue\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(cleaned)\n",
        "\n",
        "        # Store results\n",
        "        result['text'].append(raw_text)\n",
        "        result['cleaned_text'].append(cleaned)\n",
        "        result['tokens'].append(tokens)\n",
        "        result['language'].append(language)\n",
        "        result['label'].append(label if label is not None else row.get('label', None))\n",
        "\n",
        "    return pd.DataFrame(result)"
      ],
      "metadata": {
        "id": "QuHPtQP5CWb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from huggingface_hub import login\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "#shona_data = load_dataset(\"DigitalUmuganda/AfriVoice\", \"sn\", streaming=True, split=\"train[:10%]\")\n",
        "\n",
        "# english_data = load_dataset(\"oscar-corpus/OSCAR-2201\", language=\"en\", streaming=True)\n",
        "\n",
        "# num_samples = 500\n",
        "# samples = []\n",
        "# for i, example in enumerate(english_data[\"train\"]):\n",
        "#     if i >= num_samples:\n",
        "#         break\n",
        "#     samples.append(example['text'])\n",
        "\n",
        "# eng_df = pd.DataFrame(samples, columns=['text'])\n",
        "# eng_df.head()\n",
        "\n",
        "zulu_data = load_dataset(\"dsfsi/vukuzenzele-monolingual\", \"zul\")\n",
        "zulu_data2 = load_dataset(\"masakhane/afriqa\", \"zul\")\n",
        "\n",
        "combined_data = pd.concat([\n",
        "    zulu_data['train'].to_pandas(),\n",
        "    zulu_data['test'].to_pandas()\n",
        "])\n",
        "\n",
        "combined_data2 = pd.concat([\n",
        "    zulu_data2['train'].to_pandas(),\n",
        "    zulu_data2['validation'].to_pandas(),\n",
        "    zulu_data2['test'].to_pandas()\n",
        "])\n",
        "\n",
        "zul_df1 = clean_and_reformat_data(combined_data, text_column='text', language='zul', clean_text_flag=True, label='0')\n",
        "#zul_df1['tokens'] = zul_df1['cleaned_text'].apply(word_tokenize)\n",
        "#zul_df1['label'] = 0\n",
        "#zul_df1.head()\n",
        "\n",
        "zul_df2 = clean_and_reformat_data(combined_data2, text_column='answer', language='zul', clean_text_flag=True, label='0')\n",
        "#zul_df2.head()\n",
        "\n",
        "zul_df = pd.concat([zul_df1, zul_df2], ignore_index=True)\n",
        "zul_df.head()"
      ],
      "metadata": {
        "id": "n8iYGKo6Mpgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eng_df['cleaned_text'] = eng_df['text'].apply(clean_text)\n",
        "# eng_df['tokens'] = eng_df['cleaned_text'].apply(word_tokenize)\n",
        "# print(eng_df.head())\n",
        "\n",
        "# zul_df['cleaned_text'] = zul_df['text'].apply(clean_text)\n",
        "# zul_df['tokens'] = zul_df['cleaned_text'].apply(word_tokenize)\n",
        "# zul_df.head()\n",
        "\n",
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "en_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "\n",
        "english_prompts = [\n",
        "    \"Explain the significance of lobola in Southern Africa\",\n",
        "    \"Write a short dialogue between two friends in Johannesburg\",\n",
        "    \"Describe linguistic features that make isiZulu agglutinative\"\n",
        "]\n",
        "\n",
        "zulu_prompts = [\n",
        "    \"Chaza ngokubaluleka kwesiko lwelobola eNingizimu Afrika\",\n",
        "    \"Bhala inkulumo emfushane phakathi kwabangani ababili eGoli\",\n",
        "    \"Landela indaba yamaZulu ngokomlando\",\n",
        "    \"Chaza ngamasiko amasha eZulu eskhathini samanje\",\n",
        "    \"Bhala inganekwane ethi 'UNogwaja noFudu'\"\n",
        "]\n",
        "\n",
        "# def generate_with_prompts(prompts, generator, language, samples_per_prompt=3):\n",
        "#     data = []\n",
        "#     for prompt in prompts:\n",
        "#         for _ in range(samples_per_prompt):\n",
        "#             output = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
        "#             data.append({\n",
        "#                 'prompt': prompt,\n",
        "#                 'text': output[0]['generated_text'],\n",
        "#                 'label': 'machine',\n",
        "#                 'language': language,\n",
        "#                 'prompt_type': 'cultural' if \"tsika\" in prompt else 'linguistic'  # Tag for analysis\n",
        "#             })\n",
        "#     return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# eng_mg_df = generate_with_prompts(english_prompts, en_generator, 'English')\n",
        "# eng_mg_df['cleaned_text'] = eng_mg_df['text'].apply(clean_text)\n",
        "# eng_mg_df['tokens'] = eng_mg_df['cleaned_text'].apply(word_tokenize)\n",
        "# print(eng_mg_df['text'].iloc[0][:300])\n",
        "\n",
        "def load_jsonl_data(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:  # Skip empty lines\n",
        "                    try:\n",
        "                        data.append(json.loads(line))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"Error parsing JSON line: {e}\")\n",
        "                        continue\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return []\n",
        "\n",
        "    return data\n",
        "\n",
        "zul_mg_df = pd.DataFrame(load_jsonl_data('zulu_mg_text.jsonl'))\n",
        "zul_mg_df2 = pd.DataFrame(load_jsonl_data('mg_text.jsonl'))\n",
        "\n",
        "\n",
        "zul_mg_df1 = clean_and_reformat_data(zul_mg_df, text_column='text', language='zul', clean_text_flag=True, label='1')\n",
        "zul_mg_df2 = clean_and_reformat_data(zul_mg_df2, text_column='text', language='zul', clean_text_flag=True, label='1')\n",
        "\n",
        "zul_mg_df = pd.concat([zul_mg_df1, zul_mg_df2], ignore_index=True)\n",
        "zul_mg_df.head()\n"
      ],
      "metadata": {
        "id": "n9Iw-T-_M91O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMBINE THE DATASETS\n",
        "-------------------------"
      ],
      "metadata": {
        "id": "KKhdMXTaNC30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both datasets, this will be bad for us if we have more than 70% differnce in data length\n",
        "# Combine both datasets\n",
        "all_texts = pd.concat([zul_df, zul_mg_df], ignore_index=True)\n",
        "all_texts_clean = all_texts[\n",
        "    (all_texts['text'].notna()) &  # Remove NaN/None\n",
        "    (all_texts['text'].astype(str).str.strip() != '[]') &  # Remove empty lists\n",
        "    (all_texts['text'].astype(str).str.strip() != \"['']\") &  # Remove lists with empty strings\n",
        "    (all_texts['text'].astype(str).str.strip() != '') &  # Remove empty strings\n",
        "    (all_texts['text'].str.len() > 0)  # Remove zero-length strings\n",
        "].copy()\n",
        "\n",
        "all_texts_shuffled = all_texts_clean.sample(frac=1).reset_index(drop=True)\n",
        "print(f\"Total texts after combining: {len(all_texts)}\")\n",
        "print(f\"Human-written texts: {len(zul_df)}\")\n",
        "print(f\"Machine-generated texts: {len(zul_mg_df)}\")\n",
        "\n",
        "# Create a DataFrame for easier analysis\n",
        "combined_df = pd.DataFrame(all_texts_shuffled)\n",
        "combined_df.head()\n",
        "\n",
        "# Create a balanced dataset with equal samples from each class\n",
        "# we can choose to use this of the combined one\n",
        "def create_balanced_dataset(df, target_size_per_class=None):\n",
        "    class_counts = df['label'].value_counts()\n",
        "    min_class_size = class_counts.min()\n",
        "    if target_size_per_class:\n",
        "        sample_size = min(target_size_per_class, min_class_size)\n",
        "    else:\n",
        "        sample_size = min_class_size\n",
        "    balanced_df = df.groupby('label').sample(n=sample_size, random_state=42)\n",
        "    return balanced_df.reset_index(drop=True)\n",
        "\n",
        "# Create balanced dataset, because we need the same number of samples for each class\n",
        "balanced_df = create_balanced_dataset(combined_df)\n",
        "print(f\"\\nBalanced dataset created with {len(balanced_df)} samples\")\n",
        "print(f\"Label distribution in balanced dataset: \\n{balanced_df['label'].value_counts()}\")\n",
        "\n",
        "# Save balanced dataset\n",
        "balanced_output_path = 'balanced_zulu_texts.csv'\n",
        "balanced_df.to_csv(balanced_output_path, index=False, encoding='utf-8')\n",
        "print(f\"Balanced dataset saved to: {balanced_output_path}\")\n",
        "balanced_df.head()"
      ],
      "metadata": {
        "id": "HO3JSaWjNIA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFbvZ3v9rd_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28z9e72eMM9v"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import shap\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import re\n",
        "from textstat import flesch_reading_ease, automated_readability_index\n",
        "from collections import Counter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class ZuluTextDataset(Dataset):\n",
        "    \"\"\"Custom Dataset class for Zulu text classification\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class ZuluTextClassifier:\n",
        "    \"\"\"AfriBERTa classifier for Zulu human vs machine text detection\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='castorini/afriberta_base', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize tokenizer and model with AfriBERTa\n",
        "        print(f\"Loading AfriBERTa model: {model_name}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\"Model initialized on {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "    def prepare_data(self, df, text_column='cleaned_text', label_column='label', test_size=0.2):\n",
        "        \"\"\"Prepare and split the data\"\"\"\n",
        "        # Filter for Zulu language if language column exists\n",
        "        if 'language' in df.columns:\n",
        "            df_zulu = df[df['language'].str.lower().isin(['zul'])].copy()\n",
        "            print(f\"Filtered {len(df_zulu)} Zulu samples from {len(df)} total samples\")\n",
        "        else:\n",
        "            df_zulu = df.copy()\n",
        "            print(f\"Using all {len(df_zulu)} samples (no language filtering)\")\n",
        "\n",
        "        if len(df_zulu) == 0:\n",
        "            raise ValueError(\"No Zulu samples found in the dataset\")\n",
        "\n",
        "        # Extract texts and labels\n",
        "        texts = df_zulu[text_column].tolist()\n",
        "        labels = df_zulu[label_column].tolist()\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            texts, labels,\n",
        "            test_size=test_size,\n",
        "            random_state=42,\n",
        "            stratify=labels\n",
        "        )\n",
        "\n",
        "        print(f\"Training samples: {len(X_train)}\")\n",
        "        print(f\"Test samples: {len(X_test)}\")\n",
        "        print(f\"Label distribution in training: {np.bincount(y_train)}\")\n",
        "        print(f\"Label distribution in test: {np.bincount(y_test)}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def create_datasets(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Create PyTorch datasets\"\"\"\n",
        "        train_dataset = ZuluTextDataset(X_train, y_train, self.tokenizer, self.max_length)\n",
        "        test_dataset = ZuluTextDataset(X_test, y_test, self.tokenizer, self.max_length)\n",
        "\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    def setup_lora(self, use_lora=True, lora_r=16, lora_alpha=32, lora_dropout=0.1):\n",
        "        \"\"\"Setup LoRA for parameter-efficient fine-tuning\"\"\"\n",
        "        if use_lora:\n",
        "            print(\"Setting up LoRA configuration...\")\n",
        "            lora_config = LoraConfig(\n",
        "                r=lora_r,\n",
        "                lora_alpha=lora_alpha,\n",
        "                target_modules=[\"query\", \"key\", \"value\"],\n",
        "                lora_dropout=lora_dropout,\n",
        "                bias=\"none\",\n",
        "                task_type=\"SEQ_CLS\"\n",
        "            )\n",
        "\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            print(f\"LoRA enabled. Trainable parameters: {self.model.num_parameters()}\")\n",
        "        else:\n",
        "            print(\"Using full fine-tuning (no LoRA)\")\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset, output_dir='./afriberta_zulu_classifier_results',\n",
        "              use_lora=True, epochs=5, batch_size=8, learning_rate=5e-5):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "\n",
        "        # Setup LoRA if requested\n",
        "        self.setup_lora(use_lora=use_lora)\n",
        "\n",
        "        # Training arguments optimized for AfriBERTa\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=epochs,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            learning_rate=learning_rate,\n",
        "            logging_dir=f'{output_dir}/logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",  # Disable wandb\n",
        "            seed=42,\n",
        "            dataloader_pin_memory=False,  # Better for some setups\n",
        "            fp16=torch.cuda.is_available(),  # Use mixed precision if available\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Starting training with AfriBERTa...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def evaluate(self, test_dataset, trainer=None):\n",
        "        \"\"\"Evaluate the model\"\"\"\n",
        "        if trainer is None:\n",
        "            # Create a new trainer for evaluation\n",
        "            trainer = Trainer(model=self.model)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "        y_true = predictions.label_ids\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "        # Per-class metrics\n",
        "        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average=None\n",
        "        )\n",
        "\n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'precision_per_class': precision_per_class,\n",
        "            'recall_per_class': recall_per_class,\n",
        "            'f1_per_class': f1_per_class,\n",
        "            'support': support,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_evaluation_results(self, results):\n",
        "        \"\"\"Print detailed evaluation results\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"AFRIBERTA ZULU CLASSIFIER - EVALUATION RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        print(f\"Overall Accuracy: {results['accuracy']:.4f}\")\n",
        "        print(f\"Weighted Precision: {results['precision']:.4f}\")\n",
        "        print(f\"Weighted Recall: {results['recall']:.4f}\")\n",
        "        print(f\"Weighted F1-Score: {results['f1']:.4f}\")\n",
        "\n",
        "        print(\"\\nPer-Class Results:\")\n",
        "        class_names = ['Human', 'Machine']\n",
        "        for i in range(len(class_names)):\n",
        "            print(f\"{class_names[i]}:\")\n",
        "            print(f\"  Precision: {results['precision_per_class'][i]:.4f}\")\n",
        "            print(f\"  Recall: {results['recall_per_class'][i]:.4f}\")\n",
        "            print(f\"  F1-Score: {results['f1_per_class'][i]:.4f}\")\n",
        "            print(f\"  Support: {results['support'][i]}\")\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(results['y_true'], results['y_pred'],\n",
        "                                  target_names=class_names))\n",
        "\n",
        "    def predict_single_text(self, text):\n",
        "        \"\"\"Predict a single text sample\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tokenize the input\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "\n",
        "        # Get prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encoding)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = predictions.max().item()\n",
        "\n",
        "        class_names = ['Human', 'Machine']\n",
        "        return {\n",
        "            'predicted_class': class_names[predicted_class],\n",
        "            'confidence': confidence,\n",
        "            'probabilities': {\n",
        "                'Human': predictions[0][0].item(),\n",
        "                'Machine': predictions[0][1].item()\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class TextExplainabilityAnalyzer:\n",
        "    \"\"\"\n",
        "    Explainability analysis for machine vs human text classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.class_names = ['Human', 'Machine']\n",
        "\n",
        "        # Initialize LIME explainer\n",
        "        self.lime_explainer = LimeTextExplainer(\n",
        "            class_names=self.class_names\n",
        "        )\n",
        "\n",
        "    def predict_proba_for_lime(self, texts):\n",
        "        \"\"\"\n",
        "        Prediction function compatible with LIME\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        self.model.eval()\n",
        "\n",
        "        for text in texts:\n",
        "            # Tokenize\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=512,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
        "\n",
        "            # Get prediction\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encoding)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predictions.append(probs.cpu().numpy()[0])\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def explain_with_lime(self, text, num_features=10):\n",
        "        \"\"\"\n",
        "        Generate LIME explanation for a single text\n",
        "        \"\"\"\n",
        "        explanation = self.lime_explainer.explain_instance(\n",
        "            text,\n",
        "            self.predict_proba_for_lime,\n",
        "            num_features=num_features,\n",
        "            num_samples=1000\n",
        "        )\n",
        "\n",
        "        return explanation\n",
        "\n",
        "    def explain_with_shap(self, texts, background_texts=None):\n",
        "        \"\"\"\n",
        "        Generate SHAP explanations for texts\n",
        "        \"\"\"\n",
        "        # Create a wrapper for SHAP\n",
        "        def model_wrapper(texts):\n",
        "            return self.predict_proba_for_lime(texts)\n",
        "\n",
        "        # Initialize SHAP explainer\n",
        "        if background_texts is None:\n",
        "            # Use a subset of training data as background\n",
        "            background_texts = texts[:50]  # Adjust size as needed\n",
        "\n",
        "        explainer = shap.Explainer(model_wrapper, background_texts)\n",
        "        shap_values = explainer(texts)\n",
        "\n",
        "        return shap_values\n",
        "\n",
        "    def analyze_feature_importance(self, test_texts, test_labels, num_samples=100):\n",
        "        \"\"\"\n",
        "        Analyze what features the model focuses on for classification\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'human_features': [],\n",
        "            'machine_features': [],\n",
        "            'common_patterns': {}\n",
        "        }\n",
        "\n",
        "        # Sample texts for analysis\n",
        "        sample_indices = np.random.choice(len(test_texts), min(num_samples, len(test_texts)), replace=False)\n",
        "\n",
        "        for idx in sample_indices:\n",
        "            text = test_texts[idx]\n",
        "            true_label = test_labels[idx]\n",
        "\n",
        "            # Check if the text is empty or whitespace-only\n",
        "            if not text or text.isspace():\n",
        "                print(f\"Skipping analysis for empty or whitespace-only text at index {idx}\")\n",
        "                continue # Skip this sample if it has no words\n",
        "\n",
        "\n",
        "            # Get LIME explanation\n",
        "            # Add a try-except block in case LIME encounters issues with the text\n",
        "            try:\n",
        "                explanation = self.explain_with_lime(text)\n",
        "\n",
        "                # Extract important features\n",
        "                features = explanation.as_list()\n",
        "\n",
        "                if true_label == 0:  # Human\n",
        "                    results['human_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "                else:  # Machine\n",
        "                    results['machine_features'].extend([f[0] for f in features if f[1] > 0])\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating LIME explanation for text at index {idx}: {e}\")\n",
        "                continue # Skip this sample if explanation fails\n",
        "\n",
        "\n",
        "        # Find common patterns\n",
        "        from collections import Counter\n",
        "        results['human_patterns'] = Counter(results['human_features']).most_common(20)\n",
        "        results['machine_patterns'] = Counter(results['machine_features']).most_common(20)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def visualize_explanations(self, text, explanation_type='lime'):\n",
        "        \"\"\"\n",
        "        Visualize model explanations\n",
        "        \"\"\"\n",
        "        if explanation_type == 'lime':\n",
        "            explanation = self.explain_with_lime(text)\n",
        "\n",
        "            # Show in notebook (if available) or save as HTML\n",
        "            explanation.show_in_notebook(text=True)\n",
        "\n",
        "            # Also create a matplotlib visualization\n",
        "            fig = explanation.as_pyplot_figure()\n",
        "            plt.tight_layout()\n",
        "            return fig\n",
        "\n",
        "    def linguistic_feature_analysis(self, texts, labels):\n",
        "        \"\"\"\n",
        "        Analyze linguistic features that distinguish human vs machine text\n",
        "        \"\"\"\n",
        "\n",
        "        features = {\n",
        "            'avg_sentence_length': [],\n",
        "            'lexical_diversity': [],\n",
        "            'repetition_score': []\n",
        "        }\n",
        "\n",
        "        for text in texts:\n",
        "            # Average sentence length\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            avg_sent_len = np.mean([len(s.split()) for s in sentences if s.strip()])\n",
        "            features['avg_sentence_length'].append(avg_sent_len)\n",
        "\n",
        "            # Lexical diversity (unique words / total words)\n",
        "            words = text.lower().split()\n",
        "            lexical_div = len(set(words)) / len(words) if words else 0\n",
        "            features['lexical_diversity'].append(lexical_div)\n",
        "\n",
        "            # Repetition score (simple measure)\n",
        "            word_counts = Counter(words)\n",
        "            repetition = sum(count - 1 for count in word_counts.values()) / len(words) if words else 0\n",
        "            features['repetition_score'].append(repetition)\n",
        "\n",
        "        # Analyze differences between human and machine text\n",
        "        human_features = {k: [v[i] for i, label in enumerate(labels) if label == 0]\n",
        "                         for k, v in features.items()}\n",
        "        machine_features = {k: [v[i] for i, label in enumerate(labels) if label == 1]\n",
        "                           for k, v in features.items()}\n",
        "\n",
        "        return human_features, machine_features, features\n",
        "\n",
        "    def plot_feature_distributions(self, human_features, machine_features):\n",
        "        \"\"\"\n",
        "        Plot distributions of linguistic features\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        feature_names = list(human_features.keys())\n",
        "\n",
        "        for i, feature in enumerate(feature_names):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Plot histograms\n",
        "            ax.hist(human_features[feature], alpha=0.7, label='Human', bins=20)\n",
        "            ax.hist(machine_features[feature], alpha=0.7, label='Machine', bins=20)\n",
        "\n",
        "            ax.set_title(feature.replace('_', ' ').title())\n",
        "            ax.set_xlabel('Value')\n",
        "            ax.set_ylabel('Frequency')\n",
        "            ax.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def error_analysis(self, test_texts, test_labels, predictions):\n",
        "        \"\"\"\n",
        "        Analyze model errors and identify patterns\n",
        "        \"\"\"\n",
        "        # Find misclassified examples\n",
        "        errors = []\n",
        "        for i, (true_label, pred_label) in enumerate(zip(test_labels, predictions)):\n",
        "            if true_label != pred_label:\n",
        "                errors.append({\n",
        "                    'text': test_texts[i],\n",
        "                    'true_label': self.class_names[true_label],\n",
        "                    'predicted_label': self.class_names[pred_label],\n",
        "                    'index': i\n",
        "                })\n",
        "\n",
        "        # Analyze error patterns\n",
        "        error_analysis = {\n",
        "            'total_errors': len(errors),\n",
        "            'false_positives': len([e for e in errors if e['true_label'] == 'Human']),\n",
        "            'false_negatives': len([e for e in errors if e['true_label'] == 'Machine']),\n",
        "            'examples': errors[:10]  # Show first 10 errors\n",
        "        }\n",
        "\n",
        "        # Get explanations for error cases\n",
        "        error_explanations = []\n",
        "        for error in errors[:5]:  # Analyze first 5 errors\n",
        "            explanation = self.explain_with_lime(error['text'])\n",
        "            error_explanations.append({\n",
        "                'error': error,\n",
        "                'explanation': explanation.as_list()\n",
        "            })\n",
        "\n",
        "        error_analysis['explanations'] = error_explanations\n",
        "\n",
        "        return error_analysis\n",
        "\n",
        "    def generate_explanation_report(self, test_texts, test_labels, predictions):\n",
        "        \"\"\"\n",
        "        Generate comprehensive explanation report\n",
        "        \"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"MODEL EXPLAINABILITY ANALYSIS REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Feature importance analysis\n",
        "        print(\"\\n1. FEATURE IMPORTANCE ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        feature_analysis = self.analyze_feature_importance(test_texts, test_labels)\n",
        "\n",
        "        print(\"Top features indicating HUMAN text:\")\n",
        "        for feature, count in feature_analysis['human_patterns'][:10]:\n",
        "            print(f\"  - '{feature}': {count} occurrences\")\n",
        "\n",
        "        print(\"\\nTop features indicating MACHINE text:\")\n",
        "        for feature, count in feature_analysis['machine_patterns'][:10]:\n",
        "            print(f\"  - '{feature}': {count} occurrences\")\n",
        "\n",
        "        # Linguistic analysis\n",
        "        print(\"\\n2. LINGUISTIC FEATURE ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        human_features, machine_features, all_features = self.linguistic_feature_analysis(test_texts, test_labels)\n",
        "\n",
        "        for feature_name in human_features.keys():\n",
        "            human_mean = np.mean(human_features[feature_name])\n",
        "            machine_mean = np.mean(machine_features[feature_name])\n",
        "            print(f\"{feature_name.replace('_', ' ').title()}:\")\n",
        "            print(f\"  Human avg: {human_mean:.3f}, Machine avg: {machine_mean:.3f}\")\n",
        "\n",
        "        # Error analysis\n",
        "        print(\"\\n3. ERROR ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        error_analysis = self.error_analysis(test_texts, test_labels, predictions)\n",
        "\n",
        "        print(f\"Total classification errors: {error_analysis['total_errors']}\")\n",
        "        print(f\"False positives (Human → Machine): {error_analysis['false_positives']}\")\n",
        "        print(f\"False negatives (Machine → Human): {error_analysis['false_negatives']}\")\n",
        "\n",
        "        print(\"\\nExample misclassifications:\")\n",
        "        for i, example in enumerate(error_analysis['examples'][:3]):\n",
        "            print(f\"  {i+1}. True: {example['true_label']}, Predicted: {example['predicted_label']}\")\n",
        "            print(f\"     Text: {example['text'][:100]}...\")\n",
        "\n",
        "        return {\n",
        "            'feature_analysis': feature_analysis,\n",
        "            'linguistic_analysis': (human_features, machine_features),\n",
        "            'error_analysis': error_analysis\n",
        "        }\n",
        "\n",
        "# Usage example:\n",
        "def add_explainability_to_classifier(classifier, test_texts, test_labels, predictions):\n",
        "    \"\"\"\n",
        "    Add explainability analysis to existing classifier\n",
        "    \"\"\"\n",
        "    explainer = TextExplainabilityAnalyzer(\n",
        "        classifier.model,\n",
        "        classifier.tokenizer,\n",
        "        classifier.device\n",
        "    )\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    report = explainer.generate_explanation_report(test_texts, test_labels, predictions)\n",
        "\n",
        "    # Create visualizations\n",
        "    human_features, machine_features = report['linguistic_analysis']\n",
        "    fig = explainer.plot_feature_distributions(human_features, machine_features)\n",
        "    plt.show()\n",
        "\n",
        "    return explainer, report\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the Zulu text classification with AfriBERTa\"\"\"\n",
        "\n",
        "    # Assuming balanced_df is available in your environment\n",
        "    df = balanced_df\n",
        "\n",
        "    # Initialize classifier with AfriBERTa\n",
        "    classifier = ZuluTextClassifier(model_name='castorini/afriberta_base')\n",
        "\n",
        "    # Prepare data\n",
        "    X_train, X_test, y_train, y_test = classifier.prepare_data(df)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset, test_dataset = classifier.create_datasets(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Train model with LoRA (recommended for efficiency)\n",
        "    trainer = classifier.train(\n",
        "        train_dataset,\n",
        "        test_dataset,\n",
        "        use_lora=True,  # Set to False for full fine-tuning\n",
        "        epochs=5,\n",
        "        batch_size=8,\n",
        "        learning_rate=5e-5\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    results = classifier.evaluate(test_dataset, trainer)\n",
        "    classifier.print_evaluation_results(results)\n",
        "\n",
        "    explainer, report = add_explainability_to_classifier(\n",
        "    classifier, X_test, y_test, results['y_pred']\n",
        "    )\n",
        "\n",
        "    sample_text = \"Ezinye izisindo ze-XLMClassification azizange ziqaliswe endaweni yokuhlola yemodeli e-castorini futhi zisanda kuqaliswa. KUFANELE UQEQESHE le modeli emsebenzini osakaza phansi ukuze ukwazi ukuyisebenzisela ukuqagela nokukhomba.\"  # Example Zulu text\n",
        "    prediction = classifier.predict_single_text(sample_text)\n",
        "    print(f\"\\nSample prediction for '{sample_text}':\")\n",
        "    print(f\"Predicted: {prediction['predicted_class']} (confidence: {prediction['confidence']:.4f})\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}