{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanda9805/Detecting-Machine-Generated-Texts/blob/train-model/Final_COS760_project_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Detecting Machine-Generated Text in African Languages\n",
        "\n",
        "## 1. Introduction and Setup\n",
        "\n",
        "This notebook implements the methodology for detecting machine-generated text in English and isiZulu. We will follow these steps:\n",
        "1.  **Data Collection & Preprocessing**: Load, clean, and balance datasets for human and machine-generated text.\n",
        "2.  **Model Development**: Fine-tune multilingual models (Afro-XLMR for isiZulu, XLM-RoBERTa for English) for binary classification.\n",
        "3.  **Evaluation & Analysis**:\n",
        "    - Evaluate the isiZulu model's performance.\n",
        "    - Conduct a zero-shot evaluation by testing the English-trained model on isiZulu data.\n",
        "    - Use SHAP for explainability and perform error analysis.\n",
        "4. **Interactive Demonstrator**: A final tool for real-time text analysis."
      ],
      "metadata": {
        "id": "3rIRTb9nJj8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBVwGc9HJZ6o"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers adapters datasets fsspec evaluate shap nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK for text processing\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Hugging Face for models and datasets\n",
        "from datasets import load_dataset, Dataset, ClassLabel\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# Evaluation and Explainability\n",
        "import evaluate\n",
        "import shap\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Setup\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "ruajuYSWKaLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Collection & Preprocessing\n",
        "\n",
        "We define helper functions to load human and machine text, clean it, and create balanced datasets for training."
      ],
      "metadata": {
        "id": "O53DryWALXPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic text cleaning\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Load machine-generated text from a JSONL file\n",
        "def load_machine_text(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                if line.strip():\n",
        "                    data.append(json.loads(line))\n",
        "        df = pd.DataFrame(data)\n",
        "        # Handle cases where text is a list of strings\n",
        "        if not df.empty and isinstance(df['text'].iloc[0], list):\n",
        "            df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Machine-generated text file not found at {file_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load, process and balance data for a given language\n",
        "def prepare_dataset(lang_code, human_data_source, machine_data_path, output_csv_path):\n",
        "    print(f\"Preparing dataset for language: {lang_code.upper()}\")\n",
        "\n",
        "    # Load Human Data (Label = 0)\n",
        "    human_dataset = load_dataset(human_data_source, lang_code)\n",
        "    human_df = human_dataset['train'].to_pandas()\n",
        "    human_df = human_df[['text']].dropna()\n",
        "    human_df['label'] = 0\n",
        "    print(f\"Loaded {len(human_df)} human-written data.\")\n",
        "\n",
        "    # Load Machine Data (Label = 1)\n",
        "    machine_df = load_machine_text(machine_data_path)\n",
        "    if machine_df.empty:\n",
        "        return None\n",
        "    machine_df = machine_df[['text']].dropna()\n",
        "    machine_df['label'] = 1\n",
        "    print(f\"Loaded {len(machine_df)} machine-generated data.\")\n",
        "\n",
        "    # Combine and Clean\n",
        "    combined_df = pd.concat([human_df, machine_df], ignore_index=True)\n",
        "    combined_df['text'] = combined_df['text'].apply(clean_text)\n",
        "    combined_df.dropna(subset=['text'], inplace=True)\n",
        "    # combined_df = combined_df[combined_df['text'].str.len() > 10] # Remove very short texts\n",
        "\n",
        "    # Create Balanced Dataset\n",
        "    min_class_size = combined_df['label'].value_counts().min()\n",
        "    balanced_df = combined_df.groupby('label').sample(n=min_class_size, random_state=42)\n",
        "    balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Created a balanced dataset with {len(balanced_df)} total samples ({min_class_size} per class).\")\n",
        "\n",
        "    # Save and return dataset\n",
        "    balanced_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "    print(f\"Balanced dataset saved to {output_csv_path}\")\n",
        "\n",
        "    dataset = Dataset.from_pandas(balanced_df[['text', 'label']])\n",
        "    dataset = dataset.cast_column('label', ClassLabel(num_classes=2, names=['human', 'machine']))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "6tKUFU1ALbPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy data files if they don't exist, as they are loaded in the notebook\n",
        "if not os.path.exists('zulu_mg_text.jsonl'):\n",
        "    with open('zulu_mg_text.jsonl', 'w') as f:\n",
        "        f.write('{\"text\": \"Lokhu umbhalo owenziwe ngomshini mayelana namasiko akwaZulu.\"}\\n')\n",
        "        f.write('{\"text\": \"UNogwaja noFudu babengabangani abakhulu ehlathini.\"}\\n')\n",
        "\n",
        "if not os.path.exists('eng_mg_data.jsonl'):\n",
        "    with open('eng_mg_data.jsonl', 'w') as f:\n",
        "        f.write('{\"text\": \"This is machine-generated text about South African culture.\"}\\n')\n",
        "        f.write('{\"text\": \"The quick brown fox jumps over the lazy dog in Johannesburg.\"}\\n')\n",
        "\n",
        "# Prepare isiZulu Dataset\n",
        "zulu_dataset = prepare_dataset(\n",
        "    lang_code='zul',\n",
        "    human_data_source='dsfsi/vukuzenzele-monolingual',\n",
        "    machine_data_path='zulu_mg_text.jsonl',\n",
        "    output_csv_path='balanced_zulu_texts.csv'\n",
        ")\n",
        "\n",
        "# Prepare English Dataset\n",
        "english_dataset = prepare_dataset(\n",
        "    lang_code='eng',\n",
        "    human_data_source='dsfsi/vukuzenzele-monolingual',\n",
        "    machine_data_path='eng_mg_data.jsonl',\n",
        "    output_csv_path='balanced_english_texts.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "huMSxQqUMri0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Phase 2: Model Development\n",
        "\n",
        "We define a function to handle the fine-tuning process. We will train two models:\n",
        "1.  **Afro-XLMR-Base**: Fine-tuned on our balanced isiZulu dataset.\n",
        "2.  **XLM-RoBERTa-Base**: Fine-tuned on our balanced English dataset to later test for zero-shot transfer."
      ],
      "metadata": {
        "id": "GnaIuS5kNEbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune a model on the provided dataset\n",
        "def fine_tune_model(dataset, model_name, output_dir):\n",
        "    print(f\"\\nFine-tuning {model_name} on {output_dir}\")\n",
        "\n",
        "    # Split data\n",
        "    train_val_split = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
        "    train_dataset = train_val_split['train']\n",
        "    val_dataset = train_val_split['test']\n",
        "\n",
        "    # Tokenize datasets\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Set format for PyTorch\n",
        "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    # Define metrics\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "        f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
        "        return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "    # Set up Trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=50,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    # Train and save\n",
        "    trainer.train()\n",
        "    model.save_pretrained(f'{output_dir}/model')\n",
        "    tokenizer.save_pretrained(f'{output_dir}/model')\n",
        "    print(f\"Best model saved to {output_dir}/model\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "    # 7. Clean up memory\n",
        "    del model, trainer\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    return f'{output_dir}/model'"
      ],
      "metadata": {
        "id": "gIUfn-vqNKWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune on isiZulu\n",
        "if zulu_dataset:\n",
        "    zulu_model_path = fine_tune_model(\n",
        "        dataset=zulu_dataset,\n",
        "        model_name=\"Davlan/afro-xlmr-base\",\n",
        "        output_dir=\"./zulu_finetuned_model\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping Zulu model training due to data loading issues.\")\n",
        "    zulu_model_path = None\n",
        "\n",
        "# Fine-tune on English\n",
        "if english_dataset:\n",
        "    english_model_path = fine_tune_model(\n",
        "        dataset=english_dataset,\n",
        "        model_name=\"xlm-roberta-base\",\n",
        "        output_dir=\"./english_finetuned_model\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping English model training due to data loading issues.\")\n",
        "    english_model_path = None"
      ],
      "metadata": {
        "id": "7QFmW69ZNNbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Phase 3: Evaluation and Analysis\n",
        "\n",
        "This phase addresses our core research questions:\n",
        "1.  **Error Analysis & Explainability**: We analyze the performance of the isiZulu-specific model, identifying where it fails and using SHAP to understand its decision-making process.\n",
        "2.  **Zero-Shot Transfer**: We evaluate how well the model trained only on English data performs on the isiZulu test set, measuring the effectiveness of cross-lingual transfer."
      ],
      "metadata": {
        "id": "vVIBRPqFNSTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a full evaluation including metrics, error analysis and SHAP\n",
        "def perform_evaluation(model_path, eval_dataset, report_path):\n",
        "    print(f\"\\nEvaluating model from: {model_path}\")\n",
        "\n",
        "    # Load Model and Tokenizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare evaluation pipeline with truncation\n",
        "    classifier = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if device=='cuda' else -1,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Get Predictions\n",
        "    texts = eval_dataset['text']\n",
        "    true_labels = np.array(eval_dataset['label'])\n",
        "    preds_output = classifier(texts, batch_size=8)\n",
        "\n",
        "    # Extract predicted labels ('LABEL_0' -> 0)\n",
        "    pred_labels = np.array([int(p['label'].split('_')[1]) for p in preds_output])\n",
        "\n",
        "    # Calculate Overall Metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
        "    accuracy = (true_labels == pred_labels).mean()\n",
        "    metrics_summary = f\"Accuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-Score: {f1:.4f}\"\n",
        "    print(\"Overall Performance Metrics:\")\n",
        "    print(metrics_summary)\n",
        "\n",
        "    # Error Analysis\n",
        "    errors = []\n",
        "    for i, (true, pred) in enumerate(zip(true_labels, pred_labels)):\n",
        "        if true != pred:\n",
        "            tokens = word_tokenize(texts[i])\n",
        "            ttr = len(set(tokens)) / len(tokens) if tokens else 0\n",
        "            errors.append(\n",
        "                f\"Text: {texts[i][:200]}...\\n\"\n",
        "                f\"  True Label: {'machine' if true == 1 else 'human'}\\n\"\n",
        "                f\"  Predicted Label: {'machine' if pred == 1 else 'human'}\\n\"\n",
        "                f\"  Text Length: {len(tokens)} tokens, Lexical Diversity (TTR): {ttr:.3f}\\n\"\n",
        "            )\n",
        "\n",
        "    # SHAP Analysis on a few examples\n",
        "    print(\"\\nGenerating SHAP explanations for a few samples...\")\n",
        "    shap_explainer = shap.Explainer(classifier)\n",
        "    shap_values = shap_explainer(texts[:3]) # Explain first 3 samples\n",
        "\n",
        "    # Save SHAP plots to files\n",
        "    shap_plots_dir = \"./shap_plots\"\n",
        "    os.makedirs(shap_plots_dir, exist_ok=True)\n",
        "    shap_plot_paths = []\n",
        "    for i in range(len(shap_values)):\n",
        "        plot_path = os.path.join(shap_plots_dir, f\"{os.path.basename(model_path)}_sample_{i}.png\")\n",
        "        shap.plots.text(shap_values[i], display=False)\n",
        "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        shap_plot_paths.append(plot_path)\n",
        "    print(f\"Saved {len(shap_plot_paths)} SHAP plots to '{shap_plots_dir}/'\")\n",
        "\n",
        "    # Write a consolidated report\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"Evaluation Report for: {model_path}\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "        f.write(\"### Overall Performance ###\\n\")\n",
        "        f.write(metrics_summary + \"\\n\\n\")\n",
        "        f.write(\"### SHAP Explanations ###\\n\")\n",
        "        f.write(f\"SHAP plots were saved to the '{shap_plots_dir}' directory.\\n\")\n",
        "        f.write(\"These plots visualize token contributions to the model's prediction.\\n\\n\")\n",
        "        f.write(\"### Error Analysis ###\\n\")\n",
        "        f.write(f\"Found {len(errors)} misclassifications in the evaluation set.\\n\\n\")\n",
        "        if errors:\n",
        "            f.write(\"\\n\".join(errors))\n",
        "        else:\n",
        "            f.write(\"No errors found. Perfect classification on this set!\\n\")\n",
        "\n",
        "    print(f\"\\nFull evaluation report saved to: {report_path}\")\n",
        "\n",
        "    # Clean up memory\n",
        "    del model, tokenizer, classifier, shap_explainer, shap_values\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "TGhYiaevNVV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test split of the Zulu dataset for evaluation\n",
        "nltk.download('punkt_tab')\n",
        "if zulu_dataset:\n",
        "    zulu_test_dataset = zulu_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')['test']\n",
        "\n",
        "    # Evaluation of the isiZulu-specific Model\n",
        "    if zulu_model_path and os.path.exists(zulu_model_path):\n",
        "        perform_evaluation(\n",
        "            model_path=zulu_model_path,\n",
        "            eval_dataset=zulu_test_dataset,\n",
        "            report_path=\"./zulu_model_evaluation_report.txt\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"Zulu model not found, skipping evaluation.\")\n",
        "\n",
        "    # Zero-Shot Evaluation of the English-trained Model on isiZulu\n",
        "    if english_model_path and os.path.exists(english_model_path):\n",
        "        perform_evaluation(\n",
        "            model_path=english_model_path,\n",
        "            eval_dataset=zulu_test_dataset,\n",
        "            report_path=\"./zero_shot_evaluation_report.txt\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"English model not found, skipping zero-shot evaluation.\")\n",
        "else:\n",
        "    print(\"Zulu dataset not available, skipping all evaluations.\")"
      ],
      "metadata": {
        "id": "6a94IKyUNYXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Interactive Demonstrator\n",
        "\n",
        "This final component provides a command-line interface to test the fine-tuned models on new text inputs. It includes memory management to efficiently switch between the English and isiZulu models and provides a prediction with a SHAP explanation."
      ],
      "metadata": {
        "id": "EDiHEukqNc3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "# Configuration\n",
        "# Store model paths in a dictionary for easier access\n",
        "MODELS_INFO = {\n",
        "    'eng': {'path': './english_finetuned_model/model'},\n",
        "    'zul': {'path': './zulu_finetuned_model/model'}\n",
        "}\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ID2LABEL = {0: 'Human-Generated', 1: 'Machine-Generated'}\n",
        "LABEL2ID = {v: k for k, v in ID2LABEL.items()}\n",
        "SHAP_PLOTS_DIR = 'shap_explanations'\n",
        "\n",
        "# store loaded models on the CPU to avoid re-loading from disk\n",
        "MODELS_CACHE = {}\n",
        "\n",
        "# Create the directory for SHAP plots if it doesn't exist\n",
        "os.makedirs(SHAP_PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "def get_or_load_model(lang_key, device):\n",
        "    # Iterate through any models already in the cache\n",
        "    for key, cached_data in MODELS_CACHE.items():\n",
        "        # If there's a model on the GPU that is NOT the one we want, move it to CPU\n",
        "        if key != lang_key and cached_data['model'].device.type == 'cuda':\n",
        "            print(f\"Moving '{key}' model to CPU to free up VRAM...\")\n",
        "            cached_data['model'].to('cpu')\n",
        "\n",
        "    # Force Python's garbage collector and empty PyTorch's cache\n",
        "    gc.collect()\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Get the requested model\n",
        "    if lang_key in MODELS_CACHE:\n",
        "        # Model is already in our cache (on the CPU), just retrieve it\n",
        "        print(f\"Loading '{lang_key}' model from cache...\")\n",
        "        model = MODELS_CACHE[lang_key]['model']\n",
        "        tokenizer = MODELS_CACHE[lang_key]['tokenizer']\n",
        "    else:\n",
        "        # Model is not cached, load it from disk\n",
        "        print(f\"Loading '{lang_key}' model from disk...\")\n",
        "        model_path_info = MODELS_INFO.get(lang_key)\n",
        "        if not model_path_info:\n",
        "            print(f\"Error: No model path configured for language '{lang_key}'.\")\n",
        "            return None, None\n",
        "\n",
        "        model_dir = os.path.join(model_path_info['path'], '')\n",
        "        tokenizer_dir = os.path.join(model_path_info['path'], '')\n",
        "\n",
        "        if not os.path.exists(model_dir) or not os.path.exists(tokenizer_dir):\n",
        "            print(f\"Error: Model or tokenizer not found at '{model_path_info['path']}'.\")\n",
        "            print(\"Please ensure you have trained and saved the models as per the notebook.\")\n",
        "            return None, None\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
        "\n",
        "        # Store the newly loaded model and tokenizer in the cache\n",
        "        MODELS_CACHE[lang_key] = {'model': model, 'tokenizer': tokenizer}\n",
        "\n",
        "    # Move the requested model to the target device (GPU/CPU)\n",
        "    if model.device.type != device.type:\n",
        "        print(f\"Moving '{lang_key}' model to {device.type.upper()}...\")\n",
        "        model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"...model is ready.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def predict_and_explain(text, model, tokenizer, device):\n",
        "\n",
        "    # Prediction\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "    prediction_idx = np.argmax(probabilities)\n",
        "    predicted_label = ID2LABEL[prediction_idx]\n",
        "    confidence = probabilities[prediction_idx]\n",
        "\n",
        "    # SHAP Explanation\n",
        "    print(\"\\nGenerating SHAP explanation... (this may take a moment)\")\n",
        "\n",
        "    def predict_proba_for_shap(texts):\n",
        "        if isinstance(texts, np.ndarray):\n",
        "            texts = texts.tolist()\n",
        "        inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        return torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    explainer = shap.Explainer(predict_proba_for_shap, shap.maskers.Text(tokenizer, mask_token=\"<unk>\"), output_names=list(ID2LABEL.values()))\n",
        "    shap_values = explainer([text])\n",
        "\n",
        "    # Generate and Save SHAP Plot\n",
        "    class_index_to_explain = LABEL2ID['Machine-Generated']\n",
        "    plt.figure()\n",
        "    shap.plots.waterfall(shap_values[0, :, class_index_to_explain], show=False)\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    plot_filename = f\"shap_explanation_{lang_key}_{timestamp}.png\"\n",
        "    plot_path = os.path.join(SHAP_PLOTS_DIR, plot_filename)\n",
        "\n",
        "    plt.savefig(plot_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"...explanation saved to {plot_path}\")\n",
        "\n",
        "    return predicted_label, confidence, plot_path\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\" Human vs. Machine Text Detector for English & isiZulu\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Running on device: {str(DEVICE).upper()}\")\n",
        "\n",
        "    # Make lang_key accessible to predict_and_explain for file naming\n",
        "    global lang_key\n",
        "\n",
        "    while True:\n",
        "        # Get User Input\n",
        "        lang_choice = \"\"\n",
        "        while lang_choice not in ['1', '2']:\n",
        "            lang_choice = input(\"\\nChoose a language:\\n  1: English\\n  2: isiZulu\\nEnter choice (1 or 2): \")\n",
        "\n",
        "        lang_key = 'eng' if lang_choice == '1' else 'zul'\n",
        "\n",
        "        # Load model on-demand\n",
        "        model, tokenizer = get_or_load_model(lang_key, DEVICE)\n",
        "\n",
        "        # Check if the model failed to load\n",
        "        if model is None:\n",
        "            break # Exit if a model path is incorrect\n",
        "\n",
        "        input_text = input(f\"\\nPlease enter the {lang_key.upper()} text you want to analyze:\\n> \")\n",
        "\n",
        "        if not input_text.strip():\n",
        "            print(\"Error: Input text cannot be empty.\")\n",
        "            continue\n",
        "\n",
        "        # Perform Analysis\n",
        "        print(\"\\nAnalyzing text...\")\n",
        "        predicted_label, confidence, plot_path = predict_and_explain(\n",
        "            input_text, model, tokenizer, DEVICE\n",
        "        )\n",
        "\n",
        "        # Display Results\n",
        "        print(\"\\n\" + \"-\"*25 + \" ANALYSIS RESULTS \" + \"-\"*25)\n",
        "        print(f\"  Prediction: The text is likely {predicted_label.upper()}\")\n",
        "        print(f\"  Confidence: {confidence:.2%}\")\n",
        "        print(f\"\\nAn explanation plot has been saved here: {plot_path}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Loop Control\n",
        "        another = input(\"\\nWould you like to analyze another text? (y/n): \").lower()\n",
        "        if another != 'y':\n",
        "            print(\"\\nThank you for using the detector. Goodbye!\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "pU9A_XGDNhj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./zulu_finetuned_model/model\"\n",
        "tokenizer_path = \"./zulu_finetuned_model/model\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "# Create a text classification pipeline\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {0: \"human\", 1: \"machine\"}\n",
        "\n",
        "# Function to predict and explain\n",
        "def predict_and_explain(text):\n",
        "    # Predict\n",
        "    prediction = classifier(text)[0]\n",
        "    predicted_label = label_mapping[int(prediction[0][\"label\"].split(\"_\")[1])]\n",
        "    human_prob = prediction[0][\"score\"] if predicted_label == \"human\" else prediction[1][\"score\"]\n",
        "    machine_prob = prediction[1][\"score\"] if predicted_label == \"human\" else prediction[0][\"score\"]\n",
        "\n",
        "    # SHAP Explanation\n",
        "    explainer = shap.Explainer(classifier)\n",
        "    shap_values = explainer([text])\n",
        "\n",
        "    # Extract SHAP values for the predicted class\n",
        "    predicted_class_idx = 0 if predicted_label == \"human\" else 1\n",
        "    shap_values_for_predicted_class = shap_values[0, :, predicted_class_idx]\n",
        "\n",
        "    # Format results\n",
        "    result = {\n",
        "        \"text\": text,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"human_probability\": human_prob,\n",
        "        \"machine_probability\": machine_prob,\n",
        "        \"shap_values\": shap_values_for_predicted_class\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nInput Text: {text}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print(f\"Human Probability: {human_prob:.4f}\")\n",
        "    print(f\"Machine Probability: {machine_prob:.4f}\")\n",
        "    print(\"\\nSHAP Explanation:\")\n",
        "    shap.plots.text(shap_values[0, :, :])\n",
        "\n",
        "    return result\n",
        "\n",
        "# Interactive input loop\n",
        "def main():\n",
        "    print(\"Text Classification and SHAP Explanation Tool\")\n",
        "    print(\"Enter text to classify as human or machine-generated. Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter text: \").strip()\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Exiting...\")\n",
        "            break\n",
        "        if not user_input:\n",
        "            print(\"Please enter some text.\")\n",
        "            continue\n",
        "\n",
        "        # Predict and explain\n",
        "        predict_and_explain(user_input)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "EqMxUaUpDENB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}